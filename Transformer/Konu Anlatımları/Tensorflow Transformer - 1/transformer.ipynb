{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ecbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow ve Keras temel modülleri\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    "    Dropout\n",
    ")\n",
    "\n",
    "# Eğitim için\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Veri hazırlama\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Yardımcı kütüphaneler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d9924",
   "metadata": {},
   "source": [
    "# 📘 TRANSFORMER MİMARİSİNE GİRİŞ – KONU BAŞLIKLARI VE AÇIKLAMALARI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a533ae9",
   "metadata": {},
   "source": [
    "## 1. 🔍 Transformer Nedir?\n",
    "* LSTM/RNN gibi sıralı yapılara alternatif olarak geliştirilmiştir.\n",
    "\n",
    "* 2017’de “Attention is All You Need” makalesiyle tanıtılmıştır.\n",
    "\n",
    "* Doğal dil işleme (NLP), görüntü işleme, zaman serisi ve çok daha fazlasında kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171e75b",
   "metadata": {},
   "source": [
    "## 2. 🧠 Self-Attention Mekanizması\n",
    "Her kelime, diğer kelimelerle olan ilişkisini değerlendirir.\n",
    "\n",
    "* Query, Key, Value matrisleri üzerinden hesaplanır.\n",
    "\n",
    "Uzun dizilerdeki bağlam ilişkilerini çok daha etkili şekilde öğrenir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca071ec",
   "metadata": {},
   "source": [
    "## 3. 🧩 Multi-Head Attention\n",
    "* Self-Attention işlemi birden fazla “baş” ile yapılır.\n",
    "\n",
    "* Farklı temsillerin paralel olarak öğrenilmesini sağlar.\n",
    "\n",
    "* Her baş, farklı bir bilgiye odaklanabilir (örneğin dilbilgisi, bağlam, kelime ilişkisi)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106e885",
   "metadata": {},
   "source": [
    "## 4. 🌐 Positional Encoding\n",
    "* Transformer'lar sıralı bilgiye doğrudan sahip değildir.\n",
    "\n",
    "* Pozisyon bilgisini sinüsoidal fonksiyonlarla modele enjekte eder.\n",
    "\n",
    "* Her kelimenin cümle içindeki konumu böylece anlaşılır hale gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1aed51",
   "metadata": {},
   "source": [
    "## 5. 🧱 Transformer Bloklarının Yapısı\n",
    "Her Encoder/Decoder bloğu şu bileşenleri içerir:\n",
    "\n",
    "* Multi-Head Attention\n",
    "\n",
    "* Add & Layer Normalization\n",
    "\n",
    "* Feedforward Neural Network\n",
    "\n",
    "* Add & Layer Normalization\n",
    "\n",
    "Her blokta residual bağlantılar vardır (skip connections), eğitim sürecini stabil tutar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e97b1",
   "metadata": {},
   "source": [
    "## 6. 🔁 Encoder ve Decoder Yapısı\n",
    "Encoder:\n",
    "\n",
    "* Giriş cümlesini işler, gizli temsiller üretir.\n",
    "\n",
    "Decoder:\n",
    "\n",
    "* Bu temsillerden yola çıkarak çıkış dizisi üretir.\n",
    "\n",
    "Masked Attention ile gelecekteki kelimelere bakmayı engeller (örneğin çeviride)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ac948",
   "metadata": {},
   "source": [
    "## 7. ⚙️ Transformer’ın Avantajları\n",
    "* Paralel işlemeye uygun (GPU/TPU için hızlı).\n",
    "\n",
    "* Uzun bağımlılıkları daha iyi öğrenir.\n",
    "\n",
    "* Daha az bellekle daha fazla bağlam yakalayabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cb323",
   "metadata": {},
   "source": [
    "## 8. 🛠️ Transformer ile Uygulama Senaryoları\n",
    "* Makine çevirisi (EN → TR)\n",
    "\n",
    "* Chatbotlar (GPT mimarisi)\n",
    "\n",
    "* Metin sınıflandırma (BERT gibi)\n",
    "\n",
    "* Metin tamamlama / üretim (GPT-2, GPT-3)\n",
    "\n",
    "* Zaman serisi tahmini, görüntü işlemede ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69be55",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12748a2",
   "metadata": {},
   "source": [
    "# SEQ2SEQ ile TRANSFORMER arasındaki farklar nelerdir ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074e046",
   "metadata": {},
   "source": [
    "### 💡 1. Veri İşleme Yöntemi (Sıralı mı? Paralel mi?)\n",
    "* LSTM tabanlı modeller veriyi sırayla işler. Her adımda bir kelime alınır, önceki gizli durum aktarılır. Bu nedenle hesaplama zaman bağımlı ve sıralıdır.\n",
    "\n",
    "* Transformer ise tüm kelimeleri aynı anda işler. Çünkü self-attention tüm diziyi bir kerede görebilir. Bu sayede eğitim süreci çok daha hızlıdır ve paralel hesaplama yapılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5b1e1",
   "metadata": {},
   "source": [
    "\n",
    "### 🧠 2. Uzun Bağlamları Öğrenme Yeteneği\n",
    "* LSTM’ler uzun cümlelerde erken gelen kelimeleri unutabilir. Evet, bidirectional LSTM ve attention eklenerek bu kısmen iyileştirilir ama tam çözüm değildir.\n",
    "\n",
    "* Transformer, her kelimenin diğer tüm kelimelerle olan ilişkisini self-attention ile doğrudan öğrenir. Bu sayede uzun bağlamlar çok daha sağlam öğrenilir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b7ef0",
   "metadata": {},
   "source": [
    "### 🧲 3. Attention Kullanımı\n",
    "* LSTM tabanlı modellerde Attention genellikle Encoder’ın son gizli durumuna göre çalışır ve Decoder'da dıştan eklenir.\n",
    "\n",
    "* Transformer'da ise Attention modelin merkezi parçasıdır. Hem Encoder hem Decoder çok katmanlı Attention bloklarından oluşur. Yani attention kenarda bir eklenti değil, doğrudan yapının kendisidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009efee",
   "metadata": {},
   "source": [
    "### 🧱 4. Yapısal Fark (Layer Bazında)\n",
    "* LSTM modeller katman katman ilerlese de her katman kendi içinde sıralı bilgi taşır. Layer’lar arasında veri genellikle bir hidden state olarak geçer.\n",
    "\n",
    "Transformer’da her katman:\n",
    "\n",
    "* Multi-Head Attention\n",
    "\n",
    "* Residual Connection\n",
    "\n",
    "* Layer Normalization\n",
    "\n",
    "* Feedforward katman içerir.\n",
    "\n",
    "Bu yapı sayesinde çok daha derin modeller stabil şekilde eğitilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f042b7",
   "metadata": {},
   "source": [
    "### ✅ Özetle Neden Transformer?\n",
    "* LSTM'ler sıralı ve yavaş, Transformer paralel ve hızlı.\n",
    "\n",
    "* Transformer, daha uzun bağlamları etkili biçimde öğrenebilir.\n",
    "\n",
    "* Attention, LSTM'de sonradan eklenir ama Transformer'da yapının kalbidir.\n",
    "\n",
    "* Eğitimde verimli, ölçeklenebilir ve daha derin mimarilere uygundur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106db3b",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d41f7",
   "metadata": {},
   "source": [
    "# 🧾  Frame Tabanlı Transformer Sohbet Modeli\n",
    "### 📌 Giriş\n",
    "* Bu çalışmada, Transformer Encoder-Decoder mimarisi kullanılarak geliştirilen frame tabanlı bir sohbet modeli eğitilecektir. Bu model, kullanıcıdan gelen belirli kalıplardaki (frame) girdilere karşılık olarak önceden tanımlanmış çıktıları öğrenmeyi amaçlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5ffd9",
   "metadata": {},
   "source": [
    "### 🔍 Amaç\n",
    "* Bu modelin amacı, çok büyük dil modelleri (örneğin GPT-2, GPT-3) gibi genel amaçlı bir dil zekası üretmek değil; sadece eğitim sırasında verilen input-output çiftleri üzerinden öğrenerek, sınırlı bir etkileşim sağlamaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82949c1",
   "metadata": {},
   "source": [
    "### 🧠 Model Özellikleri\n",
    "* Mimari: Transformer Encoder–Decoder (Attention temelli)\n",
    "\n",
    "* Eğitim verisi: Girdi–çıktı çiftlerinden oluşan küçük bir özel veri kümesi\n",
    "\n",
    "* Çalışma mantığı: Kullanıcıdan gelen belirli cümleleri tanıyıp, karşılık gelen çıktıyı üretmek\n",
    "\n",
    "* Genelleme kabiliyeti: Düşüktür, sadece örneklerine benzer girdilere anlamlı cevap verir\n",
    "\n",
    "* Avantajı: Küçük veriyle çalışabilir, hızlı eğitilir\n",
    "\n",
    "* Dezavantajı: Ezberci davranır; eğitim dışındaki örüntülerde başarısı düşer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97df054",
   "metadata": {},
   "source": [
    "### 📚 Uygulama Alanı\n",
    "* Menü tabanlı chatbot\n",
    "\n",
    "* Sık sorulan sorular yanıtlayıcısı (FAQ bot)\n",
    "\n",
    "* Kapalı alan asistanları (örneğin: otel resepsiyonu, otomatik müşteri temsilcisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976a737",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415acc4d",
   "metadata": {},
   "source": [
    "### Şimdi sizinle frame tabanlı Transformer mantığını inceyeleyelim.Bu adımda kendi oluşturduğum verisetini kullanacağız.Adımları tek tek açıklayarak devam edeceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8f322",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77857e0",
   "metadata": {},
   "source": [
    "# -- Frame Tabanlı Transformerlar -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9feb4",
   "metadata": {},
   "source": [
    "* BU NOTEBOOK ANLATIMINDA RNN SEQ2SEQ MODELLERİN , RNN'LERİN , ANN'LERİN , CNN'LERİN BİLİNDİĞİ VAR SAYILMIŞTIR.ANLATIMLAR DİĞER REPOLARIN DEVAMI OLARAK SUNULACAKTIR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba97ede",
   "metadata": {},
   "source": [
    "### 🧩 Frame Tabanlı Transformer Modeli – Adım Adım Plan\n",
    "\n",
    "2️⃣ 📊 Veriyi pandas ile oku (pd.read_csv)\n",
    "\n",
    "3️⃣ 📝 Cümleleri listeye al (input ve output ayrı ayrı)\n",
    "\n",
    "4️⃣ 🔤 Tokenizer oluştur (hem input hem output için)\n",
    "\n",
    "5️⃣ 📏 Cümleleri tokenize et, pad uygula (maksimum uzunluk belirle)\n",
    "\n",
    "6️⃣ 📦 Veriyi ayır → X, y_input, y_target (encoder/decoder ayrımı)\n",
    "\n",
    "7️⃣ 🏗️ Transformer modelini kur (Encoder–Decoder yapısı)\n",
    "\n",
    "8️⃣ ⚙️ Loss ve optimizer tanımla (örneğin SparseCategoricalCrossentropy)\n",
    "\n",
    "9️⃣ 🎯 Modeli eğit (model.fit)\n",
    "\n",
    "🔟 💬 Tahmin fonksiyonu yaz (input ver, output üret)\n",
    "\n",
    "1️⃣1️⃣ 🧪 Modeli test et (örnek girdilerle dene)\n",
    "\n",
    "1️⃣2️⃣ 💾 Modeli kaydet (isteğe bağlı olarak model.save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1513aca",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd06a6",
   "metadata": {},
   "source": [
    "##  📊 Veriyi pandas ile oku (pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda9eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 input                                             output\n",
      "0              Merhaba           Merhaba, size nasıl yardımcı olabilirim?\n",
      "1            Nasılsın?           İyiyim, teşekkür ederim. Siz nasılsınız?\n",
      "2             Adın ne?  Ben bir yapay zekâ asistanıyım. Adım yok ama y...\n",
      "3      Kaç yaşındasın?                        Benim yaşım yok, dijitalim!\n",
      "4  Bugün günlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\Transformerlar\\Konu Anlatımları\\Tenserflow Transformer - Teorik\\örnek_set.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cb1c6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89389344",
   "metadata": {},
   "source": [
    "## 3️⃣ 📝 Cümleleri listeye al (input ve output ayrı ayrı)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd32f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['input'].astype(str).to_list()\n",
    "target_texts = df['output'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1b9e6",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc239a7b",
   "metadata": {},
   "source": [
    "## 4️⃣ 🔤 Tokenizer oluştur (hem input hem output için)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f7762",
   "metadata": {},
   "source": [
    "* Aslında seq2seq modellerde kullandığımızdan çok daha farklı bir işlem yapmayacağız.Neredeyse aynı bile denebilir.Üstüne daha iyi hale getirmek için eklentiler açacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c081932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# START VE END TOKENLARINI CÜMLEYE EKLEDİK\n",
    "target_texts_with_token = ['<start> ' + t + ' <end>' for t in target_texts]\n",
    "\n",
    "# TOKENIZERLARI FİLTRE VE OOV TOKEN İLE OLUŞTURDUK\n",
    "input_tokenizer = Tokenizer(filters=\"\",oov_token=\"<OOV>\")\n",
    "target_tokenizer = Tokenizer(filters=\"\",oov_token=\"<OOV>\")\n",
    "\n",
    "# TOKENLARI FİT ETTİK\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "target_tokenizer.fit_on_texts(target_texts_with_token)\n",
    "\n",
    "# CÜMLELERİ TOKENİZE EDECEĞİZ\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "decoder_input_sequences = target_tokenizer.texts_to_sequences(['<start> ' + t for t in target_texts])\n",
    "\n",
    "decoder_target_sequences = target_tokenizer.texts_to_sequences([t+' <end>' for t in target_texts])\n",
    "\n",
    "# MAX UZUNLUĞU HESAPLADIK\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_output_len = max(len(seq) for seq in input_sequences)\n",
    "\n",
    "max_len = max(max_input_len,max_output_len)\n",
    "\n",
    "# PAD İŞLEMİNİ GERÇEKLEŞTİRDİK\n",
    "encoder_input = pad_sequences(input_sequences,maxlen=max_len,padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input_sequences,maxlen=max_len,padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target_sequences,maxlen=max_len,padding=\"post\")  \n",
    "\n",
    "# VOCAB_SİZE BELİRLEDİK.\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a80d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "print(input_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273b279",
   "metadata": {},
   "source": [
    "### Bu işlemleri daha iyi bir hale getirebiliriz.Mesela verileri temizleriz ya da özel tokenları elle ekleriz.Gelin bu halden daha iyi bir hale getirelim bu tokenizerları."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e71a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### 🔤 1. Özel tokenları tanımla\n",
    "SPECIAL_TOKENS = ['<pad>', '<start>', '<end>', '<OOV>']\n",
    "\n",
    "### 🧹 2. Temizlik fonksiyonu\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"–\", \"-\").replace(\"...\", \".\")\n",
    "    return text\n",
    "\n",
    "### 📝 3. Cümleleri temizle\n",
    "input_texts_clean = [clean_text(t) for t in input_texts]\n",
    "target_texts_clean = [clean_text(t) for t in target_texts]\n",
    "\n",
    "### 🪄 4. Target cümlelere start ve end token ekle\n",
    "target_texts_with_tokens = ['<start> ' + t + ' <end>' for t in target_texts_clean]\n",
    "\n",
    "### 🧠 5. Tokenizer oluştur ve özel tokenları vocab'a enjekte et\n",
    "def build_tokenizer(texts, num_words=None):\n",
    "    tokenizer = Tokenizer(filters='', oov_token='<OOV>', num_words=num_words)\n",
    "    tokenizer.fit_on_texts(SPECIAL_TOKENS + texts)  # özel tokenları ilk sıraya ekle\n",
    "    return tokenizer\n",
    "\n",
    "input_tokenizer = build_tokenizer(input_texts_clean)\n",
    "target_tokenizer = build_tokenizer(target_texts_with_tokens)\n",
    "\n",
    "### 🔢 6. Cümleleri tokenize et\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts_clean)\n",
    "decoder_input_sequences = target_tokenizer.texts_to_sequences(\n",
    "    ['<start> ' + t for t in target_texts_clean]\n",
    ")\n",
    "decoder_target_sequences = target_tokenizer.texts_to_sequences(\n",
    "    [t + ' <end>' for t in target_texts_clean]\n",
    ")\n",
    "\n",
    "### 📏 7. Maksimum uzunluk belirle (tek max_len kullanılacak)\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "max_target_len = max(len(seq) for seq in decoder_target_sequences)\n",
    "max_len = max(max_input_len, max_target_len)\n",
    "\n",
    "### 🧱 8. Padding uygula\n",
    "encoder_input = pad_sequences(input_sequences, maxlen=max_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input_sequences, maxlen=max_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "### 📌 10. Vocab boyutları\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6bfd5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271118c",
   "metadata": {},
   "source": [
    "## 📦 6️⃣ Veriyi ayır → X, y_input, y_target\n",
    "\n",
    "* Bu adımda modelin 3 temel girdisini oluşturacağız:\n",
    "\n",
    "\n",
    "X =>\tEncoder'a gidecek input (kullanıcının mesajı)\n",
    "\n",
    "y_input =>\tDecoder'a giriş olarak verilen diziler (<start token'lı)\n",
    "\n",
    "y_target =>\t Modelin üretmesi gereken doğru çıktı (<end token'lı)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d90580",
   "metadata": {},
   "source": [
    "#### 🧠 Kısaca:\n",
    "y_input = Decoder’a “tahmin etmeye başla” demek\n",
    "\n",
    "y_target = “Ne tahmin etmen gerekiyordu” demek\n",
    "\n",
    "Bu ikisinin ayrılması = Teacher Forcing dediğimiz tekniğin temelidir\n",
    "\n",
    "#### 🛠️ Eğer bu ayrımı yapmazsan...\n",
    "Model decoder’a giriş alamaz, çıktı üretemez\n",
    "\n",
    "loss hesaplaması yanlış olur (çıktıyı neyle kıyaslayacağını bilemez)\n",
    "\n",
    "Eğitim başarısız olur ya da saçma sonuçlar üretir\n",
    "\n",
    "#### 🎓 Sonuç:\n",
    "Encoder–Decoder mimarilerde doğru input ve target ayrımı, modelin dili öğrenmesinin temel taşıdır.\n",
    "\n",
    "Bu nedenle bu adım, \"Transformer’ın konuşmayı öğrenmesini sağlayan yapı taşlarının montajı\" gibidir diyebiliriz 🧩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba91da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder input → kullanıcı cümlesi\n",
    "X = encoder_input\n",
    "\n",
    "# Decoder input → <start> ile başlayan hedef cümle\n",
    "y_input = decoder_input\n",
    "\n",
    "# Decoder target → <end> ile biten gerçek cevap\n",
    "y_target = decoder_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7c328cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input örneği: [24  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Decoder input örneği: [ 2 54 14 55  7 56  0  0  0  0  0  0]\n",
      "Decoder target örneği: [54 14 55  7 56  3  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoder input örneği:\", X[0])\n",
    "print(\"Decoder input örneği:\", y_input[0])\n",
    "print(\"Decoder target örneği:\", y_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09bc0d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f9e7b",
   "metadata": {},
   "source": [
    "## 🧱 7️⃣ Transformer Encoder–Decoder Modeli (TensorFlow/Keras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c3bc0",
   "metadata": {},
   "source": [
    "* Model içinde çok daha farklı terimler göreceğiz.Hatırlatmak için tekrar yazıyorum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290b792",
   "metadata": {},
   "source": [
    "### 📍 Positional Encoding Nedir?\n",
    "* Transformer modelleri, RNN'lerin aksine veriyi sırayla işlemez — yani kelimelerin hangi sırada geldiğini doğrudan bilmez.\n",
    "\n",
    "Çünkü Attention mekanizması aynı anda tüm kelimelere bakar.\n",
    "\n",
    "Bu da sırayı \"unutmasına\" sebep olur.\n",
    "\n",
    "* İşte bu yüzden Positional Encoding devreye girer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44360004",
   "metadata": {},
   "source": [
    "####  🧠 Tanım:\n",
    "* Positional Encoding, her kelimenin sırasını modele bildirmek için embedding’lere eklenen özel bir sinyaldir.\n",
    "\n",
    "#### 🔢 Nasıl çalışır?\n",
    "* Her pozisyon (0, 1, 2, ...) için bir vektör üretilir.\n",
    "\n",
    "* Bu vektörler sinüs ve kosinüs fonksiyonlarıyla oluşturulur.\n",
    "\n",
    "* Böylece model şu farkı anlayabilir:\n",
    "\n",
    "* “2. kelime ile 5. kelime arasında ne kadar mesafe var?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dbbc4",
   "metadata": {},
   "source": [
    "#### 📌 Sonuç:\n",
    "* Positional Encoding, kelimelerin sırasını modele gizlice öğretir\n",
    "\n",
    "* Embedding’lere eklenir (toplama yapılır)\n",
    "\n",
    "* Model artık sadece ne dediğini değil, ne zaman dediğini de bilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92756d",
   "metadata": {},
   "source": [
    "#### Aslında bu konu benim de merakımdı.Yani neden sin/cos kullanıyoruz?Bu hangi amaca hitap ediyor ki?Gelin beraber inceleyelim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e7b82",
   "metadata": {},
   "source": [
    "### 🧩 Neden sin/kos?\n",
    "* Sinüs ve kosinüs, ardışık pozisyonlar arasındaki farkı düzgün ve öğrenilebilir biçimde taşır\n",
    "\n",
    "Daha da önemlisi:\n",
    "\n",
    "* Model, mutlak pozisyon değil, pozisyonlar arası fark üzerinde öğrenir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c337e0",
   "metadata": {},
   "source": [
    "### Gelin Positional  Encoder'ın kod bloklarına bakalımm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2352700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def positionel_encoding(max_len,d_model):\n",
    "\n",
    "# ➤ max_len kadar sıradaki pozisyon numaralarını tek tek bir sütun vektörü haline getirdik.\n",
    "    pos = tf.range(max_len,dtype = tf.float32)[:, tf.newaxis]\n",
    "\n",
    "# ➤ d_model, her kelimenin embedding (veya temsil) vektörünün boyutudur. Aşağıdaki satır ise bu boyutlar boyunca pozisyonel sinyallerin nasıl yayılacağını belirlemek için kullanılır.\"\n",
    "    i = tf.range(d_model , dtype=tf.float32)[tf.newaxis,:]\n",
    "\n",
    "# ➤ pozisyona göre hangi frekansta sinyal (açı) verileceğini belirler.\n",
    "    angle_rates =  1 / tf.pow(100000.0 ,(2 * ( i//2)) / tf.cast(d_model,tf.float32))\n",
    "\n",
    "# ➤ Her pozisyondaki kelime için, her embedding boyutuna karşılık gelen açısal (frekanslı) sinyali hesapla\n",
    "    angel_rads = pos * angle_rates\n",
    "\n",
    "    sines = tf.math.sin(angel_rads[:, 0::2])\n",
    "    cosines = tf.math.cos(angel_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines,cosines] , axis = 1)\n",
    "\n",
    "    return pos_encoding[tf.newaxis , ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040aaeb8",
   "metadata": {},
   "source": [
    "## 🔢 1. pos = tf.range(max_len)[:, tf.newaxis]\n",
    "\n",
    "* Çıktı olarak\n",
    "\n",
    "[[0]\n",
    " [1]\n",
    " [2]\n",
    " [3]\n",
    " [4]]\n",
    "\n",
    " ##### 📌 Anlamı:\n",
    "\n",
    "* Bu, her pozisyonu temsil eder.\n",
    "* Yani \"Merhaba dünya bugün nasılsın?\" cümlesi varsa:\n",
    "\n",
    "\"Merhaba\" → pozisyon 0\n",
    "\n",
    "\"dünya\" → pozisyon 1\n",
    "\n",
    "...\n",
    "\n",
    "* Yani pos = kaçıncı kelime olduğunu belirler.\n",
    "\n",
    "➤ max_len kadar sıradaki pozisyon numaralarını tek tek bir sütun vektörü haline getirdik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4f196",
   "metadata": {},
   "source": [
    "### 🎯 Kısaca:\n",
    "* tıpkı tokenizer'da her kelimeye ID verir gibi burada da her pozisyona ID veriyoruz\n",
    "\n",
    "* Ama ID'ler sayısal değil, birazdan sinyale dönüşecek 🧠📡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e6b88",
   "metadata": {},
   "source": [
    "## 2️⃣  i = tf.range(d_model)[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b549af",
   "metadata": {},
   "source": [
    "#### * ✅ Ne yapar?\n",
    "\n",
    "Her embedding boyutunun indeksini çıkarır. Yani \"0. boyut, 1. boyut, 2. boyut, ...\" gibi değerleri verir.\n",
    "\n",
    "\n",
    "##### *  Örnek: d_model = 4\n",
    "[[0, 1, 2, 3]]\n",
    "\n",
    "0 → embedding'in 1. boyutu\n",
    "\n",
    "1 → embedding'in 2. boyutu\n",
    "\n",
    "...\n",
    "\n",
    "Bu, her vektör boyutu için indeksleri temsil eder\n",
    "\n",
    "Ve angle_rates hesaplanırken her boyuta farklı sinyal verebilmek için lazım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68c868",
   "metadata": {},
   "source": [
    "##### 🧠 Amacı ne?\n",
    "Her embedding boyutuna ayrı bir sinyal frekansı verebilmek.\n",
    "Bu, positional encoding'in sinüs/kosinüs hesaplarında kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b08a6",
   "metadata": {},
   "source": [
    "##### 📌 Şunu unutma:\n",
    "* pos → pozisyonlar (0. kelime, 1. kelime...)\n",
    "\n",
    "* i → embedding boyutu indeksleri (0. boyut, 1. boyut...)\n",
    "\n",
    "* pos satırdır → her kelime sırası için bir vektör\n",
    "\n",
    "* i sütundur → her vektörün kaçıncı boyutu olduğunu belirtir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10bcb0",
   "metadata": {},
   "source": [
    "#### 📌 Kısa ve Net Tanım:\n",
    "* i = tf.range(d_model)[tf.newaxis, :]\n",
    "\n",
    "➤ Her pozisyon vektörünün hangi boyutuna hangi frekansla sinyal verileceğini belirlemek için kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f6973",
   "metadata": {},
   "source": [
    "## 3️⃣ 📐 angle_rates = 1 / tf.pow(10000., (2 * (i // 2)) / d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53e5fb",
   "metadata": {},
   "source": [
    "* Bu satır = pozisyona göre hangi frekansta sinyal (açı) verileceğini belirler.\n",
    "\n",
    "* Yani: “Bu embedding boyutunda nasıl bir sinyal eğrisi kullanayım?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57b892",
   "metadata": {},
   "source": [
    "#### 🧠 Ne Anlama Geliyor?\n",
    "🎯 Amaç:\n",
    "\n",
    "* Her embedding boyutuna farklı frekanslı sinyal vermek\n",
    "\n",
    "* Böylece pozisyon farkları hem küçük hem büyük ölçeklerde yakalanabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65158f",
   "metadata": {},
   "source": [
    "### 🔍 Parça Parça İnceleyelim:\n",
    "✅ i // 2\n",
    "\n",
    "* sin/cos dönüşümü için çift-tek ayrımı\n",
    "\n",
    "* Boyutları 2’şer 2’şer gruplandırmak için\n",
    "\n",
    "✅ (2 * (i // 2)) / d_model\n",
    "\n",
    "* d_model’e oranla ne kadar hızlı değişim olacak?\n",
    "\n",
    "* Daha küçük boyutlarda yüksek frekans (detaylı sinyal)\n",
    "\n",
    "* Daha büyük boyutlarda düşük frekans (genel yapı)\n",
    "\n",
    "✅ tf.pow(10000., ...)\n",
    "\n",
    "* Bu, frekansı üstel olarak azaltmak için kullanılır\n",
    "\n",
    "* 10000 sabiti → paper’da önerilen sabit; sinyalleri yeterince seyreltiyor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47919d2f",
   "metadata": {},
   "source": [
    "####  💡 Neden Bu Kadar Özenli?\n",
    "* Çünkü bazı kelime ilişkileri kısa mesafededir (örneğin: \"merhaba → nasılsın\"),\n",
    "bazıları uzundur (\"bugün → dışarısı → hava → güzel\")\n",
    "\n",
    "* Modelin her mesafeye göre sinyali olsun diye çok frekanslı bir yapı gerekir → işte bu satır bunun içindir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eacb7a",
   "metadata": {},
   "source": [
    "Terim ====== \tNe işe yarar?\n",
    "\n",
    "10000 =>\tÜssün bazı, frekansları yaymak için\n",
    "\n",
    "2i/d_model =>\tEmbedding boyutuna göre frekansı ölçeklemek\n",
    "\n",
    "i // 2 =\tAynı frekansı sin/cos olarak çift-tek boyutlara dağıtmak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63f82f",
   "metadata": {},
   "source": [
    "## 4️⃣ 📡 angle_rads = pos * angle_rates\n",
    "\n",
    "* Her pozisyondaki kelime için, her embedding boyutuna karşılık gelen açısal (frekanslı) sinyali hesaplar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25856c",
   "metadata": {},
   "source": [
    "##### 📐 Ne Oluyor Burada?\n",
    "* pos → her kelimenin sırası (0, 1, 2, 3, ...) → shape: [max_len, 1]\n",
    "\n",
    "* angle_rates → her embedding boyutu için frekans katsayısı → shape: [1, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1b066",
   "metadata": {},
   "source": [
    "## 5️⃣ 🎛️ Sin / Cos Uygulama ve Positional Encoding Oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38a117",
   "metadata": {},
   "source": [
    "#### 🔎 Satır Satır Açıklama:\n",
    "🧪 1. sines = sin(angle_rads[:, 0::2])\n",
    "\n",
    "* Her kelime için çift numaralı boyutlara sinüs uygula\n",
    "\n",
    "* Yani: 0, 2, 4, 6, 8, ...\n",
    "\n",
    "🧊 2. cosines = cos(angle_rads[:, 1::2])\n",
    "\n",
    "* Her kelime için tek numaralı boyutlara kosinüs uygula\n",
    "\n",
    "* Yani: 1, 3, 5, 7, 9, ...\n",
    "\n",
    "🎯 Sin/cos karışımı → faz farkı ekleyerek zenginleştirilmiş sinyal yapısı oluşturur\n",
    "(sin ve cos aynı frekansta ama 90 derece kayık = sinyal farklarını büyütüyor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d66285",
   "metadata": {},
   "source": [
    "#### 🔗 3. tf.concat([sines, cosines], axis=-1)\n",
    "* sines ve cosines dizilerini yan yana birleştir\n",
    "\n",
    "* Sonuç = [max_len, d_model] boyutunda positional sinyal tablosu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fa28a",
   "metadata": {},
   "source": [
    "#### 🧱 4. return pos_encoding[tf.newaxis, ...]\n",
    "* Bu satırla shape şu hale gelir: [1, max_len, d_model]\n",
    "\n",
    "* Yani bu encoding, batch’e uygun hale gelir ve direkt embedding ile toplanabilir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50074bd",
   "metadata": {},
   "source": [
    "## 🧩 Positional Encoding – Adım Adım Özet\n",
    "\n",
    "| Adım                  | Ne yaptı?                                                                 |\n",
    "|-----------------------|---------------------------------------------------------------------------|\n",
    "| `pos`                | Her kelimenin cümledeki sırasını aldı                                     |\n",
    "| `i`                  | Embedding boyutlarının indekslerini oluşturdu                             |\n",
    "| `angle_rates`        | Her embedding boyutu için frekans hesaplama katsayısı belirledi            |\n",
    "| `pos * angle_rates`  | Her pozisyon-boyut kombinasyonu için açısal değerler hesapladı             |\n",
    "| `sin / cos`          | Sinyalleri faz farkıyla dönüştürerek pozisyonel bilgi kazandırdı           |\n",
    "| `concat`             | Sin ve cos verilerini birleştirerek final sinyal vektörünü oluşturdu       |\n",
    "| `+ embedding`        | Embedding vektörüne pozisyon bilgisini ekledi → model sıraları “görür” ✅   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7489ee",
   "metadata": {},
   "source": [
    "## 📐 Positional Encoding Son Demler\n",
    "\n",
    "- Positional Encoding, aslında **embedding katmanına eklenen sinyaldir**.\n",
    "- Model sırayı bilemediği için, bu sinyalle **kelimenin kaçıncı sırada olduğunu** anlar.\n",
    "- İlk olarak, cümledeki **her pozisyonu (`pos`)** temsil eden bir vektör oluşturduk.\n",
    "- Ardından, her **embedding boyutu (`d_model`)** için bir boyut vektörü (`i`) oluşturduk.\n",
    "- Sonra bu boyutlara göre **açısal frekans katsayıları (`angle_rates`)** belirledik.\n",
    "- Bu frekanslar ile pozisyonları çarparak **açısal değerler (`angle_rads`)** elde ettik.\n",
    "- Daha sonra çift indeksli boyutlara `sin`, tek indeksli boyutlara `cos` uyguladık.\n",
    "- Bu `sin` ve `cos` sinyallerini birleştirerek **pozisyonel sinyal vektörü** oluşturduk.\n",
    "- En sonunda bu vektörü embedding çıktısıyla toplayarak **modelin sıralamayı \"görmesini\" sağladık**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828e26d",
   "metadata": {},
   "source": [
    "-----------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f324e",
   "metadata": {},
   "source": [
    "# Şimdi Positionel Encodingden Sonra Gelen Embedding Katmanını İnceleyelim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5000c",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece8b00",
   "metadata": {},
   "source": [
    "#   ✅ Embedding + Positional Encoding Katmanı (Fonksiyon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ab427",
   "metadata": {},
   "source": [
    "* Aslında embedding katmanını biliyoruz.Metinsel değerlerde kullandığımız bir vektör dönüştürücü.Bundna önce de size zaten positionel encoding i anlatmıştım.Şimdi bu positionel encoding katmaını embedding e bağlamamız gerekecek.Gelin bu işlemlere bakalım.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f923be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Add, Lambda\n",
    "\n",
    "def token_and_position_embedding(vocab_size, max_len, d_model):\n",
    "    inputs = Input(shape=(None,), name=\"input_tokens\")\n",
    "\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=d_model)(inputs)\n",
    "\n",
    "    # positional encoding önceden sabit oluşturulmuş (max_len kadar)\n",
    "    pos_encoding = positionel_encoding(max_len, d_model)\n",
    "\n",
    "    # Lambda ile slice işlemini runtime’da yapıyoruz\n",
    "    def slice_position(x_input):\n",
    "        seq_len = tf.shape(x_input)[1]\n",
    "        return pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    pos_slice = Lambda(slice_position)(x)\n",
    "\n",
    "    x = Add()([x, pos_slice])\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x, name=\"TokenPosEmbedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d7767",
   "metadata": {},
   "source": [
    "### 📌 Kullanımı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "800d12db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hdgn5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoder_inputs bizim encoder için Input katmanımız\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(None,), name=\"encoder_input\")\n",
    "embedding_layer = token_and_position_embedding(\n",
    "    vocab_size=input_vocab_size, max_len=max_len, d_model=128\n",
    ")\n",
    "\n",
    "# çağırdığımızda sanki bir layer gibi davranır\n",
    "x = embedding_layer(encoder_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a3dbbf",
   "metadata": {},
   "source": [
    "--------\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fcc6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Şimdi Sırada Encoder Yapısı Var.Merak Etmeyin Bu Fonksiyonları Daha Güçlü Hale Getireceğiz.Bu Yalnızca Bir Başlangıçç..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb6dc8",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fa54c",
   "metadata": {},
   "source": [
    "## 🏗️ build_encoder() Fonksiyonu — Tek Blok Encoder Yapısı\n",
    "\n",
    "####  🧱 build_encoder() Fonksiyonunun Amacı\n",
    "\n",
    "- Bu fonksiyon, Transformer mimarisindeki **Encoder bloğunu** oluşturur.\n",
    "- Encoder, verilen token dizisinden (yani giriş cümlesinden), **sıralı ve bağlama duyarlı bir vektör temsili** üretir.\n",
    "- İçerisinde:\n",
    "  - Embedding + Positional Encoding (kelimenin anlamı + sırası)\n",
    "  - Multi-Head Self Attention (kelimeler arası ilişki öğrenimi)\n",
    "  - Feed Forward Network (her pozisyonu bireysel işler)\n",
    "  - Residual bağlantılar + Layer Normalization (öğrenmeyi stabilize eder)\n",
    "- Modelin giriş kısmında (input side) yer alır ve decoder’a bilgi sağlar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248693c",
   "metadata": {},
   "source": [
    "## 🔧 Feed Forward Network (FFN) Nedir?\n",
    "\n",
    "- Transformer’daki her encoder/decoder bloğunda bulunan küçük sinir ağıdır.\n",
    "- Her kelime pozisyonuna **ayrı ayrı** uygulanır.\n",
    "- Yapısı genellikle:\n",
    "\n",
    "  Dense(d_model * 4, activation='relu') → Dense(d_model)\n",
    "  \n",
    "- Attention çıktısını işler, modeli derinleştirir ve anlam çıkarım gücünü artırır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97007334",
   "metadata": {},
   "source": [
    "#### 🎯 FFN nedir, ne işe yarar?\n",
    "* Transformer içinde her bir pozisyon (token) için ayrı ayrı uygulanan küçük bir sinir ağıdır.\n",
    "\n",
    "* Yani: Kelime başına “bireysel olarak düşünme” mekanizmasıdır 🤖🧠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafe3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(d_model * 4, activation='relu')(x)\n",
    "x = Dense(d_model)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eafe069",
   "metadata": {},
   "source": [
    "Bu şu anlama gelir:\n",
    "\n",
    "* İlk katman: Genişlet → d_model → 4 × d_model\n",
    "\n",
    "* ReLU aktivasyonu: doğrusal olmayanlık katar\n",
    "\n",
    "* İkinci katman: Daralt → 4 × d_model → d_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_encoder(vocab_size, max_len, d_model, num_heads):\n",
    "    # 1️⃣ Giriş katmanı\n",
    "    encoder_inputs = layers.Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "    # 2️⃣ Embedding + Positional Encoding (hazır fonksiyon)\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(encoder_inputs)\n",
    "\n",
    "    # 3️⃣ Multi-Head Self-Attention\n",
    "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "\n",
    "    # 4️⃣ Residual + Layer Normalization\n",
    "    x = layers.Add()([x, attn_out])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # 5️⃣ Feed Forward Network\n",
    "    ffn = layers.Dense(d_model * 4, activation='relu')(x)\n",
    "    ffn = layers.Dense(d_model)(ffn)\n",
    "\n",
    "    # 6️⃣ Residual + Layer Normalization\n",
    "    x = layers.Add()([x, ffn])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # 7️⃣ Modeli döndür\n",
    "    return Model(inputs=encoder_inputs, outputs=x, name=\"TransformerEncoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfaf8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerEncoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerEncoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">55,296</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,200,960</span> │ TokenPosEmbeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m55,296\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m4,200,960\u001b[0m │ TokenPosEmbeddin… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m1,049,088\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,358,016</span> (24.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,358,016\u001b[0m (24.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,358,016</span> (24.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,358,016\u001b[0m (24.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 📌 Kullanımı:\n",
    "\n",
    "encoder = build_encoder(\n",
    "    vocab_size=input_vocab_size,\n",
    "    max_len=max_len,\n",
    "    d_model=512,\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ff495",
   "metadata": {},
   "source": [
    "### 🎯 Self-Attention Neydi?\n",
    "* Bir kelimenin diğer kelimelere olan bağlamını öğrenmesi.\n",
    "\n",
    "Örnek:\n",
    "\n",
    "* “Kedi koltuğun üstünde uyuyor.”\n",
    "\n",
    "* “kedi” → dikkatini “uyuyor” kelimesine vermeli\n",
    "\n",
    "* “koltuğun” → konum bilgisiyle ilişkili\n",
    "\n",
    "🧠 Self-attention bunu yapar:\n",
    "\n",
    "* Her kelime, diğer kelimelerle olan ilişkisini kendisi çözer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b7a1c",
   "metadata": {},
   "source": [
    "## 🤔 Peki neden Multi-Head Attention?\n",
    "\n",
    "### 🔍 1. Farklı Bakış Açıları Sağlar\n",
    "\n",
    "Her “head” = farklı bir öğrenme perspektifi sağlar.\n",
    "\n",
    "#### 🧠 Örnek:\n",
    "\n",
    "| Head 1              | Head 2               | Head 3                    |\n",
    "|---------------------|----------------------|---------------------------|\n",
    "| dilbilgisel ilişki  | duygusal tonlama     | zamansal bağlam          |\n",
    "| \"özne-fiil\" ilişkisi | \"sevgi/korku\" analizi | \"önceki/sonraki kelime\" ilişkisi |\n",
    "\n",
    "Tek bir self-attention bu bağlamları aynı anda yakalayamaz.  \n",
    "Ama birden fazla attention head, **paralel zihinler gibi** çalışarak her biri farklı bir bilgi yönüne odaklanabilir. 🧠🧠🧠\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240008b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🔍 2. Farklı Projeksiyon Uzaylarında Hesaplama\n",
    "\n",
    "Her head:\n",
    "- Kendi `Wq`, `Wk`, `Wv` ağırlıklarını kullanır\n",
    "- Yani aynı input üzerinden farklı projeksiyonlar oluşturur\n",
    "- Bu, daha **geniş ilişki çeşitliliği** öğrenmesini sağlar\n",
    "\n",
    "\n",
    "\n",
    "### 🔍 3. Zenginleştirilmiş Temsil\n",
    "\n",
    "Tüm head'lerin çıktısı birleştirilir (`concat`) ve ardından bir `Dense` katmanla orijinal boyuta dönülür:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50d7e3",
   "metadata": {},
   "source": [
    "\n",
    "Bu sayede farklı bağlamlar içeren çıktılar, tek bir güçlü vektörde toplanır.\n",
    "\n",
    "\n",
    "### ✅ Özet\n",
    "\n",
    "| Özellik                  | Tek Head             | Multi-Head                         |\n",
    "|--------------------------|----------------------|------------------------------------|\n",
    "| Perspektif               | Tek bakış açısı      | Farklı bakış açılarının birleşimi  |\n",
    "| Hesaplama                | Sabit projeksiyon    | Farklı ağırlıklarla farklı dikkat  |\n",
    "| Öğrenme kapasitesi       | Sınırlı              | Daha esnek ve güçlü                |\n",
    "| Bağlam çeşitliliği       | Düşük                | Yüksek                             |\n",
    "\n",
    "Multi-head attention sayesinde model, dilin çok katmanlı doğasını daha iyi kavrayabilir. 🔥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772ce1d",
   "metadata": {},
   "source": [
    "## 🧱 Profesyonel build_encoder() — Temiz ve Katmanlaştırılmış"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e9d935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization, MultiHeadAttention\n",
    "\n",
    "\n",
    "def add_and_norm(x,sublayer_output,dp=0.3):\n",
    "    dropped = layers.Dropout(dp)(sublayer_output)\n",
    "    added = layers.Add()([x,dropped])\n",
    "\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(added)\n",
    "\n",
    "def feed_forward_network(d_model):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(d_model * 4, activation='relu'),\n",
    "        layers.Dense(d_model),\n",
    "        layers.Dropout(0.3)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17c0e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x, d_model, num_heads, dropout_rate=0.1):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    x = add_and_norm(x, attn_output, dp=dropout_rate)\n",
    "\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(d_model * 4, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(d_model)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = add_and_norm(x, ffn_output, dp=dropout_rate)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cbc9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(vocab_size, max_len, d_model, num_heads, num_layers=1, dropout_rate=0.1):\n",
    "    encoder_inputs = layers.Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(encoder_inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_block(x, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    return Model(inputs=encoder_inputs, outputs=x, name=\"TransformerEncoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b062887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerEncoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerEncoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,648</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ TokenPosEmbeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,904</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_3        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │     \u001b[38;5;34m27,648\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,051,904\u001b[0m │ TokenPosEmbeddin… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,051,904\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,051,904\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequential_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m1,051,904\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_3        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequential_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m512\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,341,632</span> (24.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,341,632\u001b[0m (24.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,341,632</span> (24.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,341,632\u001b[0m (24.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = build_encoder(\n",
    "    vocab_size=input_vocab_size,\n",
    "    max_len=max_len,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4  # ya da kaç katman görmek istiyorsan\n",
    ")\n",
    "\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe267f0",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5293e20",
   "metadata": {},
   "source": [
    "## 🔁 Hatırlatma: Transformer Decoder Ne Yapar?\n",
    "### 📤 Encoder’dan gelen bilgileri kullanır\n",
    "### 🧠 Kendi geçmiş çıktılarıyla yeni kelime tahmini yapar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3e009",
   "metadata": {},
   "source": [
    "#### 🔧 Yapısı:\n",
    "Her decoder bloğu şunları içerir:\n",
    "\n",
    "* Masked Multi-Head Self Attention\n",
    "\n",
    "→ Kendi önceki çıktılara bakar (geleceği görmez ❌)\n",
    "\n",
    "* Encoder–Decoder Attention\n",
    "\n",
    "→ Encoder’ın çıktısına dikkat eder\n",
    "\n",
    "* Feed Forward Network\n",
    "\n",
    "→ Kelimeyi anlam olarak işler\n",
    "\n",
    "* Residual + LayerNorm\n",
    "\n",
    "→ Her adımda stabilite sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a11cca",
   "metadata": {},
   "source": [
    "### 🔍 1️⃣ Decoder’da Neden Yine Multi-Head Attention?\n",
    "Çünkü decoder da:\n",
    "\n",
    "* Kelimeler arasındaki ilişkileri kurmak istiyor\n",
    "\n",
    "* Ama bunu paralel ve bağlamlı şekilde yapmak istiyor\n",
    "\n",
    "* Aynı encoder gibi, decoder da:\n",
    "\n",
    "* Kendi geçmiş çıktılarının bağlamını anlamaya çalışır\n",
    "\n",
    "* Ve bunu birden fazla “bakış açısıyla” yaparsa çok daha zengin temsil oluşur\n",
    "\n",
    "### 🎯 Multi-head ne sağlar?\n",
    "\n",
    "* Head 1: dilbilgisel yapı\n",
    "\n",
    "* Head 2: geçmiş anahtar kelimeye dikkat\n",
    "\n",
    "* Head 3: zaman/zaman dışı bağlantı\n",
    "\n",
    "Hepsi birleştirilir = çok katmanlı anlam çözümü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbeb02e",
   "metadata": {},
   "source": [
    "### 📚 GPT-2, GPT-3, ChatGPT hepsi ne kullanıyor?\n",
    "* Transformer decoder only\n",
    "\n",
    "* Hepsi multi-head self-attention kullanıyor\n",
    "\n",
    "Çünkü bu:\n",
    "\n",
    "* Paralel\n",
    "\n",
    "* Dikkatli\n",
    "\n",
    "* Ölçeklenebilir\n",
    "\n",
    "Her kelimenin diğerleriyle bağını çoklu seviyede kurabiliyor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c8ff1",
   "metadata": {},
   "source": [
    "### ✅ 1️⃣ decoder_block() Fonksiyonu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f70b2",
   "metadata": {},
   "source": [
    "* Bunu önceki encoder yapısına göre tanımlıyoruz. 2 Attention + 1 FFN + residual + dropout + layernorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x,enc_output,d_model,num_heads,dp=0.3):\n",
    "\n",
    "    attn1 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=d_model)(x,x)\n",
    "\n",
    "    x = add_and_norm(x,attn1,dp=dp)\n",
    "\n",
    "    attn2 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=d_model)(x,enc_output)\n",
    "    x = add_and_norm(x,attn2,dp = dp)\n",
    "\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(d_model * 4,activation=\"relu\"),\n",
    "        layers.Dropout(dp),\n",
    "        layers.Dense(d_model)\n",
    "        ])\n",
    "    \n",
    "    ffn_out = ffn(x)\n",
    "    x = add_and_norm(x,ffn_out,dp=dp)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34db3a",
   "metadata": {},
   "source": [
    "### ✅ 2️⃣ build_decoder() Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(vocab_size, max_len, d_model, num_heads, num_layers=4, dropout_rate=0.1):\n",
    "    decoder_inputs = layers.Input(shape=(None,), name=\"decoder_input\")\n",
    "    encoder_outputs = layers.Input(shape=(None, d_model), name=\"encoder_output\")  # DİKKAT!\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(decoder_inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = decoder_block(x, encoder_outputs, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    return Model(inputs=[decoder_inputs, encoder_outputs], outputs=x, name=\"TransformerDecoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33643fd3",
   "metadata": {},
   "source": [
    "#### 📌 Kullanımı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90171ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerDecoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerDecoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,648</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ TokenPosEmbeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_102         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ dropout_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_104         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_17       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_106         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_108         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_110         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_18       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_112         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_112[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_114         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_114[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_116         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_19       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_118         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_120         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_122         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_20       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_124         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_126         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,728</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_128         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_21       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_130         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TokenPosEmbedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m27,648\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ TokenPosEmbeddin… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ TokenPosEmbeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_102         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_62 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ TokenPosEmbeddin… │\n",
       "│                     │                   │            │ dropout_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_104         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_63 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_17       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_106         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_17[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_64 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_108         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_65 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_110         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_66 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_18       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_112         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_18[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_67 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_112[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_114         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_68 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_114[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_116         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_69 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_19       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_118         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_19[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_70 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_120         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_71 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_122         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_72 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_20       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_124         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_20[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_73 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_126         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_74 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m1,577,728\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_128         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_75 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_21       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_130         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ sequential_21[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_76 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ add_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,440,448</span> (70.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,440,448\u001b[0m (70.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,440,448</span> (70.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,440,448\u001b[0m (70.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder = build_decoder(vocab_size=input_vocab_size,\n",
    "                        max_len=max_len,\n",
    "                        d_model = 256,\n",
    "                        num_heads= 6,\n",
    "                        num_layers= 5)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3027c5",
   "metadata": {},
   "source": [
    "----------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ad714",
   "metadata": {},
   "source": [
    "## 🎯 Encoder + Decoder birleşimiyle tam bir Transformer modeli kuruyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a631ad1",
   "metadata": {},
   "source": [
    "Bu, eğitimde kullanılacak olan \"tam model\" olacak.\n",
    "\n",
    "* Encoder input → Encoder\n",
    "\n",
    "* Decoder input → Decoder\n",
    "\n",
    "Son olarak:\n",
    "\n",
    "* Decoder'ın çıktısı → Dense(vocab_size) → tahmin edilen kelime dizis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540598f",
   "metadata": {},
   "source": [
    "### 🧱 1️⃣ build_transformer() Fonksiyonu 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_vocab_size,target_vocab_size,max_len,d_model,num_heads,num_layers,dp=0.3):\n",
    "    \n",
    "    encoder_inputs = layers.Input(shape=(None,) ,name=\"encoder_inputs\")\n",
    "    decoder_inputs = layers.Input(shape=(None,) ,name =\"decoder_inputs\")\n",
    "\n",
    "    encoder = build_encoder(input_vocab_size,max_len,d_model,num_heads,num_layers,dp)\n",
    "    decoder = build_decoder(target_vocab_size,max_len,d_model,num_heads,num_layers,dp)\n",
    "\n",
    "    enc_output = encoder(encoder_inputs)\n",
    "    dec_output = decoder([decoder_inputs,enc_output])\n",
    "\n",
    "    final_outputs = layers.Dense(target_vocab_size,activation=\"softmax\" , name=\"output_layer\")(dec_output)\n",
    "\n",
    "    return Model(inputs=[encoder_inputs,decoder_inputs],outputs =final_outputs , name=\"TransformerModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fef62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,591,808</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,648,064</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,740</span> │ TransformerDecod… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │  \u001b[38;5;34m1,591,808\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │  \u001b[38;5;34m2,648,064\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)     │      \u001b[38;5;34m7,740\u001b[0m │ TransformerDecod… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,247,612</span> (16.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,247,612\u001b[0m (16.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,247,612</span> (16.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,247,612\u001b[0m (16.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = build_transformer(\n",
    "    input_vocab_size=60,\n",
    "    target_vocab_size=60,\n",
    "    max_len=max_len,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4\n",
    ")\n",
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e207d5",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c03d9e",
   "metadata": {},
   "source": [
    "# ⚙️ Loss + optimizer fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    loss = loss_object(y_true, y_pred)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "# 🔧 Modeli compile et\n",
    "transformer.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b3d9b",
   "metadata": {},
   "source": [
    "--------\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc23b9",
   "metadata": {},
   "source": [
    "## MODEL TANIMLAMALARI İÇİN BÜTÜN KODLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf1057",
   "metadata": {},
   "source": [
    "### 🔧 1️⃣ encoder_block + build_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df58b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x, d_model, num_heads, dropout_rate=0.4):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    x = add_and_norm(x, attn_output, dp=dropout_rate)\n",
    "\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(d_model * 5, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(d_model)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = add_and_norm(x, ffn_output, dp=dropout_rate)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_encoder(vocab_size, max_len, d_model, num_heads, num_layers=3, dropout_rate=0.4):\n",
    "    encoder_inputs = layers.Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(encoder_inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_block(x, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    return Model(inputs=encoder_inputs, outputs=x, name=\"TransformerEncoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075281e1",
   "metadata": {},
   "source": [
    "### 🔧 2️⃣ decoder_block + build_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "187c2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x, enc_output, d_model, num_heads, dropout_rate=0.4):\n",
    "    attn1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    x = add_and_norm(x, attn1, dp=dropout_rate)\n",
    "\n",
    "    attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, enc_output)\n",
    "    x = add_and_norm(x, attn2, dp=dropout_rate)\n",
    "\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(d_model * 5, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(d_model)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = add_and_norm(x, ffn_output, dp=dropout_rate)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_decoder(vocab_size, max_len, d_model, num_heads, num_layers=4, dropout_rate=0.4):\n",
    "    decoder_inputs = layers.Input(shape=(None,), name=\"decoder_input\")\n",
    "    encoder_outputs = layers.Input(shape=(None, d_model), name=\"encoder_output\")\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(decoder_inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = decoder_block(x, encoder_outputs, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    return Model(inputs=[decoder_inputs, encoder_outputs], outputs=x, name=\"TransformerDecoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a94b0",
   "metadata": {},
   "source": [
    "### 🧱 3️⃣ build_transformer (Encoder + Decoder Birleşimi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f365dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_vocab_size, target_vocab_size, max_len, d_model, num_heads, num_layers=4, dropout_rate=0.4):\n",
    "    encoder_inputs = layers.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "    decoder_inputs = layers.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "\n",
    "    encoder = build_encoder(input_vocab_size, max_len, d_model, num_heads, num_layers, dropout_rate)\n",
    "    decoder = build_decoder(target_vocab_size, max_len, d_model, num_heads, num_layers, dropout_rate)\n",
    "\n",
    "    enc_output = encoder(encoder_inputs)\n",
    "    dec_output = decoder([decoder_inputs, enc_output])\n",
    "\n",
    "    final_output = layers.Dense(target_vocab_size, activation=\"softmax\", name=\"output_layer\")(dec_output)\n",
    "\n",
    "    return Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_output, name=\"TransformerModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c443aa",
   "metadata": {},
   "source": [
    "#### 📌 Örnek Çağırma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9f6e070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,286,592</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16,637,696</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,507</span> │ TransformerDecod… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │ \u001b[38;5;34m10,286,592\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │ \u001b[38;5;34m16,637,696\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m251\u001b[0m)    │     \u001b[38;5;34m64,507\u001b[0m │ TransformerDecod… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,988,795</span> (102.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,988,795\u001b[0m (102.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,988,795</span> (102.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,988,795\u001b[0m (102.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "transformer = build_transformer(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_len=max_len,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=6,\n",
    "    dropout_rate=0.4\n",
    ")\n",
    "\n",
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df774828",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    loss = loss_object(y_true, y_pred)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "# 🔧 Modeli compile et\n",
    "transformer.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39666389",
   "metadata": {},
   "source": [
    "------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229a24e",
   "metadata": {},
   "source": [
    "# Şimdi de Eğitim Zamanı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae3117",
   "metadata": {},
   "source": [
    "* Model sonucuna takılmayın arkadaşlar.Veri seti 1. aşama olarak 50 veriden oluşuyor ki bu zaten öğrenme konusunda hiçbir fayda sağlamaz.Biz burada temel yapıları çalıştık.Modelin sonucu bizi ilgilendirmiyor.Biz burada işlemleri anlamaya ve teorik olarak oturtmaya çalışıyoruz.İlerleyen zamanlarda projeler yapa yapa sonuca doğru gideceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b698fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.0100 - loss: 5.8811 - val_accuracy: 0.0833 - val_loss: 6.6894\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665ms/step - accuracy: 0.0812 - loss: 5.5551 - val_accuracy: 0.0833 - val_loss: 6.7695\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 688ms/step - accuracy: 0.0833 - loss: 5.4373 - val_accuracy: 0.0833 - val_loss: 5.7509\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 678ms/step - accuracy: 0.0721 - loss: 5.3819 - val_accuracy: 0.0833 - val_loss: 5.5772\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671ms/step - accuracy: 0.0741 - loss: 5.4143 - val_accuracy: 0.0833 - val_loss: 5.6213\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 662ms/step - accuracy: 0.0779 - loss: 5.3910 - val_accuracy: 0.0833 - val_loss: 5.7628\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 708ms/step - accuracy: 0.0830 - loss: 5.2922 - val_accuracy: 0.0833 - val_loss: 5.7985\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 709ms/step - accuracy: 0.0812 - loss: 5.2403 - val_accuracy: 0.0833 - val_loss: 5.7230\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 731ms/step - accuracy: 0.0788 - loss: 5.1795 - val_accuracy: 0.0833 - val_loss: 5.6535\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 703ms/step - accuracy: 0.0854 - loss: 5.2014 - val_accuracy: 0.0833 - val_loss: 5.6431\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 710ms/step - accuracy: 0.0821 - loss: 5.2418 - val_accuracy: 0.0833 - val_loss: 5.6691\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 745ms/step - accuracy: 0.0833 - loss: 5.1543 - val_accuracy: 0.0833 - val_loss: 5.7014\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 705ms/step - accuracy: 0.0875 - loss: 5.1567 - val_accuracy: 0.0833 - val_loss: 5.7177\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 787ms/step - accuracy: 0.0854 - loss: 5.1314 - val_accuracy: 0.0833 - val_loss: 5.7380\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 707ms/step - accuracy: 0.0825 - loss: 5.1747 - val_accuracy: 0.0833 - val_loss: 5.7295\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 741ms/step - accuracy: 0.0800 - loss: 5.1950 - val_accuracy: 0.0833 - val_loss: 5.7154\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 717ms/step - accuracy: 0.0858 - loss: 5.1302 - val_accuracy: 0.0833 - val_loss: 5.7096\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 766ms/step - accuracy: 0.0833 - loss: 5.1256 - val_accuracy: 0.0833 - val_loss: 5.7165\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 711ms/step - accuracy: 0.0758 - loss: 5.1717 - val_accuracy: 0.0833 - val_loss: 5.7529\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 770ms/step - accuracy: 0.0825 - loss: 5.0968 - val_accuracy: 0.0833 - val_loss: 5.7703\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 825ms/step - accuracy: 0.0842 - loss: 5.0977 - val_accuracy: 0.0833 - val_loss: 5.7829\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 814ms/step - accuracy: 0.0825 - loss: 5.0824 - val_accuracy: 0.0833 - val_loss: 5.7920\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 861ms/step - accuracy: 0.0884 - loss: 5.0954 - val_accuracy: 0.0833 - val_loss: 5.7891\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 796ms/step - accuracy: 0.0854 - loss: 5.0726 - val_accuracy: 0.0833 - val_loss: 5.7923\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 892ms/step - accuracy: 0.0854 - loss: 5.0247 - val_accuracy: 0.0833 - val_loss: 5.8152\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 776ms/step - accuracy: 0.0779 - loss: 5.1049 - val_accuracy: 0.1167 - val_loss: 5.8472\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 850ms/step - accuracy: 0.0846 - loss: 5.0280 - val_accuracy: 0.1333 - val_loss: 5.8684\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 695ms/step - accuracy: 0.0900 - loss: 5.0207 - val_accuracy: 0.1167 - val_loss: 5.8865\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 702ms/step - accuracy: 0.0879 - loss: 4.9800 - val_accuracy: 0.0833 - val_loss: 5.9019\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 725ms/step - accuracy: 0.0875 - loss: 4.9678 - val_accuracy: 0.0833 - val_loss: 5.9180\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 717ms/step - accuracy: 0.0858 - loss: 5.0065 - val_accuracy: 0.0833 - val_loss: 5.9497\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 700ms/step - accuracy: 0.0879 - loss: 5.0127 - val_accuracy: 0.0833 - val_loss: 5.9693\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 704ms/step - accuracy: 0.0900 - loss: 4.9863 - val_accuracy: 0.1000 - val_loss: 6.0291\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 766ms/step - accuracy: 0.0972 - loss: 4.9617 - val_accuracy: 0.1000 - val_loss: 6.1007\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 719ms/step - accuracy: 0.0888 - loss: 5.0149 - val_accuracy: 0.1167 - val_loss: 6.1161\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 853ms/step - accuracy: 0.0854 - loss: 4.9528 - val_accuracy: 0.1167 - val_loss: 6.0711\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 730ms/step - accuracy: 0.0875 - loss: 4.9214 - val_accuracy: 0.1500 - val_loss: 6.0086\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 720ms/step - accuracy: 0.0951 - loss: 4.9469 - val_accuracy: 0.1500 - val_loss: 6.0191\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 750ms/step - accuracy: 0.0914 - loss: 4.9539 - val_accuracy: 0.1333 - val_loss: 6.0914\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 734ms/step - accuracy: 0.0909 - loss: 4.9691 - val_accuracy: 0.1167 - val_loss: 6.1602\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 705ms/step - accuracy: 0.0963 - loss: 4.9860 - val_accuracy: 0.1000 - val_loss: 6.1798\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 704ms/step - accuracy: 0.0930 - loss: 4.8987 - val_accuracy: 0.1000 - val_loss: 6.1814\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 772ms/step - accuracy: 0.0917 - loss: 4.9326 - val_accuracy: 0.1000 - val_loss: 6.1571\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671ms/step - accuracy: 0.1051 - loss: 4.9195 - val_accuracy: 0.0833 - val_loss: 6.1429\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 723ms/step - accuracy: 0.0930 - loss: 4.8817 - val_accuracy: 0.1000 - val_loss: 6.1758\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 708ms/step - accuracy: 0.0997 - loss: 4.8547 - val_accuracy: 0.1000 - val_loss: 6.2019\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 694ms/step - accuracy: 0.1023 - loss: 4.8523 - val_accuracy: 0.1000 - val_loss: 6.2580\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 694ms/step - accuracy: 0.1014 - loss: 4.8196 - val_accuracy: 0.1333 - val_loss: 6.3275\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 782ms/step - accuracy: 0.1005 - loss: 4.8205 - val_accuracy: 0.1333 - val_loss: 6.3667\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816ms/step - accuracy: 0.0972 - loss: 4.8262 - val_accuracy: 0.1500 - val_loss: 6.3639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ae90799820>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(\n",
    "    [encoder_input, decoder_input],\n",
    "    decoder_target,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.1  # istersen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c775e",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "\n",
    "## Şimdi Modeli Deneyelim\n",
    "\n",
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438ed63",
   "metadata": {},
   "source": [
    "### ✅ 1️⃣ evaluate_sentence() Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cecf78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence, tokenizer_in, tokenizer_out, transformer, max_len):\n",
    "    # 1. Girdiyi tokenize et\n",
    "    input_seq = tokenizer_in.texts_to_sequences([sentence])\n",
    "    encoder_input = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    # 2. Decoder'ı <start> token ile başlat\n",
    "    start_token = tokenizer_out.word_index.get('<start>', 1)\n",
    "    end_token = tokenizer_out.word_index.get('<end>', 2)\n",
    "\n",
    "    decoder_input = [start_token]\n",
    "    result = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Padle\n",
    "        dec_in = pad_sequences([decoder_input], maxlen=max_len, padding='post')\n",
    "\n",
    "        # Model tahmini\n",
    "        predictions = transformer.predict([encoder_input, dec_in], verbose=0)\n",
    "        predicted_id = tf.argmax(predictions[0, len(decoder_input)-1]).numpy()\n",
    "\n",
    "        # <end> geldiyse dur\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "\n",
    "        result.append(predicted_id)\n",
    "        decoder_input.append(predicted_id)\n",
    "\n",
    "    # ID → kelime çevir\n",
    "    reverse_target_word_index = {i: w for w, i in tokenizer_out.word_index.items()}\n",
    "    predicted_sentence = ' '.join([reverse_target_word_index.get(i, '?') for i in result])\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c2609",
   "metadata": {},
   "source": [
    "#### 📌 Kullanımı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9680811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: bir bir bir bir\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Merhaba\"\n",
    "output = evaluate_sentence(test_input, input_tokenizer, target_tokenizer, transformer, max_len=max_len)\n",
    "print(\"Bot:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a357f",
   "metadata": {},
   "source": [
    "------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11c32b",
   "metadata": {},
   "source": [
    "# 🧠 MASKING – Transformer’da Ne İşe Yarar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c26e1c",
   "metadata": {},
   "source": [
    "#### Transformer paralel çalıştığı için:\n",
    "\n",
    "* Tüm cümle bir anda işlenir\n",
    "\n",
    "Ama bazı yerleri gizlemek gerekir:\n",
    "\n",
    "* PAD tokenları (eğitimi bozmasın diye)\n",
    "\n",
    "* Decoder'da gelecekteki kelimeler (daha yazılmamışsa bakmamalı)\n",
    "\n",
    "İşte bu durumda devreye \"masking\" girer ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b84be8",
   "metadata": {},
   "source": [
    "## 🎭 MASKING TÜRLERİ\n",
    "#### 1️⃣ Padding Mask (Gerçek cümle ≠ pad token)\n",
    "\n",
    "* PAD token'lar model için bilgi taşımaz\n",
    "\n",
    "* Hem encoder hem decoder'da kullanılır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ab669",
   "metadata": {},
   "source": [
    "Cümle:      merhaba nasılsın <pad <pad\n",
    "\n",
    "Mask:       1          1         0     0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4f57ac",
   "metadata": {},
   "source": [
    "### 2️⃣ Look-Ahead Mask (Causal Masking)\n",
    "* Decoder kendinden sonraki kelimeye bakamaz\n",
    "\n",
    "* Çünkü henüz yazılmamıştır\n",
    "\n",
    "* Yani decoder_input[t] → sadece [:t]’ye kadar bakar\n",
    "\n",
    "Örn (3 kelime için):\n",
    "\n",
    "1 0 0\n",
    "\n",
    "1 1 0\n",
    "\n",
    "1 1 1\n",
    "\n",
    "* Bu bir üst üçgen matris'tir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada28603",
   "metadata": {},
   "source": [
    "### 3️⃣ Combined Mask\n",
    "* Padding + look-ahead birlikte\n",
    "\n",
    "* Decoder’ın hem pad’leri hem geleceği görmemesini sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248816e",
   "metadata": {},
   "source": [
    "## 🔧 Nerelerde Kullanılır?\n",
    "| Maske Türü     | Kullanıldığı Yer             |\n",
    "|----------------|-------------------------------|\n",
    "| Padding Mask   | Encoder & Decoder Attention   |\n",
    "| Look-Ahead     | Decoder Self-Attention        |\n",
    "| Combined Mask  | Decoder’ın giriş kısmında     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc9838",
   "metadata": {},
   "source": [
    "### 💡 Neden Bu Kadar Önemli?\n",
    "❌ Eğer mask yapılmazsa:\n",
    "\n",
    "* Pad token’lar sıfır olduğu halde öğrenme hatası oluşur\n",
    "\n",
    "* Decoder ilerideki kelimeyi görerek “hile yapar”\n",
    "\n",
    "✅ Mask → Doğru öğrenmeyi garanti eder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed395e53",
   "metadata": {},
   "source": [
    "\n",
    "### Ama asıl kritik soru şu:\n",
    "⚙️ \"Transformer’ın neresine bu mask’leri enjekte edeceğiz?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77c55d",
   "metadata": {},
   "source": [
    "## 🧠 Özet: Masking'i Nerede Kullanıyoruz?\n",
    "\n",
    "| Nerede?                       | Ne Veriyoruz?                   |\n",
    "|-------------------------------|----------------------------------|\n",
    "| 🔹 Encoder Self-Attention     | `attention_mask = padding_mask` |\n",
    "| 🔹 Decoder Self-Attention     | `attention_mask = combined_mask`|\n",
    "| 🔹 Decoder Enc-Dec Attention  | `attention_mask = padding_mask` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f606e8",
   "metadata": {},
   "source": [
    "### Şimdi kodlamaya geçelim.Ama unutmayın.Biz bu mask işlemlerinin hepsini uyguluyoruz.Yalnızca yukarıda duran kavramlara göre yapacağız.Yani farklı katmanlara enjekte edeceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9dc0a7",
   "metadata": {},
   "source": [
    "## 🔹 1️⃣ create_padding_mask() – TEORİ\n",
    "Ne yapar?\n",
    "\n",
    "* Giriş tensöründe (örneğin [5, 7, 0, 0])\n",
    "\n",
    "* 0 olanları tespit eder (bunlar <pad>)\n",
    "\n",
    "* Bunları 1 yapar → yani maskelenecek yer\n",
    "\n",
    "* Ardından True → 1.0, False → 0.0 olarak float mask üretir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a31ed928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # PAD token = 0 → True olur\n",
    "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # [batch, seq_len] → [batch, 1, 1, seq_len]\n",
    "    # Bu, MultiHeadAttention için beklenen shape’tir\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2dccf",
   "metadata": {},
   "source": [
    "#### 📌 Örnek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4983eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[0. 0. 1. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq = tf.constant([[7, 6, 0, 0, 0]])\n",
    "mask = create_padding_mask(seq)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51171d61",
   "metadata": {},
   "source": [
    "#### 🔍 Çıktı:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a254ae",
   "metadata": {},
   "source": [
    "\n",
    "[[[[0. 0. 1. 1. 1.]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe72ae",
   "metadata": {},
   "source": [
    "* Yani 0 olan yerlere dikkat edilmesin dedik ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219588b",
   "metadata": {},
   "source": [
    "## 🔹 2️⃣ create_look_ahead_mask() – TEORİ\n",
    "Amaç:\n",
    "\n",
    "* Decoder kendi cümlesini üretirken gelecekteki kelimelere bakmamalı\n",
    "\n",
    "* Yani t=3 anındayken sadece 0, 1, 2 pozisyonlarını görmeli\n",
    "\n",
    "* Bu, causal (nedensel) attention sağlar → kelime sırasına sadık kalınır ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ded9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    # tf.ones((size, size)) → [size x size] boyutunda sadece 1'lerden oluşan matris üretir\n",
    "    # Örn: size=4 ise → [[1 1 1 1], [1 1 1 1], [1 1 1 1], [1 1 1 1]]\n",
    "\n",
    "    # tf.linalg.band_part(tensor, num_lower, num_upper):\n",
    "    # - 'band_part' fonksiyonu matrisi üçgen hale getirir\n",
    "    # - num_lower=-1 → alttaki tüm satırları tut\n",
    "    # - num_upper=0  → sadece diagonal ve alt taraf kalır, üst kısmı sıfırlar\n",
    "\n",
    "    # tf.linalg.band_part(tf.ones(...), -1, 0)\n",
    "    # → Alt üçgen matris üretir:\n",
    "    # [[1 0 0 0]\n",
    "    #  [1 1 0 0]\n",
    "    #  [1 1 1 0]\n",
    "    #  [1 1 1 1]]\n",
    "\n",
    "    # 1 - (...) → üst üçgeni 1 yapar:\n",
    "    # [[0 1 1 1]\n",
    "    #  [0 0 1 1]\n",
    "    #  [0 0 0 1]\n",
    "    #  [0 0 0 0]]\n",
    "\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "\n",
    "    return mask  # shape: [size, size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94aa59",
   "metadata": {},
   "source": [
    "### 📌 Örnek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20e5d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = create_look_ahead_mask(5)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af2953",
   "metadata": {},
   "source": [
    "* Yani pozisyon t yalnızca 0:t aralığını görebilir.\n",
    "\n",
    "* Yukarısı 1 → gizlenecek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd2931",
   "metadata": {},
   "source": [
    "## 🔗 Combined Mask = Padding Mask + Look-Ahead Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0310c56",
   "metadata": {},
   "source": [
    "* Bu maskeyi decoder’ın self-attention kısmında kullanıyoruz.\n",
    "\n",
    "Amaç:\n",
    "\n",
    "🔒 Geleceğe bakamasın (look-ahead)\n",
    "\n",
    "🧼 Pad token'lara dikkat etmesin (padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9af30",
   "metadata": {},
   "source": [
    "### 🧠 Neden Combine Ediyoruz?\n",
    "* Tek başına look_ahead_mask:\n",
    "\n",
    "Sadece geleceği gizler, ama pad’leri görür ❌\n",
    "\n",
    "* Tek başına padding_mask:\n",
    "\n",
    "Pad’leri gizler, ama geleceğe bakar ❌\n",
    "\n",
    "* Bunları tf.maximum() ile birleştirince:\n",
    "\n",
    "Neresi 1 ise orayı gizle → hem pad hem gelecek ✔️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44613355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_mask(decoder_input):\n",
    "\n",
    "    look_ahead = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
    "\n",
    "    padding = create_padding_mask(decoder_input)\n",
    "\n",
    "    look_ahead = tf.maximum(look_ahead,padding[:,:,0:])\n",
    "\n",
    "    return look_ahead[:,tf.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803c0db",
   "metadata": {},
   "source": [
    "### Yukarıda bulunan kodlarla temel masking bilgisini anlatmaya çalıştım.Şimdi bunları nasıl modele aktaracağımıza bakalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac5027",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a02a9",
   "metadata": {},
   "source": [
    "## Hazırladığımız padding_mask, look_ahead_mask, combined_mask fonksiyonlarınışimdi encoder_block ve decoder_block'a entegre ediyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ab056",
   "metadata": {},
   "source": [
    "### 🧱 1️⃣ encoder_block – Mask Eklemesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da57255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x, d_model, num_heads, dropout_rate, padding_mask=None):\n",
    "    attn_output = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model\n",
    "    )(x, x, attention_mask=padding_mask)  # Maske burada kullanılıyor\n",
    "\n",
    "    x = add_and_norm(x, attn_output, dp=dropout_rate)\n",
    "\n",
    "    ffn = tf.keras.Sequential([\n",
    "        Dense(d_model * 4, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(d_model)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = add_and_norm(x, ffn_output, dp=dropout_rate)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac69d6d",
   "metadata": {},
   "source": [
    "### 🔧 1️⃣ build_encoder() – Mask Entegreli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6bf8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(vocab_size, max_len, d_model, num_heads, num_layers=1, dropout_rate=0.1):\n",
    "    encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(encoder_inputs)\n",
    "\n",
    "    # Padding mask (mask shape: [batch, 1, 1, seq_len])\n",
    "    padding_mask = Lambda(lambda seq: create_padding_mask(seq))(encoder_inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_block(x, d_model, num_heads, dropout_rate, padding_mask=padding_mask)\n",
    "\n",
    "    return Model(inputs=encoder_inputs, outputs=x, name=\"TransformerEncoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2327dbc6",
   "metadata": {},
   "source": [
    "### 🔁 2️⃣ decoder_block – 2 Maske Eklemesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74197d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x, enc_output, d_model, num_heads,\n",
    "                  dropout_rate=0.1, combined_mask=None, padding_mask=None):\n",
    "    # 1. Decoder Self-Attention (geleceği görememeli)\n",
    "    attn1 = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model\n",
    "    )(x, x, attention_mask=combined_mask)  # ✅ Look-Ahead + Padding mask\n",
    "\n",
    "    x = add_and_norm(x, attn1, dp=dropout_rate)\n",
    "\n",
    "    # 2. Encoder-Decoder Attention (pad'leri gizle)\n",
    "    attn2 = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model\n",
    "    )(x, enc_output, attention_mask=padding_mask)  # ✅ Padding mask\n",
    "\n",
    "    x = add_and_norm(x, attn2, dp=dropout_rate)\n",
    "\n",
    "    # 3. Feed-Forward\n",
    "    ffn = tf.keras.Sequential([\n",
    "        Dense(d_model * 4, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(d_model)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = add_and_norm(x, ffn_output, dp=dropout_rate)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc7dd2",
   "metadata": {},
   "source": [
    "### 🔁 2️⃣ build_decoder() – Full Mask Entegreli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0a01c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(vocab_size, max_len, d_model, num_heads, num_layers=1, dropout_rate=0.1):\n",
    "    decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "    encoder_outputs = Input(shape=(None, d_model), name=\"encoder_output\")\n",
    "\n",
    "    embedding_layer = token_and_position_embedding(vocab_size, max_len, d_model)\n",
    "    x = embedding_layer(decoder_inputs)\n",
    "\n",
    "    # Maskleri oluştur\n",
    "    padding_mask = Lambda(lambda seq: create_padding_mask(seq))(decoder_inputs)\n",
    "    look_ahead = Lambda(lambda seq: create_look_ahead_mask(tf.shape(seq)[1]))(decoder_inputs)\n",
    "    combined_mask = Lambda(lambda args: tf.maximum(args[0], args[1]))([look_ahead, padding_mask])\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = decoder_block(\n",
    "            x,\n",
    "            encoder_outputs,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout_rate,\n",
    "            combined_mask=combined_mask,\n",
    "            padding_mask=padding_mask\n",
    "        )\n",
    "\n",
    "    return Model(inputs=[decoder_inputs, encoder_outputs], outputs=x, name=\"TransformerDecoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba438aa8",
   "metadata": {},
   "source": [
    "### 🧱 build_transformer() – Mask Entegreli Final Sürüm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2733109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_vocab_size, target_vocab_size, max_len,\n",
    "                      d_model, num_heads, num_layers=1, dropout_rate=0.1):\n",
    "    \n",
    "    # Girişler\n",
    "    encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "    decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "\n",
    "    # Encoder ve Decoder modellerini oluştur\n",
    "    encoder = build_encoder(input_vocab_size, max_len, d_model, num_heads, num_layers, dropout_rate)\n",
    "    decoder = build_decoder(target_vocab_size, max_len, d_model, num_heads, num_layers, dropout_rate)\n",
    "\n",
    "    # Encoder output\n",
    "    enc_output = encoder(encoder_inputs)\n",
    "\n",
    "    # Decoder output (encoder output'u da alır)\n",
    "    dec_output = decoder([decoder_inputs, enc_output])\n",
    "\n",
    "    # Final Dense layer: her timestep'te vocab boyutunda softmax\n",
    "    final_output = Dense(target_vocab_size, activation='softmax', name=\"output_layer\")(dec_output)\n",
    "\n",
    "    # Modeli tanımla\n",
    "    return Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_output, name=\"TransformerModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73842d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TransformerModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TransformerModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,763,136</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,956,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,507</span> │ TransformerDecod… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerEncoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m4,763,136\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ TransformerDecoder  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m7,956,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ TransformerEncod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m251\u001b[0m)    │     \u001b[38;5;34m64,507\u001b[0m │ TransformerDecod… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,784,635</span> (48.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,784,635\u001b[0m (48.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,784,635</span> (48.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,784,635\u001b[0m (48.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = build_transformer(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    max_len=max_len,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed2551",
   "metadata": {},
   "source": [
    "### ✅ 1️⃣ Loss Fonksiyonu – PAD maskeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be0b86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction='none'  # PAD için maskeleme uygulayacağız\n",
    ")\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)  # PAD'ler 0'dır → onları dışla\n",
    "    loss = loss_object(y_true, y_pred)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99867d4f",
   "metadata": {},
   "source": [
    "### ⚙️ 2️⃣ Optimizer - Compile Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd513067",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "transformer.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68779eda",
   "metadata": {},
   "source": [
    "### 📦 4️⃣ Eğitime Başla (fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "322de4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.0012 - loss: 5.9597 - val_accuracy: 0.0833 - val_loss: 5.6882\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379ms/step - accuracy: 0.0339 - loss: 5.7190 - val_accuracy: 0.0833 - val_loss: 6.4323\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346ms/step - accuracy: 0.0728 - loss: 5.5923 - val_accuracy: 0.0833 - val_loss: 7.1825\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - accuracy: 0.0833 - loss: 5.4467 - val_accuracy: 0.0833 - val_loss: 7.2245\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341ms/step - accuracy: 0.0846 - loss: 5.4262 - val_accuracy: 0.0833 - val_loss: 6.7729\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - accuracy: 0.0800 - loss: 5.3585 - val_accuracy: 0.0833 - val_loss: 6.3168\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - accuracy: 0.0842 - loss: 5.3457 - val_accuracy: 0.1167 - val_loss: 6.0718\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - accuracy: 0.0749 - loss: 5.2766 - val_accuracy: 0.1500 - val_loss: 5.9738\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428ms/step - accuracy: 0.0804 - loss: 5.3280 - val_accuracy: 0.1500 - val_loss: 5.9410\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step - accuracy: 0.0674 - loss: 5.3824 - val_accuracy: 0.1333 - val_loss: 5.9640\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411ms/step - accuracy: 0.0737 - loss: 5.2775 - val_accuracy: 0.1167 - val_loss: 6.0302\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - accuracy: 0.0863 - loss: 5.2653 - val_accuracy: 0.0833 - val_loss: 6.1324\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - accuracy: 0.0791 - loss: 5.2206 - val_accuracy: 0.0833 - val_loss: 6.2359\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384ms/step - accuracy: 0.0800 - loss: 5.1896 - val_accuracy: 0.0833 - val_loss: 6.2834\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396ms/step - accuracy: 0.0879 - loss: 5.1529 - val_accuracy: 0.0833 - val_loss: 6.2542\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397ms/step - accuracy: 0.0909 - loss: 5.2347 - val_accuracy: 0.0833 - val_loss: 6.1930\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393ms/step - accuracy: 0.0854 - loss: 5.2163 - val_accuracy: 0.0833 - val_loss: 6.1265\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390ms/step - accuracy: 0.0833 - loss: 5.1164 - val_accuracy: 0.1000 - val_loss: 6.0659\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386ms/step - accuracy: 0.0896 - loss: 5.1771 - val_accuracy: 0.1167 - val_loss: 6.0321\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - accuracy: 0.0854 - loss: 5.1463 - val_accuracy: 0.1167 - val_loss: 6.0416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2af454434d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(\n",
    "    [encoder_input, decoder_input],\n",
    "    decoder_target,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.1  # istersen ayır\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80f061",
   "metadata": {},
   "source": [
    "-------\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff154c",
   "metadata": {},
   "source": [
    "# Şimdi bütün bu öğrendiğimiz terimleri bir koda geçireceğiz.Temel yapı taşlarını oturttuk şimdi önemli olan bu kodları iyice teorikle beraber anlamak.Burda bahsedilmek istenilen kod parçalarına (( transformer_1_kodları.ipynb ))  içerisinden ulaşabilirsiniz.Karmaşıklığı engellemek için oradan inceleyiniz.\n",
    "\n",
    "-----\n",
    "--------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bbb08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
