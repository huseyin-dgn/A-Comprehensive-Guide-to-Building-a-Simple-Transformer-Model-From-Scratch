{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf7e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938021ed",
   "metadata": {},
   "source": [
    "# TOKENÄ°ZER - TENSORFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ebf0b",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbfa4a",
   "metadata": {},
   "source": [
    "## ğŸ”¤ Tokenizer Nedir?\n",
    "\n",
    "**Tokenizer**, ham metni modelin anlayabileceÄŸi sayÄ±sal dizilere dÃ¶nÃ¼ÅŸtÃ¼ren araÃ§tÄ±r. Her kelimeye (veya karaktere) bir ID atar.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ Temel GÃ¶revi\n",
    "- Metni bÃ¶lmek (tokenize etmek)\n",
    "- Her tokena bir numara vermek (vocab oluÅŸturmak)\n",
    "- Bu numaralara gÃ¶re diziler Ã¼retmek\n",
    "\n",
    "\n",
    "\n",
    "### âš™ï¸ KullanÄ±m Parametreleri\n",
    "\n",
    "| Parametre     | AÃ§Ä±klama |\n",
    "|---------------|----------|\n",
    "| `oov_token`   | EÄŸitimde gÃ¶rÃ¼lmeyen kelimeler iÃ§in Ã¶zel token (`<OOV>`) |\n",
    "| `filters`     | Hangi karakterlerin silineceÄŸi (Ã¶rn. noktalama) |\n",
    "| `char_level`  | `True`: karakter bazlÄ±, `False`: kelime bazlÄ± tokenize |\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§± AdÄ±mlar\n",
    "\n",
    "1. **Tokenizer oluÅŸtur**  \n",
    "   ```python\n",
    "   tokenizer = Tokenizer(oov_token=\"<OOV>\", filters=\"\", char_level=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4e0c1",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13222e",
   "metadata": {},
   "source": [
    "## ğŸ”‘ SOS ve EOS TokenlarÄ± Nedir?\n",
    "\n",
    "**SOS (Start of Sequence)** ve **EOS (End of Sequence)** tokenlarÄ±, dizilerle Ã§alÄ±ÅŸan modellerde giriÅŸ ve Ã§Ä±kÄ±ÅŸ dizilerinin sÄ±nÄ±rlarÄ±nÄ± belirlemek iÃ§in kullanÄ±lan Ã¶zel iÅŸaretleyicilerdir.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ AmaÃ§larÄ±\n",
    "- **sos**: Modelin dizinin **baÅŸladÄ±ÄŸÄ±nÄ±** anlamasÄ±nÄ± saÄŸlar.\n",
    "- **eos**: Modelin dizinin **bittiÄŸini** anlamasÄ±nÄ± saÄŸlar.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“¦ KullanÄ±m AlanlarÄ±\n",
    "- **Seq2Seq modellerinde** (Ã¶rneÄŸin: encoder-decoder yapÄ±larÄ±)\n",
    "- **Makine Ã§evirisi**\n",
    "- **Dil modeli eÄŸitimi ve tahmini**\n",
    "- **Beam Search gibi sÄ±ralÄ± Ã¼retim algoritmalarÄ±nda** Ã¼retimin ne zaman duracaÄŸÄ±nÄ± belirlemek iÃ§in\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§± Tipik Uygulama Åekli\n",
    "**Hedef metinler** ÅŸu ÅŸekilde hazÄ±rlanÄ±r:\n",
    "\n",
    "- `target_input`: `sos` + gerÃ§ek cÃ¼mle  \n",
    "- `target_output`: gerÃ§ek cÃ¼mle + `eos`\n",
    "\n",
    "Bu sayede model:\n",
    "- Ne zaman Ã¼retmeye baÅŸlayacaÄŸÄ±nÄ± (sos),\n",
    "- Ne zaman durmasÄ± gerektiÄŸini (eos) Ã¶ÄŸrenmiÅŸ olur.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§  Ã–rnek\n",
    "GerÃ§ek cÃ¼mle: `Merhaba dÃ¼nya`\n",
    "\n",
    "- `target_input`: `sos Merhaba dÃ¼nya`  \n",
    "- `target_output`: `Merhaba dÃ¼nya eos`\n",
    "\n",
    "\n",
    "### ğŸ’¡ Not\n",
    "- Bu tokenlar, tokenizer'a *manÃ¼el* olarak metnin bir parÃ§asÄ± gibi eklenmelidir.\n",
    "- Tokenizer, onlarÄ± kelime listesine dahil ederek ID atar (Ã¶rneÄŸin: `sos â†’ 2`).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a2e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NasÄ±lsÄ±n?</td>\n",
       "      <td>Ä°yiyim, teÅŸekkÃ¼r ederim. Siz nasÄ±lsÄ±nÄ±z?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdÄ±n ne?</td>\n",
       "      <td>Ben bir yapay zekÃ¢ asistanÄ±yÄ±m. AdÄ±m yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KaÃ§ yaÅŸÄ±ndasÄ±n?</td>\n",
       "      <td>Benim yaÅŸÄ±m yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BugÃ¼n gÃ¼nlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?\n",
       "1            NasÄ±lsÄ±n?           Ä°yiyim, teÅŸekkÃ¼r ederim. Siz nasÄ±lsÄ±nÄ±z?\n",
       "2             AdÄ±n ne?  Ben bir yapay zekÃ¢ asistanÄ±yÄ±m. AdÄ±m yok ama y...\n",
       "3      KaÃ§ yaÅŸÄ±ndasÄ±n?                        Benim yaÅŸÄ±m yok, dijitalim!\n",
       "4  BugÃ¼n gÃ¼nlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da620d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['input'].astype(str).to_list()\n",
    "target_texts = df['output'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31202ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "token_in = ['<start> ' + text for text in target_texts]\n",
    "token_out = [text + ' <end>' for text in target_texts]\n",
    "\n",
    "input_token = Tokenizer(filters='' , oov_token='<OOV>')\n",
    "input_token.fit_on_texts(input_texts)\n",
    "\n",
    "target_token = Tokenizer(filters='' , oov_token='<OOV>')\n",
    "target_token.fit_on_texts(token_in + token_out)\n",
    "\n",
    "input_seq = input_token.texts_to_sequences(input_texts)\n",
    "decoder_in = target_token.texts_to_sequences(token_in)\n",
    "decoder_out = target_token.texts_to_sequences(token_out)\n",
    "\n",
    "max_len_inp = max(len(seq) for seq in input_seq)\n",
    "max_len_out = max(len(seq) for seq in decoder_in + decoder_out)\n",
    "\n",
    "max_len = max(max_len_inp , max_len_out)\n",
    "\n",
    "encoder_input = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "decoder_input = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "decoder_target = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "\n",
    "inp_vocab_size = len(input_token.word_index) + 1 \n",
    "tar_vocab_size = len(target_token.word_index) + 1\n",
    "\n",
    "print(inp_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738418d",
   "metadata": {},
   "source": [
    "* YukarÄ±da bulunan bu kod klasik bir yapÄ±.Bizim amacÄ±mÄ±z bu yapÄ±ya daha fazla derinlik katmak.Normal tokenizer iÅŸlevlerine zaten hakimiz.Yani bu yapÄ±yÄ± kÃ¼Ã§Ã¼k-orta Ã¶lÃ§ekli yerlerde kullanabilirsiniz.Gayet de iÅŸ gÃ¶rÃ¼r.Ama bu notebookdaki temel amaÃ§ Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± daha ciddi yerlere getirmek.Gelin bu yapÄ±lar neymiÅŸ onlara bakalÄ±m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3f3e2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695dc1a",
   "metadata": {},
   "source": [
    "## ğŸ“Œ Neler Eklenebilir ?\n",
    "\n",
    "### 1. `char_level`  \n",
    "**Ne yapar?**  \n",
    "- Tokenizerâ€™Ä± kelime (`word`) yerine karakter (`char`) bazlÄ± Ã§alÄ±ÅŸacak ÅŸekilde ayarlar.\n",
    "\n",
    "**Neden Ã¶nemli?**  \n",
    "- Ã‡ok kÃ¼Ã§Ã¼k veri setlerinde kelime bazlÄ± model **aÅŸÄ±rÄ± â€œsparseâ€** (seyreklik) yaÅŸayabilir.  \n",
    "- Yeni (out-of-vocab) kelimelerin tamamÄ±nÄ± `<OOV>` yerine, karakter dizisi sayesinde anlamlÄ± parÃ§alara ayÄ±rabilirsiniz.  \n",
    "- Morfolojik aÃ§Ä±dan zengin (Ã¶rneÄŸin TÃ¼rkÃ§e) dillerde **kÃ¶k-ek ayrÄ±mÄ±** yapmak kolaylaÅŸÄ±r.\n",
    "\n",
    "\n",
    "### 2. `max_vocab_size`  \n",
    "**Ne yapar?**  \n",
    "- Tokenizerâ€™a kaÃ§ adet en sÄ±k gÃ¶rÃ¼len tokenâ€™Ä± â€œaktifâ€ tutacaÄŸÄ±nÄ± sÃ¶ylersiniz; geriye kalanlar hepsi `<OOV>` olur.\n",
    "\n",
    "**Neden Ã¶nemli?**  \n",
    "- **Bellek & hÄ±z**: Gigabaytlarca sÃ¶zlÃ¼ÄŸÃ¼ modelde tutmak yerine, en sÄ±k kullanÄ±lan 5kâ€“10k tokenâ€™Ä± alÄ±p gerisini atmak.  \n",
    "- **Genelleme**: AÅŸÄ±rÄ± nadir kelimeler eÄŸitim sÄ±rasÄ±nda gÃ¼rÃ¼ltÃ¼ oluÅŸturabilir; onlarÄ± `<OOV>` yapmak modelin daha saÄŸlam Ã¶ÄŸrenmesini saÄŸlar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4a9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ''\n",
    "oov_token = '<OOV>'\n",
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "char_level = False\n",
    "max_vocab_size = 10000\n",
    "padding=\"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6ccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_in = [f\"{start_token} {t}\" for t in target_texts]\n",
    "token_out = [f\"{t} {end_token}\" for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5799eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = Tokenizer(\n",
    "    num_words=max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    "                        )\n",
    "\n",
    "target_token = Tokenizer(\n",
    "    num_words=max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472afef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token.fit_on_texts(input_texts)\n",
    "target_token.fit_on_texts(token_in+token_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3621febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = input_token.texts_to_sequences(input_texts)\n",
    "decoder_in = target_token.texts_to_sequences(token_in)\n",
    "decoder_out = target_token.texts_to_sequences(token_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0428553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "max_len_inp = max(len(seq) for seq in input_seq)\n",
    "max_len_out = max(len(seq) for seq in decoder_in + decoder_out)\n",
    "max_len = max(max_len_inp , max_len_out)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7799c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(input_seq,maxlen = max_len, padding=padding)\n",
    "decoder_input = pad_sequences(decoder_in,maxlen = max_len, padding=padding)\n",
    "decoder_target = pad_sequences(decoder_out,maxlen = max_len, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b273f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = max(len(input_token.word_index) + 1 , max_vocab_size or float(\"inf\"))\n",
    "tar_vocab_size = max(len(target_token.word_index) + 1 , max_vocab_size or float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93236c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size:  10000\n",
      "Target vocab size: 10000\n",
      "<start> token id: 2\n",
      "<end> token id: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: \", inp_vocab_size)\n",
    "print(\"Target vocab size:\", tar_vocab_size)\n",
    "print(f\"{start_token} token id:\", target_token.word_index[start_token])\n",
    "print(f\"{end_token} token id:\", target_token.word_index[end_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1372fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girdi Ã¶rneÄŸi (IDsâ†’text): merhaba\n",
      "Decoder-in Ã¶rneÄŸi: <start> merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?\n",
      "Decoder-out Ã¶rneÄŸi: merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim? <end>\n",
      "TemizlenmiÅŸ target: merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?\n"
     ]
    }
   ],
   "source": [
    "# ==== TOKENIZER SONRASI HIZLI KONTROL ====\n",
    "\n",
    "# 1) Ham ID dizilerini direkt geri metne Ã§evir:\n",
    "sample_idx = 0  # ilk Ã¶rneÄŸi incele\n",
    "print(\"Girdi Ã¶rneÄŸi (IDsâ†’text):\",\n",
    "      input_token.sequences_to_texts([ input_seq[sample_idx] ])[0])\n",
    "print(\"Decoder-in Ã¶rneÄŸi:\",\n",
    "      target_token.sequences_to_texts([ decoder_in[sample_idx] ])[0])\n",
    "print(\"Decoder-out Ã¶rneÄŸi:\",\n",
    "      target_token.sequences_to_texts([ decoder_out[sample_idx] ])[0])\n",
    "\n",
    "# 2) <start> ve <end>'i temizleyip nihai cÃ¼mleyi al:\n",
    "raw = target_token.sequences_to_texts([ decoder_out[sample_idx] ])[0]\n",
    "clean = raw.replace(f\"{start_token} \", \"\").replace(f\" {end_token}\", \"\").strip()\n",
    "print(\"TemizlenmiÅŸ target:\", clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f85d9e",
   "metadata": {},
   "source": [
    "----\n",
    "## ğŸš€ Profesyonel Tokenizer Pipeline Ä°yileÅŸtirmeleri\n",
    "\n",
    "AÅŸaÄŸÄ±da â€œsadece Ã§alÄ±ÅŸÄ±p geÃ§mekâ€ yerine **gÃ¼venilir**, **Ã¶lÃ§eklenebilir** ve **esnek** bir preprocessing pipelineâ€™Ä± oluÅŸturmak iÃ§in baÅŸlÄ±ca adÄ±mlar Ã¶zetlenmiÅŸtir.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Temizlik & Normalizasyon\n",
    "- **Unicode Normalization** (`NFC`/`NFKC`):  \n",
    "  FarklÄ± biÃ§imlerde kodlanmÄ±ÅŸ karakterleri tekilleÅŸtirir.\n",
    "- **Lowercasing**:  \n",
    "  TÃ¼m metni kÃ¼Ã§Ã¼k harfe indirerek sÃ¶zlÃ¼kteki duplikasyonlarÄ± Ã¶nler.\n",
    "- **Diakritik/KarakÂ­ter Temizleme**:  \n",
    "  TÃ¼rkÃ§e â€œÃ§, ÅŸ, Ä±â€ gibi iÅŸaretleri gerektiÄŸinde standartlaÅŸtÄ±rÄ±r.\n",
    "\n",
    "\n",
    "### 2. Ã–zel Tokenâ€™lar & Sabitleme\n",
    "- **\\<pad\\>** tokenâ€™Ä±nÄ± aÃ§Ä±kÃ§a tanÄ±mlayÄ±p indexâ€™ini sÄ±fÄ±r yapÄ±n.  \n",
    "- **\\<unk\\>** veya **\\<OOV\\>** farkÄ±nÄ± belirleyin ve sabitleyin.  \n",
    "- `fit_on_texts` Ã¶ncesi Ã¶zel tokenâ€™larÄ± manuel ekleyerek aynÄ± IDâ€™leri garantileyin.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Vocab Analizi & SÄ±nÄ±rlandÄ±rma\n",
    "- **max_vocab_size**: En sÄ±k kullanÄ±lan N tokenâ€™Ä± aktif tutar, gerisi `<OOV>`.  \n",
    "- **min_freq**: Belirli bir frekans altÄ±ndaki tokenâ€™larÄ± da otomatik atar.  \n",
    "- **Coverage Raporu**: Toplam tokenlarÄ±n % kaÃ§Ä± top-N vocabâ€™a giriyor?  \n",
    "\n",
    "\n",
    "\n",
    "### 4. Subword / BPE / WordPiece DesteÄŸi\n",
    "- **SentencePiece** veya **HuggingFace Tokenizers** kullanarak:  \n",
    "  - Byte-Pair Encoding (BPE)  \n",
    "  - Unigram LM  \n",
    "  - WordPiece  \n",
    "- BÃ¶ylece OOV sorunu en aza iner, vocab boyutu kÃ¼Ã§Ã¼lÃ¼r.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Parametrik & ModÃ¼ler YapÄ±\n",
    "- Expose edilecek parametreler:  \n",
    "  - `lowercase: bool`  \n",
    "  - `normalize_unicode: bool`  \n",
    "  - `filters: str`  \n",
    "  - `min_freq: int`  \n",
    "  - `max_vocab_size: int`  \n",
    "  - `char_level: bool`  \n",
    "  - `tokenizer_backend: \"keras\" | \"sentencepiece\" | \"hf\"`  \n",
    "\n",
    "\n",
    "\n",
    "### 6. Kaydetme / Versiyonlama / Geri YÃ¼kleme\n",
    "- **JSON + Metadata** (parametreler, min_freq vs.) bir arada saklayÄ±n.  \n",
    "- Versiyonlama: `tokenizer_v1.0.json`, `tokenizer_v1.1.json`â€¦  \n",
    "- AynÄ± pipelineâ€™Ä± projeler arasÄ± tekrar kullanmak iÃ§in.\n",
    "\n",
    "\n",
    "\n",
    "### 7. Performans & Ã–lÃ§eklenebilirlik\n",
    "- **Rust-tabanlÄ±** tokenizers (HuggingFace) ile paralel hÄ±z.  \n",
    "- **Disk cache**: TFRecord/LMDB gibi formatlarda tokenize edilmiÅŸ veriyi saklama.  \n",
    "- `tf.data.Dataset` iÃ§inde â€œcache â†’ map â†’ batch â†’ prefetchâ€ akÄ±ÅŸÄ±.\n",
    "\n",
    "\n",
    "\n",
    "### 8. Debug & Ä°zlenebilirlik\n",
    "- **Sample Ã‡Ä±ktÄ±lar**: Her adÄ±mda 3â€“5 Ã¶rnek  \n",
    "- **Logging**:  \n",
    "  - Toplam kelime sayÄ±sÄ± vs. vocab boyutu  \n",
    "  - Padding/truncation daÄŸÄ±lÄ±mÄ±  \n",
    "- **Unit Test**:  \n",
    "  - Edge-case cÃ¼mleler  \n",
    "  - Ã–zel token ID kontrolÃ¼  \n",
    "  - Truncation & padding davranÄ±ÅŸÄ±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7fab7",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098de60",
   "metadata": {},
   "source": [
    "### 1. Lowercasing ve Unicode normalize etmek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790de84",
   "metadata": {},
   "source": [
    "* Basit bir clean_text() fonksiyonu yazÄ±p, fit_on_texts Ã§aÄŸrÄ±sÄ±ndan Ã¶nce tÃ¼m metinleri ona geÃ§iriyoruz.\n",
    "\n",
    "* Bunu tokenizerlarÄ±n en baÅŸÄ±na ekliyoruz.\n",
    "\n",
    "* BÃ¶ylece hem bÃ¼tÃ¼n metinler aynÄ± formata gelir hem tokenizerâ€™a â€œgereksizâ€ karakterler girmez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63dee88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_texts(text,lowercase=True,normalize_unicod = True):\n",
    "\n",
    "    if normalize_unicod:\n",
    "        text = unicodedata.normalize(\"NFC\",text)\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s]\", \" \", text)\n",
    "    # birden fazla arka arkaya boÅŸluÄŸu tek boÅŸluk yap\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "input_texts = [clean_texts(t) for t in df[\"input\"].astype(str)]\n",
    "target_texts = [clean_texts(t) for t in df[\"output\"].astype(str)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8577566",
   "metadata": {},
   "source": [
    "---\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24648604",
   "metadata": {},
   "source": [
    "## Ã–zel Tokenâ€™lar & Sabitleme (Teori)\n",
    "\n",
    "### Neden Ã–zel Tokenâ€™lar?\n",
    "- **\\<pad\\>**  \n",
    "  - Dizilerin sabit uzunluÄŸa getirilmesi iÃ§in kullanÄ±lan dolgu (padding) tokenâ€™Ä±.  \n",
    "  - Modelin gerÃ§ek veriden ayÄ±rt edebilmesi iÃ§in Ã¶zel bir IDâ€™ye sahip olmalÄ±dÄ±r.\n",
    "\n",
    "- **\\<unk\\> / \\<OOV\\>**  \n",
    "  - EÄŸitim sÄ±rasÄ±nda gÃ¶rÃ¼lmeyen veya nadir kelimeleri temsil eder.  \n",
    "  - Bilinmeyen kelimelerin modelde tutarlÄ± ÅŸekilde iÅŸlenmesini saÄŸlar.\n",
    "\n",
    "- **\\<sos\\> (Start of Sequence)**  \n",
    "  - Decoderâ€™Ä±n diziyi nereden Ã¼retmeye baÅŸlayacaÄŸÄ±nÄ± belirtir.  \n",
    "  - SÄ±ralÄ± Ã¼retim algoritmalarÄ±nda mutlaka ilk token olarak kullanÄ±lÄ±r.\n",
    "\n",
    "- **\\<eos\\> (End of Sequence)**  \n",
    "  - Decoderâ€™Ä±n Ã¼retimi nerede durduracaÄŸÄ±nÄ± iÅŸaretler.  \n",
    "  - Ã–zellikle beam search veya greedy decoding sÄ±rasÄ±nda Ã¶nem taÅŸÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "### Sabitlemenin AmacÄ±\n",
    "1. **TutarlÄ±lÄ±k**  \n",
    "   - AynÄ± token her Ã§alÄ±ÅŸmada aynÄ± IDâ€™ye karÅŸÄ±lÄ±k gelmeli.  \n",
    "   - Ã–zellikle eÄŸitim ve Ã§Ä±karÄ±m (inference) ortamlarÄ± arasÄ±nda uyumluluk saÄŸlar.\n",
    "\n",
    "2. **Padding GÃ¼venliÄŸi**  \n",
    "   - \\<pad\\> tokenâ€™Ä±nÄ±n **ID = 0** olmasÄ±, `pad_sequences` gibi fonksiyonlarÄ±n varsayÄ±lan dolgu deÄŸeriyle Ã¶rtÃ¼ÅŸmesini garantiler.\n",
    "\n",
    "3. **OOV YÃ¶netimi**  \n",
    "   - \\<unk\\> tokenâ€™Ä±nÄ±n sabit bir IDâ€™ye sahip olmasÄ±, bilinmeyen kelimelerin daima aynÄ± ÅŸekilde kodlanmasÄ±nÄ± saÄŸlar ve modelin genel performansÄ±nÄ± korur.\n",
    "\n",
    "4. **Decoder KontrolÃ¼**  \n",
    "   - \\<sos\\> ve \\<eos\\> tokenâ€™larÄ±nÄ±n IDâ€™leri deÄŸiÅŸmediÄŸi sÃ¼rece, dizinin baÅŸlangÄ±Ã§ ve bitiÅŸ sÄ±nÄ±rlarÄ± model tarafÄ±ndan kesinlikle doÄŸru algÄ±lanÄ±r.\n",
    "\n",
    "\n",
    "### Sabitleme AdÄ±mlarÄ± (Ã–zet)\n",
    "- Ã–zel tokenâ€™larÄ± iÃ§eren bir sabit eÅŸleme (`word â†’ ID`) tanÄ±mlanÄ±r.  \n",
    "- Tokenizer baÅŸlatÄ±ldÄ±ktan hemen sonra bu sabit eÅŸleme yÃ¼klenir, bÃ¶ylece Ã¶zel tokenâ€™lar kesin IDâ€™lere sahip olur.  \n",
    "- ArdÄ±ndan gerÃ§ek metin verisi ile tokenizasyon iÅŸlemi yapÄ±lÄ±r; Ã¶zel tokenâ€™larÄ±n IDâ€™leri hiÃ§bir zaman deÄŸiÅŸmez.\n",
    "\n",
    "\n",
    "### FaydalarÄ±\n",
    "- Modelin hem eÄŸitim hem de Ã§Ä±karÄ±m aÅŸamasÄ±nda **Ã¶ngÃ¶rÃ¼lebilir ve tutarlÄ±** davranmasÄ±nÄ± saÄŸlar.  \n",
    "- **Kodun bakÄ±mÄ±** ve **yeniden Ã¼retilebilirlik** (reproducibility) aÃ§Ä±sÄ±ndan kritik bir uygulamadÄ±r.  \n",
    "- SÄ±ralÄ± metin Ã¼retimi (seq2seq, Transformer) gibi ileri dÃ¼zey gÃ¶revlerde hata yapmayÄ± engeller.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189db7d",
   "metadata": {},
   "source": [
    "## `min_freq` (Minimum Frequency) Teorisi\n",
    "\n",
    "### Neden Gerekli?\n",
    "- **GÃ¼rÃ¼ltÃ¼ Azaltma**  \n",
    "  - Veri setindeki bazÄ± kelimeler yalnÄ±zca birkaÃ§ kez geÃ§er; bu nadir kelimeler modelin Ã¶ÄŸrenme sÃ¼recini olumsuz etkileyebilir.  \n",
    "- **Vocab Boyutunu Kontrol Etme**  \n",
    "  - BÃ¼yÃ¼k metin koleksiyonlarÄ±nda onbinlerce farklÄ± token olabilir.  \n",
    "  - SÃ¶zlÃ¼ÄŸÃ¼ kÃ¼Ã§Ã¼ltmek, hem bellek kullanÄ±mÄ±nÄ± hem de eÄŸitim sÃ¼resini iyileÅŸtirir.\n",
    "\n",
    "\n",
    "### NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "1. **Frekans EÅŸiÄŸi**  \n",
    "   - Bir tokenâ€™Ä±n sÃ¶zlÃ¼ÄŸe dahil edilmesi iÃ§in **en az** `min_freq` kez gÃ¶rÃ¼nmesi gerekir.\n",
    "2. **Filtreleme**  \n",
    "   - `min_freq = 3` ise, veri setinde 1 veya 2 kez gÃ¶rÃ¼len tÃ¼m tokenâ€™lar  \n",
    "     - Model giriÅŸinde `<unk>` (OOV) olarak iÅŸaretlenir  \n",
    "     - SÃ¶zlÃ¼kte yer almaz  \n",
    "3. **SonuÃ§**  \n",
    "   - SÃ¶zlÃ¼kte sadece **yeterli sÄ±klÄ±kta** geÃ§en tokenâ€™lar kalÄ±r  \n",
    "   - Nadir tokenâ€™lar, modelin â€œgarbageâ€ Ã¶ÄŸrenmesini Ã¶nler\n",
    "\n",
    "\n",
    "### FaydalarÄ±\n",
    "- **Daha Temiz Vocab**  \n",
    "  - GerÃ§ekten anlamlÄ± ve istatistiksel olarak gÃ¼Ã§lÃ¼ tokenâ€™lar tutulur.  \n",
    "- **Daha Ä°yi Genelleme**  \n",
    "  - Nadir ve gÃ¼rÃ¼ltÃ¼lÃ¼ tokenâ€™lar OOVâ€™a dÃ¼ÅŸtÃ¼ÄŸÃ¼ iÃ§in model, ana kalÄ±plarÄ± daha iyi kavrar.  \n",
    "- **Kaynak VerimliliÄŸi**  \n",
    "  - KÃ¼Ã§Ã¼k bir sÃ¶zlÃ¼k, daha az bellek ve daha hÄ±zlÄ± matris iÅŸlemleri demektir.\n",
    "\n",
    "\n",
    "### Uygulama AdÄ±mlarÄ± (Teori)\n",
    "1. **Ã–n Analiz**  \n",
    "   - TÃ¼m metinlerdeki token frekanslarÄ±nÄ± topla  \n",
    "2. **EÅŸik Belirleme**  \n",
    "   - `min_freq` deÄŸerini seÃ§ (Ã¶rn. 2â€“5)  \n",
    "3. **Token SeÃ§imi**  \n",
    "   - SÃ¶zlÃ¼ÄŸe sadece `freq â‰¥ min_freq` olan tokenâ€™larÄ± ekle  \n",
    "4. **OOV Atama**  \n",
    "   - Geri kalan tÃ¼m tokenâ€™lar `<unk>` olarak iÅŸaretlenir  \n",
    "\n",
    "\n",
    "\n",
    "**Not:** `min_freq` deÄŸeri Ã§ok yÃ¼ksek ayarlanÄ±rsa Ã¶nemli ama az geÃ§en kelimeler de OOV olur; Ã§ok dÃ¼ÅŸÃ¼k ayarlanÄ±rsa nadir gÃ¼rÃ¼ltÃ¼ye izin verir. Ä°deal `min_freq`, veri setinin bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne ve Ã§eÅŸitliliÄŸine baÄŸlÄ± olarak deneysel belirlenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16698e",
   "metadata": {},
   "source": [
    "* Åimdi bÃ¼tÃ¼n bu teorik konularÄ±nÄ± diÄŸer Ã¶ÄŸrendiÄŸimiz yapÄ±larla birleÅŸtirip yola devam edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a125ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Temizleyici Fonksiyon ===\n",
    "def clean_text(text, lowercase=True, normalize_unicode=True):\n",
    "    if normalize_unicode:\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"[^a-zÃ§Ä±0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "50b776a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Parametreler ===\n",
    "filters        = \"\"           # noktalama silme filtresi\n",
    "oov_token      = \"<unk>\"\n",
    "pad_token      = \"<pad>\"\n",
    "sos_token      = \"<sos>\"\n",
    "eos_token      = \"<eos>\"\n",
    "char_level     = False        # True=char, False=word\n",
    "max_vocab_size = None         # None = sÄ±nÄ±rsÄ±z\n",
    "min_freq       = 2            # en az 2 kez geÃ§en tokenÊ¼lar kalacak\n",
    "padding        = \"post\"\n",
    "\n",
    "fixed_index = {\n",
    "    pad_token : 0,\n",
    "    oov_token : 1,\n",
    "    sos_token : 2,\n",
    "    eos_token : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c87a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Veri YÃ¼kleme & Temizleme ===\n",
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a183acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts  = [clean_text(t) for t in raw_inputs]\n",
    "target_texts = [clean_text(t) for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cd270696",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in_text  = [f\"{sos_token} {t}\" for t in target_texts]\n",
    "dec_out_text = [f\"{t} {eos_token}\" for t in target_texts]\n",
    "all_texts    = input_texts + dec_in_text + dec_out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09c1c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. GeÃ§ici Tokenizer ile FrekanslarÄ± Topla ===\n",
    "tmp_tok = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    ")\n",
    "tmp_tok.fit_on_texts(all_texts)\n",
    "word_counts = tmp_tok.word_counts  # OrderedDict {word: count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac726467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. min_freq Uygula ===\n",
    "if min_freq is not None:\n",
    "    keep_tokens = [w for w, cnt in word_counts.items() if cnt >= min_freq]\n",
    "else:\n",
    "    keep_tokens = list(word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7573ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6. max_vocab_size Uygula ===\n",
    "if max_vocab_size is not None:\n",
    "    keep_tokens = sorted(\n",
    "        keep_tokens,\n",
    "        key=lambda w: word_counts[w],\n",
    "        reverse=True\n",
    "    )[: max_vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2463f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Final Tokenizer OluÅŸtur ve Fit Et ===\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=(len(keep_tokens) + 1) if max_vocab_size is None else max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46f39a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–nce sadece keep listesiyle fit, bÃ¶ylece index sabitlenir\n",
    "tokenizer.fit_on_texts(keep_tokens)\n",
    "# ArdÄ±ndan tÃ¼m metinlerle fit\n",
    "tokenizer.fit_on_texts(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "646933e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Sequence DÃ¶nÃ¼ÅŸÃ¼mÃ¼ & Padding ===\n",
    "inp_seq    = tokenizer.texts_to_sequences(input_texts)\n",
    "dec_in_seq = tokenizer.texts_to_sequences(dec_in_text)\n",
    "dec_out_seq= tokenizer.texts_to_sequences(dec_out_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "09df0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(\n",
    "    max(len(s) for s in inp_seq),\n",
    "    max(len(s) for s in dec_in_seq),\n",
    "    max(len(s) for s in dec_out_seq)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "72e0958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input  = pad_sequences(inp_seq,    maxlen=maxlen, padding=padding)\n",
    "decoder_input  = pad_sequences(dec_in_seq, maxlen=maxlen, padding=padding)\n",
    "decoder_target = pad_sequences(dec_out_seq,maxlen=maxlen, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06fd9e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab size:    329\n",
      "Max sequence length: 14\n",
      "'<sos>' id: 2\n",
      "'<eos>' id: 3\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(len(tokenizer.word_index) + 1, max_vocab_size or float(\"inf\"))\n",
    "print(\"Final vocab size:   \", vocab_size)\n",
    "print(\"Max sequence length:\", maxlen)\n",
    "print(f\"{sos_token!r} id:\", tokenizer.word_index.get(sos_token))\n",
    "print(f\"{eos_token!r} id:\", tokenizer.word_index.get(eos_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary coverage (min_freq=2): 94.61%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nVerideki her 100 tokenâ€™Ä±n yaklaÅŸÄ±k 94.6â€™sÄ±, sÃ¶zlÃ¼ÄŸÃ¼nÃ¼zde tuttuÄŸunuz tokenâ€™lar tarafÄ±ndan karÅŸÄ±lanÄ±yor. \\nGeriye kalan ~5.4% ise nadir tokenâ€™lar (1 kez geÃ§enler) olduÄŸu iÃ§in <unk> olarak iÅŸleniyor.\\n\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage = sum(word_counts[w] for w in keep_tokens) / sum(word_counts.values())\n",
    "print(f\"Vocabulary coverage (min_freq={min_freq}): {coverage:.2%}\")\n",
    "\n",
    "'''\n",
    "Verideki her 100 tokenâ€™Ä±n yaklaÅŸÄ±k 94.6â€™sÄ±, sÃ¶zlÃ¼ÄŸÃ¼nÃ¼zde tuttuÄŸunuz tokenâ€™lar tarafÄ±ndan karÅŸÄ±lanÄ±yor. \n",
    "Geriye kalan ~5.4% ise nadir tokenâ€™lar (1 kez geÃ§enler) olduÄŸu iÃ§in <unk> olarak iÅŸleniyor.\n",
    "\n",
    "Ne kadar yÃ¼ksek ise modelinizin gÃ¶rdÃ¼ÄŸÃ¼ gerÃ§ek kelimeleri ne kadar iyi kapsadÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "Ã‡ok dÃ¼ÅŸÃ¼kse, belki min_freq veya max_vocab_sizeâ€™Ä± yeniden dÃ¼ÅŸÃ¼nmek gerekir.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "810f8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens:\n",
      "<unk>      â†’ 1\n",
      "<sos>      â†’ 2\n",
      "<eos>      â†’ 3\n",
      "bir        â†’ 4\n",
      "g          â†’ 5\n",
      "ama        â†’ 6\n",
      "n          â†’ 7\n",
      "i          â†’ 8\n",
      "ben        â†’ 9\n",
      "ne         â†’ 10\n",
      "m          â†’ 11\n",
      "yardÄ±mcÄ±   â†’ 12\n",
      "yok        â†’ 13\n",
      "de         â†’ 14\n",
      "ba         â†’ 15\n",
      "ve         â†’ 16\n",
      "nedir      â†’ 17\n",
      "r          â†’ 18\n",
      "python     â†’ 19\n",
      "ya         â†’ 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 tokens:\")\n",
    "for word, idx in list(tokenizer.word_index.items())[:20]:\n",
    "    print(f\"{word:<10} â†’ {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41f9a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input text:   merhaba <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Sample decoder-in:   <sos> merhaba size nasÄ±l yardÄ±mcÄ± olabilirim <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Sample decoder-out:  merhaba size nasÄ±l yardÄ±mcÄ± olabilirim <eos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample input text:  \", tokenizer.sequences_to_texts([encoder_input[0]])[0])\n",
    "print(\"Sample decoder-in:  \", tokenizer.sequences_to_texts([decoder_input[0]])[0])\n",
    "print(\"Sample decoder-out: \", tokenizer.sequences_to_texts([decoder_target[0]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3980ec1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## tf.data.Dataset Entegrasyonu (Teori)\n",
    "\n",
    "**Nedir?**  \n",
    "TensorFlowâ€™un `tf.data.Dataset` APIâ€™si, Ã¶nceden tokenize edip pad ettiÄŸiniz NumPy dizileri veya diÄŸer veri kaynaklarÄ±nÄ± â€œpipelineâ€ olarak adlandÄ±rÄ±lan akÄ±ÅŸa dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Bu akÄ±ÅŸ Ã¼zerinde **shuffle**, **batch**, **cache**, **map** ve **prefetch** gibi iÅŸlemleri zincirleyerek, modelin eÄŸitim sÄ±rasÄ±nda veriyle kesintisiz ve verimli bir ÅŸekilde beslenmesini saÄŸlar.\n",
    "\n",
    "\n",
    "## Ne Zaman KullanÄ±lÄ±r?\n",
    "- **BÃ¼yÃ¼k veri setleri** belleÄŸe sÄ±ÄŸmadÄ±ÄŸÄ±nda veya diskten â€œstreamingâ€ okumak gerektiÄŸinde  \n",
    "- EÄŸitim sÄ±rasÄ±nda **shuffle** ve **batch** iÅŸlemlerini CPU/GPU ile paralel hale getirerek performansÄ± artÄ±rmak istediÄŸinizde  \n",
    "- **Veri augmentasyonu** veya ek Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± (Ã¶rneÄŸin, `map` ile token filtreleme) model akÄ±ÅŸÄ±na dahil etmek istediÄŸinizde  \n",
    "- TFRecord, CSV, gÃ¶rÃ¼ntÃ¼ dosyalarÄ± gibi formatlardan doÄŸrudan okuma ve Ã¶nbellekleme (`cache`) gerektiÄŸinde  \n",
    "\n",
    "\n",
    "\n",
    "## Entegrasyon AÅŸamalarÄ±\n",
    "1. **Tokenizer & Padding**  \n",
    "   - Ham metni `texts_to_sequences` ve `pad_sequences` ile NumPy dizilerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n.  \n",
    "2. **Dataset OluÅŸturma**  \n",
    "   - `tf.data.Dataset.from_tensor_slices((inputs_dict, targets))` ile veri akÄ±ÅŸÄ±nÄ± baÅŸlatÄ±n.  \n",
    "3. **Pipeline AdÄ±mlarÄ±**  \n",
    "   - **Shuffle:** Rastgelelik iÃ§in `shuffle(buffer_size)`  \n",
    "   - **Batch:** GPU/TPUâ€™ya uygun sabit boyutlarda gruplama iÃ§in `batch(batch_size)`  \n",
    "   - **Prefetch:** Bir sonraki batchâ€™i Ã¶nceden yÃ¼klemek iÃ§in `prefetch(tf.data.AUTOTUNE)`  \n",
    "   - Ä°steÄŸe baÄŸlÄ±: `cache()`, `repeat()` ve paralel `map()` gibi fonksiyonlar.  \n",
    "4. **Model.fit**  \n",
    "   - OluÅŸturduÄŸunuz `dataset` objesini doÄŸrudan `model.fit(dataset, epochs=â€¦)` ile kullanÄ±n.  \n",
    "\n",
    "\n",
    "## Nerede YazÄ±lmalÄ±?\n",
    "- **Tokenize & pad** adÄ±mÄ±ndan hemen sonra,  \n",
    "- **Model tanÄ±mÄ±** ve `.compile()` Ã§aÄŸrÄ±sÄ±ndan **Ã¶nce**,  \n",
    "- BÃ¶ylece `dataset` hazÄ±r olduktan sonra doÄŸrudan `model.fit(datasetâ€¦)` satÄ±rÄ±nÄ± yazabilirsiniz.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a3d0c",
   "metadata": {},
   "source": [
    "* Modelin oluÅŸumundan hemen Ã¶nce ya da fit fonksiyonundan hemen Ã¶nce yapmanÄ±z gerekecek.Notebook sonunda yapÄ±lacak olan projede gÃ¶sterilecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b8f43711",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4f537ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    \n",
    "    { \n",
    "      \"encoder_input\" : encoder_input,\n",
    "      \"decoder_input\" : decoder_input\n",
    "    },\n",
    "    decoder_target\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    # Shuffle: veri kÃ¼mesindeki Ã¶rnekleri karÄ±ÅŸtÄ±rarak her epochâ€™ta farklÄ± sÄ±ralama saÄŸlar.\n",
    "    # buffer_size: kaÃ§ Ã¶rneÄŸin bellekte karÄ±ÅŸtÄ±rÄ±lacaÄŸÄ±nÄ± belirler.\n",
    "    #   - BÃ¼yÃ¼k deÄŸer = daha random sÄ±ra, daha fazla bellek kullanÄ±mÄ±.\n",
    "    .shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # Batch: Ã¶rnekleri batch_size kadar gruplar.\n",
    "    # batch_size: her adÄ±mda modele kaÃ§ Ã¶rnek yollanacaÄŸÄ±nÄ± belirler.\n",
    "    # drop_remainder=True: son batchâ€™te batch_sizeâ€™a tamamlanamayan\n",
    "    #   eksik grup varsa, o kÃ¼Ã§Ã¼k batchâ€™i atlar.\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch: bir sonraki batchâ€™i eÄŸitim Ã§alÄ±ÅŸÄ±rken Ã¶nceden hazÄ±rlar.\n",
    "    # tf.data.AUTOTUNE: TensorFlowâ€™un uygun sayÄ±da thread seÃ§mesini saÄŸlar.\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7600d",
   "metadata": {},
   "source": [
    "* Daha da ileriye taÅŸÄ±mak istersek ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# === tf.data.Dataset Entegrasyonu ===\n",
    "\n",
    "# 1) NumPy dizilerinden Dataset oluÅŸtur:\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "            \"encoder_input\": encoder_input,  # Encoderâ€™a gidecek padded giriÅŸ\n",
    "            \"decoder_input\": decoder_input   # Decoderâ€™a gidecek padded giriÅŸ\n",
    "        },\n",
    "        decoder_target  # Modelin Ã¶ÄŸreneceÄŸi gerÃ§ek Ã§Ä±ktÄ±lar\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Pipeline adÄ±mlarÄ±:\n",
    "dataset = (\n",
    "    dataset\n",
    "    .cache()                            # 1. Ä°lk kullanÄ±mda cacheâ€™le (bellek/disk), sonraki epochâ€™lar iÃ§in hÄ±zlÄ± eriÅŸim\n",
    "    .shuffle(buffer_size=buffer_size)   # 2. Veriyi buffer_size kadar Ã¶rnekle karÄ±ÅŸtÄ±rarak rastgelelik ekle\n",
    "    .repeat()                           # 3. Datasetâ€™i sonsuz (veya istenen sayÄ±da) tekrar et\n",
    "    .batch(batch_size, drop_remainder=True)  # 4. batch_size kadar gruplar; eksik batchâ€™leri atar\n",
    "    .prefetch(tf.data.AUTOTUNE)        # 5. Bir sonraki batchâ€™i arka planda hazÄ±rla\n",
    ")\n",
    "\n",
    "# ArtÄ±k dataset, doÄŸrudan model.fit ile kullanÄ±lmaya hazÄ±r:\n",
    "# model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13617e1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##  Morfolojik Analiz  \n",
    "\n",
    "**Kavram:**  \n",
    "TÃ¼rkÃ§e gibi eklemeli (agglutinative) dillerde, bir kelime tek bir stringâ€™de kÃ¶k + Ã§ok sayÄ±da ek barÄ±ndÄ±rÄ±r.  \n",
    "Morfolojik analiz, bu kelimeyi â€œkÃ¶kâ€ ve â€œekâ€ bileÅŸenlerine ayÄ±rarak anlamsal yapÄ±yÄ± ortaya Ã§Ä±karÄ±r.\n",
    "\n",
    "\n",
    "### NasÄ±l Ã‡alÄ±ÅŸÄ±r?  \n",
    "1. **KÃ¶k Ã‡Ä±karÄ±mÄ± (Stemming/Lemmatization)**  \n",
    "   - â€œyapabilirÂ­-dimâ€ â†’ kÃ¶k: â€œyapâ€, ekler: [â€œ-abilirâ€, â€œ-dimâ€]  \n",
    "2. **Ek SÄ±nÄ±flandÄ±rmasÄ±**  \n",
    "   - Ã‡ekim ekleri (â€œ-dimâ€, â€œ-sinizâ€) vs. yapÄ±m ekleri (â€œ-lÃ¼kâ€, â€œ-ciâ€)  \n",
    "3. **SÃ¶zlÃ¼k & Kurallar**  \n",
    "   - Zemberek gibi kÃ¼tÃ¼phaneler, kapsamlÄ± morfolojik sÃ¶zlÃ¼k + dilbilgisi kural seti kullanÄ±r.  \n",
    "4. **Ã‡Ä±ktÄ±**  \n",
    "   - Hem kÃ¶k hem de ek tÃ¼rleri elde edilir; Ã¶rnek:  \n",
    "     ```json\n",
    "     {\n",
    "       \"surface\": \"yapabilirÂ­dim\",\n",
    "       \"lemma\": [\"yap\"],\n",
    "       \"tags\": [\"Verb\",\"Ability\",\"Past\",\"FirstPerson\"]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### KullanÄ±labilecek AraÃ§lar  \n",
    "- **Zemberek (Java)**  \n",
    "  - `turkish-morphology` modÃ¼lÃ¼  \n",
    "  - Python iÃ§in `py4j` Ã¼zerinden eriÅŸim veya `zemberek-python` sarÄ±mlarÄ±  \n",
    "- **TRnlp / TRmorph**  \n",
    "  - Saf Python implementasyonlar  \n",
    "- **SpaCy + tr_spacy**  \n",
    "  - SpaCy Ã§erÃ§evesi iÃ§inde TÃ¼rkÃ§e morfoloji ekleri  \n",
    "- **Stanza (Stanford NLP)**  \n",
    "  - Ã‡ok dilli morfolojik analiz, TÃ¼rkÃ§e desteÄŸi de var\n",
    "\n",
    "\n",
    "### Uygulama Ã–rneÄŸi (Zemberek-Python)  \n",
    "```python\n",
    "from zemberek import TurkishMorphology\n",
    "\n",
    "morph = TurkishMorphology.create_with_defaults()\n",
    "analysis = morph.analyze(\"yardÄ±mcÄ±larÄ±nÄ±zdan\")\n",
    "for result in analysis:\n",
    "    print(result.get_stem(), result.get_morphemes())\n",
    "# Ã‡Ä±ktÄ± Ã¶rneÄŸi:\n",
    "# ya1rdÄ±mcÄ± ['Noun', 'Pos', 'Plur', 'A3sg', 'P2pl']\n",
    "\n",
    "-- **  Ne Zaman KullanÄ±lÄ±r? ** -- \n",
    "\n",
    "* TÃ¼rkÃ§e veya Fin-Ural dilleri gibi eklemeli dillerde\n",
    "\n",
    "* KÃ¼Ã§Ã¼k-orta boy veri setlerinde, kelime bazlÄ± token sayÄ±sÄ±nÄ± dÃ¼ÅŸÃ¼rmek istediÄŸinizde\n",
    "\n",
    "* Ã–zellikle bilgi Ã§Ä±karÄ±mÄ±, soru-cevap veya anlamsal analiz gibi gÃ¶revlerde kÃ¶k-ek yapÄ±sÄ±nÄ±n Ã¶nemli olduÄŸu durumlarda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87721f6",
   "metadata": {},
   "source": [
    "## Ã–zetle:\n",
    "\n",
    "* Morfolojik analiz, clean_text()â€™ten sonra, Tokenizerâ€™dan Ã¶nce gelen â€œÃ¶n-iÅŸlemeâ€ adÄ±mÄ±dÄ±r.\n",
    "\n",
    "* Kerasâ€™Ä±n Tokenizer sÄ±nÄ±fÄ± iÃ§inde otomatik bulunmaz; senin kendi pipelineâ€™Ä±nda aÃ§Ä±kÃ§a bu iÅŸlemi Ã§aÄŸÄ±rman gerekir.\n",
    "\n",
    "* BÃ¶ylece, tokenizerâ€™a verdiÄŸin her â€œtokenâ€ zaten tÃ¼m eki atÄ±lmÄ±ÅŸ kÃ¶k veya ek bilgisiyle birlikte birim olarak girer ve vocabâ€™in Ã§ok daha temiz, genellemesi gÃ¼Ã§lÃ¼ olur.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# === 1) spaCy TÃ¼rkÃ§e Modelini Kur ve YÃ¼kle ===\n",
    "# Terminalâ€™de bir kez Ã§alÄ±ÅŸtÄ±r: \n",
    "#   python -m spacy download tr_core_news_sm\n",
    "nlp = spacy.load(\"tr_core_news_sm\")\n",
    "\n",
    "# === 2) Temizleyici Fonksiyon ===\n",
    "def clean_text(text, lowercase=True, normalize_unicode=True):\n",
    "    if normalize_unicode:\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    # TÃ¼rkÃ§e karakterleri koruyup diÄŸerlerini boÅŸlukla deÄŸiÅŸtir\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# === 3) Lemmatizasyon Fonksiyonu ===\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    spaCy modeliyle verilen metindeki her token'Ä±n lemma (kÃ¶k) hali.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_ for token in doc)\n",
    "\n",
    "# === 4) Ã–rnek Ã‡alÄ±ÅŸtÄ±rma ===\n",
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")\n",
    "raw_texts = df[\"input\"].astype(str).tolist()\n",
    "\n",
    "for orig in raw_texts[:5]:\n",
    "    cleaned   = clean_text(orig)\n",
    "    lemmatized = lemmatize_text(cleaned)\n",
    "    print(f\"Orijinal:   {orig}\")\n",
    "    print(f\"Cleaned:    {cleaned}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf0633",
   "metadata": {},
   "source": [
    "* Gerekli kod aktarÄ±mlarÄ± yukarÄ±da verilmiÅŸtir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed2886",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a25b97",
   "metadata": {},
   "source": [
    "##  Subword / BPE / WordPiece DesteÄŸi\n",
    "\n",
    "### Kavram  \n",
    "- **Subword tokenization**: Metni tÃ¼m kelimeler yerine, sÄ±k tekrar eden **alt kelime parÃ§alara** (subword) bÃ¶lerek iÅŸler.  \n",
    "- **Byte-Pair Encoding (BPE)**: En sÄ±k geÃ§en bitiÅŸik karakter Ã§iftlerini (byte-pair) iteratif olarak birleÅŸtirip sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸturur.  \n",
    "- **WordPiece**: Googleâ€™Ä±n BERT modeli iÃ§in geliÅŸtirdiÄŸi, sonraki parÃ§a olasÄ±lÄ±klarÄ±nÄ± dikkate alarak alt kelime birimleri seÃ§en yÃ¶ntem.\n",
    "\n",
    "\n",
    "### NasÄ±l Ã‡alÄ±ÅŸÄ±r?  \n",
    "1. **BaÅŸlangÄ±Ã§**: TÃ¼m karakterler veya tek karakter tokenâ€™larÄ±yla baÅŸlar.  \n",
    "2. **Ã–lÃ§Ã¼m**: Karakter Ã§iftleri veya subword parÃ§a frekanslarÄ±nÄ± hesaplar.  \n",
    "3. **BirleÅŸtirme**: En sÄ±k Ã§iftleri veya parÃ§alarÄ± birleÅŸtirir.  \n",
    "4. **Tekrar**: Ä°stenilen vocab bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne ulaÅŸana dek adÄ±m 2â€“3â€™Ã¼ sÃ¼rdÃ¼rÃ¼r.  \n",
    "5. **SonuÃ§**: Hem tam kelimeler, hem de alt parÃ§alar (Ã¶r. â€œyardÄ±mâ€ + â€œcÄ±larâ€) iÃ§eren subword sÃ¶zlÃ¼ÄŸÃ¼ elde edilir.\n",
    "\n",
    "\n",
    "\n",
    "### FaydalarÄ±  \n",
    "- **OOV Problemini Minimize Eder**  \n",
    "  - Komple yeni kelimeler bile mevcut alt parÃ§alarÄ±n birleÅŸimiyle ifade edilir.  \n",
    "- **KÃ¼Ã§Ã¼k, Etkili SÃ¶zlÃ¼k**  \n",
    "  - Binlerce nadir kelime yerine, yÃ¼zlerce subword birimi yeterli olur.  \n",
    "- **Ã‡ok Dilli Destek**  \n",
    "  - Birden fazla dilde ortak subword birimleri paylaÅŸarak transfer Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r.  \n",
    "- **HafÄ±za & HÄ±z KazancÄ±**  \n",
    "  - Daha kÃ¼Ã§Ã¼k embedding matrisleri, daha hÄ±zlÄ± eÄŸitim ve Ã§Ä±karÄ±m.\n",
    "\n",
    "\n",
    "\n",
    "### Ne Zaman KullanÄ±lÄ±r?  \n",
    "- **BÃ¼yÃ¼k veri setleri** ve **Ã§ok dilli** uygulamalarda  \n",
    "- **OOV oranÄ±nÄ±n yÃ¼ksek** olduÄŸu durumlarda  \n",
    "- Modelin **daha ince anlamsal** genellemeye ihtiyacÄ± varsa  \n",
    "- **Transformer** veya **BERT** benzeri subword-temelli modellerde zorunlu\n",
    "\n",
    "\n",
    "\n",
    "### Uygulama Ã–zet  \n",
    "- **SentencePiece** (Google): BPE & Unigram modelleri, baÄŸÄ±msÄ±z olarak dil ve platformdan  \n",
    "- **HuggingFace Tokenizers** (Rust tabanlÄ±): Hem BPE hem WordPiece hem Unigram  \n",
    "- **Tokenizers.fit** â†’ **encode** / **decode** metotlarÄ±yla subword kelimeleri otomatik iÅŸle\n",
    "\n",
    "> Subword tokenization, modern NLP modellerinin temel taÅŸlarÄ±ndan biridir ve OOV sorununu bÃ¼yÃ¼k Ã¶lÃ§Ã¼de ortadan kaldÄ±rÄ±r.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131dc6c",
   "metadata": {},
   "source": [
    "### Subword / BPE Tokenizationâ€™da â€œTokenizerâ€ NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "\n",
    "1. **Klasik `Tokenizer` yerine Subword Model**  \n",
    "   - Kerasâ€™Ä±n `Tokenizer` sÄ±nÄ±fÄ±nÄ± kullanmak yerine, `SentencePiece` (veya HuggingFace Tokenizers) tarafÄ±ndan eÄŸitilmiÅŸ bir model kullanÄ±rsÄ±nÄ±z.  \n",
    "   - Bu model `.Train()` adÄ±mÄ±yla bir â€œvocab.modelâ€ dosyasÄ± ve â€œvocab.vocabâ€ dosyasÄ± oluÅŸturur.\n",
    "\n",
    "2. **`fit_on_texts` Gibi AdÄ±m Yok**  \n",
    "   - Metinleri â€œÃ¶ÄŸretenâ€ kod, `SentencePieceTrainer.Train(...)` fonksiyonudur; bu fonksiyon ham metin dosyasÄ±nÄ± (â€œall_texts.txtâ€) okur ve BPE sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ Ã§Ä±karÄ±r.  \n",
    "   - DolayÄ±sÄ±yla `tokenizer.fit_on_texts(input_texts)` yerine:\n",
    "     ```python\n",
    "     spm.SentencePieceTrainer.Train(\n",
    "         input=\"all_texts.txt\",\n",
    "         model_prefix=\"bpe_spm\",\n",
    "         vocab_size=8000,\n",
    "         model_type=\"bpe\"\n",
    "     )\n",
    "     ```\n",
    "     satÄ±rlarÄ± kullanÄ±lÄ±r.\n",
    "\n",
    "3. **Model YÃ¼kleme & Kodlama**  \n",
    "   - Elde edilen `bpe_spm.model` dosyasÄ±nÄ±:\n",
    "     ```python\n",
    "     sp = spm.SentencePieceProcessor()\n",
    "     sp.Load(\"bpe_spm.model\")\n",
    "     ```\n",
    "     ile yÃ¼klÃ¼yorsunuz.  \n",
    "   - Bu `sp` nesnesi **tokenizer**â€™Ä±nÄ±z olur.  \n",
    "   - ArdÄ±ndan metni IDâ€™lere Ã§evirmek iÃ§in:\n",
    "     ```python\n",
    "     ids = sp.EncodeAsIds(\"merhaba dÃ¼nya\")\n",
    "     ```\n",
    "     veya parÃ§a bazlÄ± metne dÃ¶ndÃ¼rmek iÃ§in\n",
    "     ```python\n",
    "     pieces = sp.EncodeAsPieces(\"merhaba dÃ¼nya\")\n",
    "     ```\n",
    "\n",
    "4. **Kerasâ€™Ä±n `pad_sequences` ile Entegrasyon**  \n",
    "   - Kendi `sp` nesnenizle dÃ¶nÃ¼ÅŸÃ¼m yaptÄ±ktan sonra, ID listelerini `pad_sequences(..., value=pad_id)` ile pad edersiniz.  \n",
    "   - Bu aÅŸamada Kerasâ€™a Ã¶zgÃ¼ `Tokenizer` kullanÄ±mÄ± tamamen devreden Ã§Ä±kar.\n",
    "\n",
    "\n",
    "**Ã–zet:**  \n",
    "- Subword tokenizationâ€™da **`SentencePiece` veya `Tokenizers` model dosyasÄ±** kendi â€œtokenizerâ€Ä±nÄ±zdÄ±r.  \n",
    "- `fit_on_texts` yerine **`Trainer.Train(...)`** adÄ±mÄ±nÄ± kullanÄ±rsÄ±nÄ±z.  \n",
    "- Kodlama/Ã§Ã¶zme iÅŸlemleri `sp.EncodeAsIds` ve `sp.EncodeAsPieces` ile yapÄ±lÄ±r.  \n",
    "- SonrasÄ±nda `pad_sequences` gibi araÃ§larla Keras pipelineâ€™Ä±nÄ±za baÄŸlarsÄ±nÄ±z.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c47cc0",
   "metadata": {},
   "source": [
    "## Kelime-bazlÄ± `Tokenizer` vs. Subword (SentencePiece/BPE)\n",
    "\n",
    "| Kriter | Keras `Tokenizer.fit_on_texts` <br>(Kelime-bazlÄ±) | SentencePiece / BPE / WordPiece <br>(Subword) |\n",
    "|--------|---------------------------------------------------|-----------------------------------------------|\n",
    "| **Kurulum** | Keras iÃ§inde hazÄ±r, ek paket gerekmez | Ek paket ( `sentencepiece` veya *HF Tokenizers*), model eÄŸitimi gerekir |\n",
    "| **OOV (Out-of-Vocab) OranÄ±** | YÃ¼ksek â€” nadir veya yeni kelimeler `<unk>` olur | Ã‡ok dÃ¼ÅŸÃ¼k â€” yeni kelimeler mevcut alt parÃ§alarÄ±n birleÅŸimiyle yazÄ±lÄ±r |\n",
    "| **Vocab Boyutu** | Kelime baÅŸÄ±na 1 giriÅŸ â‡’ bÃ¼yÃ¼k sÃ¶zlÃ¼k | AynÄ± kapsama iÃ§in Ã§ok daha kÃ¼Ã§Ã¼k (âˆ¼8 k) |\n",
    "| **Bellek / Embed Matris** | BÃ¼yÃ¼k matris, yavaÅŸlar | KÃ¼Ã§Ã¼k matris, daha hÄ±zlÄ± eÄŸitim/Ã§Ä±karÄ±m |\n",
    "| **Dil DesteÄŸi** | Ä°ngilizce-benzeri dillerde sorun az | Eklemeli dillerde (TR, FI) Ã¶zellikle gÃ¼Ã§lÃ¼ |\n",
    "| **Ä°nsan Okunabilirlik** | Ã‡Ä±ktÄ± tam kelime; log okumak kolay | ParÃ§alÄ± Ã§Ä±ktÄ± (â€œyardÄ±mcÄ±â–larâ€), yorumlamak zor |\n",
    "| **Prototip HÄ±zÄ±** | Ã‡ok hÄ±zlÄ± baÅŸlar, kod sade | Model + dosya Ã¼retmek ek adÄ±m |\n",
    "| **GÃ¼ncelleme KolaylÄ±ÄŸÄ±** | `fit_on_texts` tekrar Ã§alÄ±ÅŸtÄ±rmak yeter | Yeni veri iÃ§in BPE modelini yeniden eÄŸitmek gerekir |\n",
    "| **BÃ¼yÃ¼k Veri** | Nadir kelimeler ve OOV patlar | Kapsama hÃ¢lÃ¢ yÃ¼ksek, Ã¶lÃ§eklenebilir |\n",
    "| **Standart Modern Modeller** | LSTM/GRU tabanlÄ± seq2seq | BERT, T5, GPT, Transformer standardÄ± |\n",
    "\n",
    "\n",
    "\n",
    "### Ne Zaman Hangi YÃ¶ntem?\n",
    "\n",
    "- **HÄ±zlÄ± prototip / kÃ¼Ã§Ã¼k veri**  \n",
    "  - *Kelime-bazlÄ± Tokenizer*  \n",
    "  - Daha okunaklÄ± log, daha az dÄ±ÅŸ baÄŸÄ±mlÄ±lÄ±k.\n",
    "\n",
    "- **Veri > ~1 M cÃ¼mle, OOV % â†‘, Ã§ok dilli veya eklemeli dil**  \n",
    "  - *SentencePiece (BPE / WordPiece)*  \n",
    "  - Daha kÃ¼Ã§Ã¼k vocab, neredeyse sÄ±fÄ±r OOV, modern Transformerâ€™larla tam uyum.\n",
    "\n",
    "- **Ara Ã‡Ã¶zÃ¼mler**  \n",
    "  - Kelime-bazlÄ± Tokenizer + **lemmatizasyon & `min_freq`**  \n",
    "  - OOV hÃ¢lÃ¢ yÃ¼ksekse â†’ **Subword**â€™a geÃ§.\n",
    "\n",
    "> **Pratik Yol HaritasÄ±**  \n",
    "> 1. Kelime-bazlÄ± ile baÅŸla.  \n",
    "> 2. OOV % > 5â€“10 ise lemmatizasyon/min_freq uygula.  \n",
    "> 3. HÃ¢lÃ¢ sorunluysa veya veri bÃ¼yÃ¼yorsa SentencePieceâ€™e geÃ§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6529609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import unicodedata, re, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a084e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")\n",
    "\n",
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()\n",
    "\n",
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "input_texts = [clean_texts(t)  for t in raw_inputs]\n",
    "target_texts = [clean_texts(t) for t in raw_targets]\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) BPE modelini SentencePiece ile eÄŸit\n",
    "# ------------------------------------\n",
    "\n",
    "model_prefix = \"bpe_tr\"\n",
    "vocab_size = 8000\n",
    "data_txt = \"corpus.txt\"\n",
    "\n",
    "with io.open(data_txt,\"w\",encoding = \"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "# 2) BPE Modelini eÄŸitirken:\n",
    "UNIQUE_LIMIT = 1800          # Trainerâ€™Ä±n verdiÄŸi Ã¼st sÄ±nÄ±rÄ±n biraz altÄ±\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=data_txt,\n",
    "    model_prefix=model_prefix,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(vocab_size, UNIQUE_LIMIT),   # 8k yerine 1 800\n",
    "    user_defined_symbols=[\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Modeli yÃ¼kle & Ã¶zel IDâ€™ler\n",
    "# -----------------------------\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# 4) Metinleri ID listesine dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "# ---------------------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int)         for t in target_texts]\n",
    "dec_out  = [            sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "\n",
    "# --------------  \n",
    "# 5) Padding\n",
    "# --------------\n",
    "max_len = max(max(map(len, enc_seqs)),\n",
    "              max(map(len, dec_in)),\n",
    "              max(map(len, dec_out)))\n",
    "\n",
    "print(max_len)\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9401282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline hazÄ±r âœ dataset Ã¶rneÄŸi: (32, 12)\n",
      "pad_id: 3 sos_id: 4 eos_id: 5\n",
      "max_len: 12 vocab_size: 8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 6) tf.data.Dataset pipeline\n",
    "# -----------------------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"encoder_input\": encoder_input,\n",
    "         \"decoder_input\": decoder_input},\n",
    "        decoder_target\n",
    "    )\n",
    ").shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline hazÄ±r âœ dataset Ã¶rneÄŸi:\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(\"pad_id:\", pad_id, \"sos_id:\", sos_id, \"eos_id:\", eos_id)\n",
    "print(\"max_len:\", max_len, \"vocab_size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cff07",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "## Chatbot-OdaklÄ± Transformer Ä°Ã§in Ek Tokenizer / Veri Ä°yileÅŸtirmeleri\n",
    "\n",
    "| BaÅŸlÄ±k | Neden Ã–nemli? | NasÄ±l Eklenir? |\n",
    "|--------|---------------|----------------|\n",
    "| **KonuÅŸmacÄ± Etiketleri** (`<user>`, `<bot>`) | Rol ayrÄ±mÄ±, diyaloÄŸun yÃ¶nÃ¼nÃ¼ netleÅŸtirir; model hangi repliÄŸin kime ait olduÄŸunu Ã¶ÄŸrenir. | Her cÃ¼mleye baÅŸlÄ±k tokenâ€™Ä± ekleyin: `\" <user> merhaba\"` / `\" <bot> merhaba, nasÄ±l yardÄ±mcÄ± olabilirim?\"` |\n",
    "| **Diyalog Tur AyÄ±rÄ±cÄ±** (`<turn>`) | Uzun geÃ§miÅŸte cÃ¼mle sÄ±nÄ±rlarÄ±nÄ± netleÅŸtirir; kontekst â€œkaymasÄ±nÄ±â€ azaltÄ±r. | Ã‡ok-cÃ¼mleli geÃ§miÅŸi tek dizide birleÅŸtirip her cÃ¼mle sonuna `<turn>` ekleyin. |\n",
    "| **Persona / Stil Tokenâ€™larÄ±** (`<formal>`, `<casual>`) | KiÅŸiselleÅŸtirme ve ton kontrolÃ¼; aynÄ± modelle birden fazla stil Ã¼retimi. | EÄŸitim verisinde Ã¶rnek yanÄ±tÄ±n baÅŸÄ±na uygun token ekleyin. |\n",
    "| **Subword Regularization** (BPE sampling) | Ã‡eÅŸitli alt-parÃ§a varyasyonlarÄ± â†’ overfitting dÃ¼ÅŸer, robustluk artar. | `SentencePieceProcessor.Encode(..., enable_sampling=True, nbest_size=-1, alpha=0.1)` |\n",
    "| **Dynamic Padding Buckets** | Sabit `max_len` yerine yakÄ±n uzunlukta cÃ¼mleleri gruplayÄ±p %30-40 GPU kazanÄ±mÄ±. | `tf.data.experimental.bucket_by_sequence_length()` kullanÄ±n. |\n",
    "| **Numerik / Tarih Placeholderâ€™larÄ±** (`<NUM>`, `<DATE>`) | Rakam kÃ¼meleri anlamsal gÃ¼rÃ¼ltÃ¼ yaratÄ±r; generatif hatalarÄ± azaltÄ±r. | Regex ile sayÄ±larÄ± `<NUM>123</NUM>` veya sadece `<NUM>` ÅŸeklinde maskeleyin. |\n",
    "| **NER-tabanlÄ± De-lexicalization** (`<PERSON>`, `<ORG>`) | Gizlilik + daha iyi genelleme; Ã¶zel isimler OOV yaratmaz. | SpaCy / Stanza NER â†’ Ã¶zel isimleri placeholder ile deÄŸiÅŸtir. |\n",
    "| **Token Dropout Augmentation** | Beklenmedik eksik kelimelere karÅŸÄ± dayanÄ±klÄ±lÄ±k. | `dataset.map(token_dropout, num_parallel_calls=AUTOTUNE)` |\n",
    "| **Segment ID veya Speaker ID Embeddingâ€™i** | Self-Attentionâ€™Ä±n â€œkim konuÅŸuyorâ€ bilgisini doÄŸrudan gÃ¶rmesi. | Transformer girdi dictâ€™ine `speaker_ids` tensÃ¶rÃ¼ ekleyin (0=user,1=bot). |\n",
    "| **Label Smoothing** | Hedef dizideki tek-sÄ±cak (one-hot) kÃ¶ÅŸeliÄŸini yumuÅŸatÄ±r, over-confidence Ã¶nler. | `tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)` |\n",
    "| **Top-K / Nucleus Decoding Ayar Tokenâ€™larÄ±** (`<topk_50>`, `<topp_0.9>`) | Ã‡Ä±karÄ±mda esnek kontrol: tutarlÄ±lÄ±k vs. yaratÄ±cÄ±lÄ±k. | Promptâ€™un baÅŸÄ±na ayar tokenâ€™Ä± koy, decode sÄ±rasÄ±nda parse edip karÅŸÄ±lÄ±k gelen sampling configâ€™i kullan. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1c6e1",
   "metadata": {},
   "source": [
    "* Åimdi aÅŸaÄŸÄ±ya artÄ±k son demlerine gelmiÅŸ bir tokenizer kodu bÄ±rakÄ±yorum ( Subword Tokenization â€“ SentencePiece ile Byte-Pair Encoding (BPE ).Bu diÄŸer tokenizer iÅŸlemi.Bundan sonra ise ilk baÅŸlarda uyguladÄ±ÄŸÄ±mÄ±z tokenizer iÅŸlemlerine eklemeler yapacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dff9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset Ã¶rneÄŸi encoder shape: (32, 12)\n",
      "pad_id:3  sos_id:4  eos_id:5  unk_id:0\n",
      "max_len: 12    vocab_size: 1800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SUBWORD-TABANLI TOKENIZER PIPELINE  (masking yok)\n",
    "\n",
    "â— SentencePiece-BPE modeli (1 800 parÃ§alÄ±k sÃ¶zlÃ¼k)\n",
    "â— <sos>/<eos>/<pad>/<unk> Ã¶zel tokenâ€™larÄ±\n",
    "â— Rastgele %10 tokenâ€™Ä± <unk> yaparak UNK-dropout (data augmentation)\n",
    "â— tf.data.Dataset â†’ shuffle-batch-prefetch zinciri\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------ 0. Gereksinimler ------------------------\n",
    "# pip install sentencepiece tensorflow==2.18 pandas\n",
    "\n",
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ------------------------ 1. Veriyi YÃ¼kle & Temizle ---------------\n",
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")           # input ve output sÃ¼tunlarÄ±\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼ÄŸ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "input_texts  = [clean(t) for t in raw_inputs]\n",
    "target_texts = [clean(t) for t in raw_targets]\n",
    "\n",
    "# ------------------------ 2. SentencePiece BPE EÄŸit ----------------\n",
    "MODEL_PREFIX = \"bpe_tr\"\n",
    "VOCAB_SIZE   = 8000                 # istenen\n",
    "UNIQUE_LIMIT = 1800                 # kÃ¼Ã§Ã¼k korpus Ã¼st limiti\n",
    "CORPUS_TXT   = \"corpus.txt\"\n",
    "\n",
    "with io.open(CORPUS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=CORPUS_TXT,\n",
    "    model_prefix=MODEL_PREFIX,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(VOCAB_SIZE, UNIQUE_LIMIT),\n",
    "    user_defined_symbols=[\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# ------------------------ 3. Modeli YÃ¼kle & IDâ€™ler ----------------\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.piece_to_id(\"<unk>\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# ------------------------ 4. Encode & Padding ---------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int) for t in target_texts]\n",
    "dec_out  = [sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "max_len = max(\n",
    "    max(map(len, enc_seqs)),\n",
    "    max(map(len, dec_in)),\n",
    "    max(map(len, dec_out))\n",
    ")\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "\n",
    "# ------------------------ 5. UNK-Dropout Fonksiyonu ---------------\n",
    "DROPOUT_RATE = 0.10   # %10 token rastgele <unk>\n",
    "\n",
    "def unk_dropout(inputs, targets, rate=DROPOUT_RATE, unk=unk_id):\n",
    "    enc = inputs[\"encoder_input\"]\n",
    "    dec = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_enc = tf.cast(tf.random.uniform(tf.shape(enc)) < rate, enc.dtype)\n",
    "    mask_dec = tf.cast(tf.random.uniform(tf.shape(dec)) < rate, dec.dtype)\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_enc == 1, unk, enc)\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_dec == 1, unk, dec)\n",
    "    return inputs, targets\n",
    "\n",
    "# ------------------------ 6. Dataset Pipeline ---------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\"encoder_input\": encoder_input,\n",
    "             \"decoder_input\": decoder_input},\n",
    "            decoder_target\n",
    "        )\n",
    "    )\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)   # sadece UNK-dropout\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ------------------------ 7. Kontrol Ã‡Ä±ktÄ±larÄ± --------------------\n",
    "enc_ex = next(iter(dataset.take(1)))[0][\"encoder_input\"]\n",
    "print(\"dataset Ã¶rneÄŸi encoder shape:\", enc_ex.shape)\n",
    "print(f\"pad_id:{pad_id}  sos_id:{sos_id}  eos_id:{eos_id}  unk_id:{unk_id}\")\n",
    "print(\"max_len:\", max_len, \"   vocab_size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34a81b",
   "metadata": {},
   "source": [
    "* Son olarak da \"<user\" ve \"<bot\" taglarÄ±nÄ± ekleyip iÅŸi bÄ±rakalÄ±m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f7d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder batch shape: (32, 16)\n",
      "pad_id:3  sos_id:4  eos_id:5  unk_id:0\n",
      "max_len: 16    vocab_size: 1800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TOKENIZER PIPELINE + KONUÅMACI & STÄ°L TOKENâ€™LARI\n",
    "-------------------------------------------------\n",
    "â€¢ SentencePiece-BPE (â‰ˆ1 800 parÃ§alÄ±k sÃ¶zlÃ¼k)\n",
    "â€¢ Ã–zel semboller: <pad> <sos> <eos> <user> <bot> <formal> <casual>\n",
    "â€¢ UNK-Dropout (%10)  â†’  dayanÄ±klÄ± girdi\n",
    "â€¢ tf.data.Dataset  â†’  shuffle â€¢ batch â€¢ prefetch\n",
    "(Attention mask EKLENMEDÄ° â€” yalnÄ±zca tokenizer & pipeline)\n",
    "\"\"\"\n",
    "\n",
    "# --------------------- 0. Gereksinimler ---------------------\n",
    "# pip install sentencepiece pandas tensorflow==2.18\n",
    "\n",
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --------------------- 1. Veriyi YÃ¼kle & Temizle ------------\n",
    "df = pd.read_csv(\"Ã¶rnek_set.csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼ÄŸ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# KonuÅŸmacÄ± + stil etiketleri ekle\n",
    "SPEAKER_USER = \"<user>\"\n",
    "SPEAKER_BOT  = \"<bot>\"\n",
    "STYLE_CASUAL = \"<casual>\"\n",
    "\n",
    "input_texts  = [f\"{SPEAKER_USER} {STYLE_CASUAL} {clean(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{SPEAKER_BOT} {STYLE_CASUAL} {clean(t)}\" for t in raw_targets]\n",
    "\n",
    "# --------------------- 2. SentencePiece BPE EÄŸit ------------\n",
    "MODEL_PREFIX = \"bpe_tr\"\n",
    "VOCAB_SIZE   = 8000\n",
    "LIMIT        = 1800                     # kÃ¼Ã§Ã¼k korpus sÄ±nÄ±rÄ±\n",
    "CORPUS_TXT   = \"corpus.txt\"\n",
    "\n",
    "with io.open(CORPUS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=CORPUS_TXT,\n",
    "    model_prefix=MODEL_PREFIX,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(VOCAB_SIZE, LIMIT),\n",
    "    user_defined_symbols=[\n",
    "        \"<pad>\", \"<sos>\", \"<eos>\",\n",
    "        SPEAKER_USER, SPEAKER_BOT, STYLE_CASUAL\n",
    "    ]\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
    "\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# --------------------- 3. Encode & Pad ----------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int) for t in target_texts]\n",
    "dec_out  = [sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "max_len = max(\n",
    "    max(map(len, enc_seqs)),\n",
    "    max(map(len, dec_in)),\n",
    "    max(map(len, dec_out))\n",
    ")\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "\n",
    "# --------------------- 4. UNK-Dropout -----------------------\n",
    "DROPOUT_RATE = 0.10   # %10 token rastgele <unk>\n",
    "\n",
    "def unk_dropout(inputs, targets, rate=DROPOUT_RATE, unk=unk_id):\n",
    "    enc = inputs[\"encoder_input\"]\n",
    "    dec = inputs[\"decoder_input\"]\n",
    "    mask_enc = tf.cast(tf.random.uniform(tf.shape(enc)) < rate, enc.dtype)\n",
    "    mask_dec = tf.cast(tf.random.uniform(tf.shape(dec)) < rate, dec.dtype)\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_enc == 1, unk, enc)\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_dec == 1, unk, dec)\n",
    "    return inputs, targets\n",
    "\n",
    "# --------------------- 5. Dataset Pipeline ------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\"encoder_input\": encoder_input,\n",
    "             \"decoder_input\": decoder_input},\n",
    "            decoder_target\n",
    "        )\n",
    "    )\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)  # sadece tokenizer augmentation\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# --------------------- 6. Kontrol Ã‡Ä±ktÄ±larÄ± -----------------\n",
    "enc_example = next(iter(dataset.take(1)))[0][\"encoder_input\"]\n",
    "print(\"Encoder batch shape:\", enc_example.shape)\n",
    "print(f\"pad_id:{pad_id}  sos_id:{sos_id}  eos_id:{eos_id}  unk_id:{unk_id}\")\n",
    "print(\"max_len:\", max_len, \"   vocab_size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4eed02",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261213dd",
   "metadata": {},
   "source": [
    "## Mevcut Kelime-BazlÄ± Tokenizer Kodunu â€œDerinleÅŸtirmeâ€ Yol HaritasÄ±\n",
    "\n",
    ">AmaÃ§: **BPEâ€™yi zaten Ã§Ã¶zdÃ¼k**; ÅŸimdi klasik `Tokenizer` senaryosunu daha esnek, yeniden-kullanÄ±labilir ve â€œÃ¼retim hazÄ±râ€ hÃ¢le getirmek.\n",
    "\n",
    "\n",
    "### 1. Kodun FonksiyonlaÅŸtÄ±rÄ±lmasÄ±\n",
    "Tek seferlik betik yerine **parametrik fonksiyon** yazmak:\n",
    "```python\n",
    "def build_word_tokenizer(\n",
    "        input_texts,\n",
    "        target_texts,\n",
    "        min_freq    = 2,\n",
    "        max_vocab   = None,\n",
    "        filters     = \"\",\n",
    "        oov_token   = \"<unk>\",\n",
    "        pad_token   = \"<pad>\",\n",
    "        sos_token   = \"<sos>\",\n",
    "        eos_token   = \"<eos>\",\n",
    "        char_level  = False,\n",
    "        padding     = \"post\"):\n",
    "\n",
    "    # (kodun 1-8. adÄ±mlarÄ± burada Ã¶zetlenmiÅŸ hÃ¢lde)\n",
    "    return tokenizer, encoder_input, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4dbc0",
   "metadata": {},
   "source": [
    "### KonuÅŸmacÄ± & Stil Etiketlerini Kelime-BazlÄ± Tokenizerâ€™a Ekleme\n",
    "\n",
    "#### 1. Neden Gerekli?\n",
    "| Sorun | Etiketlerin Ã‡Ã¶zÃ¼mÃ¼ |\n",
    "|-------|-------------------|\n",
    "| **Rol BelirsizliÄŸi** â€“ Model â€œkiminâ€ konuÅŸtuÄŸunu ayÄ±rt edemez. | `<user>` ve `<bot>` tokenâ€™larÄ± konuÅŸmacÄ± rolÃ¼nÃ¼ aÃ§Ä±kÃ§a belirtir. |\n",
    "| **Ton KontrolÃ¼** â€“ AynÄ± modelle resmÃ® / samimÃ® yanÄ±t Ã¼retmek. | `<casual>` / `<formal>` gibi stil tokenâ€™larÄ± istenen tonu sinyaller. |\n",
    "| **Kontekst KaymasÄ±** â€“ Uzun diyaloÄŸun nerede â€œbÃ¶lÃ¼ndÃ¼ÄŸÃ¼â€ anlaÅŸÄ±lmaz. | Etiketler satÄ±r baÅŸÄ±nda sabit kalÄ±p kontekst hizalamasÄ±nÄ± korur. |\n",
    "\n",
    "#### 2. NasÄ±l Ã‡alÄ±ÅŸÄ±r?\n",
    "1. **Ã–n-iÅŸleme**:  \n",
    "   Her ham cÃ¼mlenin baÅŸÄ±na **rol** ve **stil** tokenâ€™Ä± eklenir:  \n",
    "   ```text\n",
    "   <user> <casual> merhaba\n",
    "   <bot>  <casual> merhaba, nasÄ±l yardÄ±mcÄ± olabilirim?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ebe9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\MasaÃ¼stÃ¼\\Transformerlar\\Konu AnlatÄ±mlarÄ±\\Encoder - Decoder - PE - ATTN ( Ä°LERÄ° DÃœZEY ) = TF\\Tokenizer\\Ã¶rnek_set .csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb4e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼ÄŸ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c305d",
   "metadata": {},
   "source": [
    "* AÅŸaÄŸÄ±da bulunan 2 satÄ±r bu iÅŸlemi anlatÄ±yor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11079438",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER , BOT ,CASUAL = \"<user>\" , \"<bot>\" , \"<casual>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f395895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"{USER} {CASUAL} {clean(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean(t)}\"  for t in raw_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda7be4",
   "metadata": {},
   "source": [
    "* DiÄŸer adÄ±mlar aynÄ± ÅŸekilde iÅŸleme devam eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600460e",
   "metadata": {},
   "source": [
    "## Dynamic UNK-Dropout (Kelime-bazlÄ± Tokenizerâ€™a Zarar Vermeden)\n",
    "\n",
    "### Neden Dinamik?\n",
    "* **Sabit oran** (Ã¶r. 0.10) her epoch boyunca aynÄ± gÃ¼rÃ¼ltÃ¼yÃ¼ verir.  \n",
    "* **Dinamik takvim**:  \n",
    "  - **Erken epochâ€™larda** dÃ¼ÅŸÃ¼k dropout â‡’ model temel yapÄ±yÄ± Ã¶ÄŸrenir.  \n",
    "  - **Orta epochâ€™larda** yÃ¼ksek dropout â‡’ genelleme gÃ¼Ã§lenir.  \n",
    "  - **Sonlara doÄŸru** oranÄ± tekrar dÃ¼ÅŸÃ¼r â‡’ duyarlÄ±lÄ±k kazanÄ±lÄ±r.\n",
    "\n",
    "\n",
    "### Basit Takvim Ã–rneÄŸi  \n",
    "| Epoch AralÄ±ÄŸÄ± | Oran (`rate`) | Yorum |\n",
    "|---------------|--------------|-------|\n",
    "| 0 â€“ 4         | 0.05         | â€œIsÄ±nmaâ€ â€“ saf veriye yakÄ±n |\n",
    "| 5 â€“ 14        | 0.12         | â€œSaÄŸlamlaÅŸtÄ±rmaâ€ |\n",
    "| 15 â€“ 19       | 0.08         | â€œÄ°nce ayarâ€ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153688ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDÄ°NAMÄ°K UNK-DROPOUT + KONUÅMACI/STÄ°L ETÄ°KETLÄ° KELÄ°ME-BAZLI TOKENIZER\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\\nâ€¢ <user> / <bot> + <casual> etiketleri\\nâ€¢ min_freq filtresi (nadir kelimeleri OOVâ€™a at)\\nâ€¢ Dynamic UNK-Dropout takvimi:\\n      epoch 0-4  â†’  0.05\\n      epoch 5-14 â†’  0.12\\n      epoch 15-19â†’  0.08\\nâ€¢ tf.data.Dataset â†’ shuffle â€¢ batch â€¢ prefetch\\n(Masking/model kÄ±smÄ± bu betiÄŸe dÃ¢hil deÄŸil)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DÄ°NAMÄ°K UNK-DROPOUT + KONUÅMACI/STÄ°L ETÄ°KETLÄ° KELÄ°ME-BAZLI TOKENIZER\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "â€¢ <user> / <bot> + <casual> etiketleri\n",
    "â€¢ min_freq filtresi (nadir kelimeleri OOVâ€™a at)\n",
    "â€¢ Dynamic UNK-Dropout takvimi:\n",
    "      epoch 0-4  â†’  0.05\n",
    "      epoch 5-14 â†’  0.12\n",
    "      epoch 15-19â†’  0.08\n",
    "â€¢ tf.data.Dataset â†’ shuffle â€¢ batch â€¢ prefetch\n",
    "(Masking/model kÄ±smÄ± bu betiÄŸe dÃ¢hil deÄŸil)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81cecc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 0) Gereksinimler\n",
    "#    pip install pandas tensorflow==2.18\n",
    "# -----------------------------------------------------------------\n",
    "import re, unicodedata, json, pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52d11ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters        = \"\"            # Noktalama iÅŸleme filtresi\n",
    "oov_token      = \"<unk>\"       # Out-of-vocab token\n",
    "pad_token      = '<pad>'       # Padding token\n",
    "sos_token      = \"<sos>\"       # Decoder baÅŸlangÄ±Ã§ token\n",
    "eos_token      = \"<eos>\"       # Decoder bitiÅŸ token\n",
    "char_level     = False         # True = karakter, False = kelime\n",
    "max_vocab_size = None          # None = sÄ±nÄ±rsÄ±z\n",
    "min_freq       = 2             # En az 2 kez geÃ§en token'lar tutulacak\n",
    "padding       = \"post\"         # \"pre\" da seÃ§ilebilir\n",
    "\n",
    "# === 2. Sabit Token TanÄ±mlarÄ± ===\n",
    "USER, BOT, CASUAL = \"<user>\", \"<bot>\", \"<casual>\"\n",
    "\n",
    "fixed_index = {\n",
    "    pad_token : 0,\n",
    "    oov_token : 1,\n",
    "    sos_token : 2,\n",
    "    eos_token : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼ÄŸ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c33040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"{USER} {CASUAL} {clean_text(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean_text(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ebfef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. SOS/EOS Ekleme ===\n",
    "dec_in_text  = [f\"{sos_token} {t}\" for t in target_texts]\n",
    "dec_out_text = [f\"{t} {eos_token}\" for t in target_texts]\n",
    "all_texts    = input_texts + dec_in_text + dec_out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d88b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- 4. GeÃ§ici Tokenizer -----------------\n",
    "tmp_tok = Tokenizer(filters=filters, oov_token=oov_token, char_level=char_level)\n",
    "tmp_tok.fit_on_texts(all_texts)\n",
    "word_counts = tmp_tok.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56416209",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = [w for w, c in word_counts.items() if c >= min_freq] # min_freq uugulamasÄ±.\n",
    "for special in (USER,BOT,CASUAL,pad_token,sos_token,eos_token):\n",
    "    if special not in keep:\n",
    "        keep_tokens.append(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7f1677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(keep) + 1, filters=filters,oov_token=oov_token ,char_level=char_level)\n",
    "tokenizer.fit_on_texts(keep)\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b727b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq = tokenizer.texts_to_sequences(input_texts)\n",
    "dec_in_seq = tokenizer.texts_to_sequences(dec_in_text)\n",
    "dec_in_out = tokenizer.texts_to_sequences(dec_out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a856bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "maxlen = max(map(len, inp_seq+dec_in_seq+dec_in_out))\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae046b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in  = pad_sequences(inp_seq,    maxlen=maxlen, padding=padding, value=0)\n",
    "dec_in  = pad_sequences(dec_in_seq, maxlen=maxlen, padding=padding, value=0)\n",
    "dec_out = pad_sequences(dec_in_out,maxlen=maxlen, padding=padding, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e4def",
   "metadata": {},
   "source": [
    "# --- Dinamik UNK-dropout iÃ§in deÄŸiÅŸken tanÄ±mÄ± ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ad88bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = tokenizer.word_index[oov_token]\n",
    "drop_var = tf.Variable(0.05 , trainable=False,dtype=tf.float32)\n",
    "\n",
    "def unk_dropout_dyn(inputs,targets):\n",
    "\n",
    "    e = inputs[\"encoder_input\"] ; d = inputs[\"decoder_input\"]\n",
    "    m_e = tf.cast(tf.random.uniform(tf.shape(e)) < drop_var , e.dtype)\n",
    "    m_d = tf.cast(tf.random.uniform(tf.shape(d)) < drop_var , d.dtype)\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(m_e==1 , unk_id, e)\n",
    "    inputs[\"decoder_input\"] = tf.where(m_d==1 , unk_id , d)\n",
    "\n",
    "    return inputs , targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "106dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch , BUFFER = 32, 1000\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((\n",
    "        {\"encoder_input\" : enc_in , \"decoder_input\" : dec_in} , dec_out\n",
    "        ))\n",
    "        .map(unk_dropout_dyn , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(BUFFER)\n",
    "        .batch(batch,drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a44789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callback ile oran takvimi ---\n",
    "\n",
    "def schedule(ep):\n",
    "\n",
    "    if ep < 5: return 0.05\n",
    "    if ep < 15 : return 0.12\n",
    "\n",
    "    return 0.08\n",
    "\n",
    "class UnkScheduler(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        new = schedule(epoch)\n",
    "        drop_var.assign(new)\n",
    "        print(f\"\\n[UNK-Dropout] epoch {epoch} â†’ rate={new:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc31a4",
   "metadata": {},
   "source": [
    "#####  ArtÄ±k model.fit(dataset, epochs=20, callbacks=[UnkScheduler()]) diyebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14773d91",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ff34e",
   "metadata": {},
   "source": [
    "# DÃ¼z Tokenizer vs. Subword-BPE Pipeline: Hangisi Daha Profesyonel ve KullanÄ±ÅŸlÄ±?\n",
    "\n",
    "AÅŸaÄŸÄ±da â€œ1) DÃ¼z kelime-bazlÄ± tokenizer + Dynamic UNK-Dropout + speaker/stil tokenâ€™larÄ±â€ ile â€œ2) SentencePiece-BPE tabanlÄ± tokenizer + UNK-Dropoutâ€ yaklaÅŸÄ±mlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rdÄ±m. Projenizin Ã¶lÃ§eÄŸi, hedefleri ve ekosistemi doÄŸrultusunda hangisinin size daha uygun olduÄŸuna karar verebilirsiniz.\n",
    "\n",
    "| Ã–zellik                          | 1) DÃ¼z Kelime-bazlÄ± Tokenizer                       | 2) Subword-BPE (SentencePiece)                         |\n",
    "|----------------------------------|------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Kurulum / BaÄŸÄ±mlÄ±lÄ±klar**      | Sadece `tensorflow`/`keras`                           | + `sentencepiece` paketi                                |\n",
    "| **SÃ¶zlÃ¼k OluÅŸumu**               | `Tokenizer.fit_on_texts` + `min_freq` + `max_vocab`  | `SentencePieceTrainer.Train` (dÄ±ÅŸ model dosyasÄ±)        |\n",
    "| **OOV (Bilinmeyen Token)**       | YÃ¼ksek (%OOV â‰ˆ 100âˆ’coverage)                         | Pratikte sÄ±fÄ±ra yakÄ±n (her yeni kelime parÃ§alara bÃ¶lÃ¼nÃ¼r) |\n",
    "| **Vocab Boyutu**                 | Kelime sayÄ±sÄ±na gÃ¶re Ã§ok bÃ¼yÃ¼k, `min_freq` ÅŸart      | KÃ¼Ã§Ã¼k ve kontrollÃ¼ (Ã¶rn. 1 000â€“8 000 parÃ§a)             |\n",
    "| **Bellek & Performans**          | BÃ¼yÃ¼k embed matrisi â†’ daha fazla bellek              | KÃ¼Ã§Ã¼k embed matrisi â†’ hem eÄŸitim hem Ã§Ä±karÄ±m hÄ±zlanÄ±r  |\n",
    "| **Dil YapÄ±sÄ± Uyum**              | Eklemeli dillerde (TÃ¼rkÃ§e) Ã§ok nadir tokenâ€™lÄ±        | TÃ¼rkÃ§e gibi eklemeli dillerde doÄŸal subword ayrÄ±ÅŸÄ±mÄ±     |\n",
    "| **KonuÅŸmacÄ±/Stil Tokenâ€™larÄ±**    | Her iki yaklaÅŸmada kolayca eklenir                   | AynÄ± etiketler `user_defined_symbols` ile eklenir       |\n",
    "| **Dynamic UNK-Dropout**          | Callback + `tf.data.map` iÃ§inde uygulamak kolay      | Yine `tf.data.map` ile direkt entegre                  |\n",
    "| **Pipeline KarmaÅŸÄ±klÄ±ÄŸÄ±**        | Daha sade, tek kod bloÄŸu                             | Biraz daha uzun; model eÄŸitimi + encode + pad adÄ±mlarÄ± |\n",
    "| **Model UyumluluÄŸu**             | LSTM/GRU veya basit seq2seq                          | Modern Transformer / BERT/GPT tabanlÄ± mimarilerle birebir uyum |\n",
    "| **Prototip & Ã–lÃ§ek**             | KÃ¼Ã§Ã¼k projeler, hÄ±zlÄ± deneme                         | BÃ¼yÃ¼k veri, Ã§ok dilli veya Ã¼retim odaklÄ±               |\n",
    "\n",
    "\n",
    "\n",
    "## Ne Zaman Hangisi?\n",
    "\n",
    "- **HÄ±zlÄ± Prototip / KÃ¼Ã§Ã¼k Veri**  \n",
    "  DÃ¼z kelime-bazlÄ± tokenizer:\n",
    "  - DÄ±ÅŸ paket kurmadan, birkaÃ§ satÄ±rlÄ±k kodla hazÄ±r.\n",
    "  - Ä°nsan okunurluÄŸu yÃ¼ksek, hata ayÄ±klamak kolay.\n",
    "\n",
    "- **BÃ¼yÃ¼k Veri / Ãœretim / Transformer-Uyumluluk**  \n",
    "  Subword-BPE:\n",
    "  - OOV sorununu pratikte ortadan kaldÄ±rÄ±r.\n",
    "  - SÃ¶zlÃ¼k boyutunu kontrol altÄ±nda tutar.\n",
    "  - Modern NLP modelleriyle tam uyumlu.\n",
    "\n",
    "\n",
    "## SonuÃ§ ve Ã–neri\n",
    "\n",
    "- EÄŸer **TÃ¼rkÃ§e chatbot** projeniz 10 kâ€“100 k cÃ¼mle civarÄ±nda, dÄ±ÅŸ baÄŸÄ±mlÄ±lÄ±klardan kaÃ§Ä±nmak istiyorsanÄ±z, **dÃ¼z tokenizer + dynamic UNK-dropout** gayet yeterli olacaktÄ±r.\n",
    "- EÄŸer **veri setiniz milyonlarca** cÃ¼mleye ulaÅŸÄ±yor, OOVâ€™larÄ± sÄ±fÄ±ra yakÄ±n gÃ¶rmek ve **Transformer**/**GPT-like** mimarilerle Ã¼retim kalitesi hedefliyorsanÄ±z, **SentencePiece-BPE** Ã§Ã¶zÃ¼mÃ¼nÃ¼ tercih edin.\n",
    "\n",
    "Her iki pipeline da konuÅŸmacÄ±/stil tokenâ€™larÄ± ve dinamik augmentasyon adÄ±mlarÄ±nÄ± destekler. Projeye ilk dÃ¼z tokenizerâ€™la baÅŸlayÄ±p, Ã¶lÃ§ek ve ihtiyaÃ§ arttÄ±kÃ§a BPEâ€™ye geÃ§iÅŸ yapmak, en az israfla en hÄ±zlÄ± sonuÃ§ almanÄ±za yardÄ±mcÄ± olur.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d370d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
