{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf7e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938021ed",
   "metadata": {},
   "source": [
    "# TOKENİZER - TENSORFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ebf0b",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbfa4a",
   "metadata": {},
   "source": [
    "## 🔤 Tokenizer Nedir?\n",
    "\n",
    "**Tokenizer**, ham metni modelin anlayabileceği sayısal dizilere dönüştüren araçtır. Her kelimeye (veya karaktere) bir ID atar.\n",
    "\n",
    "\n",
    "\n",
    "### 🎯 Temel Görevi\n",
    "- Metni bölmek (tokenize etmek)\n",
    "- Her tokena bir numara vermek (vocab oluşturmak)\n",
    "- Bu numaralara göre diziler üretmek\n",
    "\n",
    "\n",
    "\n",
    "### ⚙️ Kullanım Parametreleri\n",
    "\n",
    "| Parametre     | Açıklama |\n",
    "|---------------|----------|\n",
    "| `oov_token`   | Eğitimde görülmeyen kelimeler için özel token (`<OOV>`) |\n",
    "| `filters`     | Hangi karakterlerin silineceği (örn. noktalama) |\n",
    "| `char_level`  | `True`: karakter bazlı, `False`: kelime bazlı tokenize |\n",
    "\n",
    "\n",
    "\n",
    "### 🧱 Adımlar\n",
    "\n",
    "1. **Tokenizer oluştur**  \n",
    "   ```python\n",
    "   tokenizer = Tokenizer(oov_token=\"<OOV>\", filters=\"\", char_level=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4e0c1",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13222e",
   "metadata": {},
   "source": [
    "## 🔑 SOS ve EOS Tokenları Nedir?\n",
    "\n",
    "**SOS (Start of Sequence)** ve **EOS (End of Sequence)** tokenları, dizilerle çalışan modellerde giriş ve çıkış dizilerinin sınırlarını belirlemek için kullanılan özel işaretleyicilerdir.\n",
    "\n",
    "\n",
    "\n",
    "### 🎯 Amaçları\n",
    "- **sos**: Modelin dizinin **başladığını** anlamasını sağlar.\n",
    "- **eos**: Modelin dizinin **bittiğini** anlamasını sağlar.\n",
    "\n",
    "\n",
    "\n",
    "### 📦 Kullanım Alanları\n",
    "- **Seq2Seq modellerinde** (örneğin: encoder-decoder yapıları)\n",
    "- **Makine çevirisi**\n",
    "- **Dil modeli eğitimi ve tahmini**\n",
    "- **Beam Search gibi sıralı üretim algoritmalarında** üretimin ne zaman duracağını belirlemek için\n",
    "\n",
    "\n",
    "\n",
    "### 🧱 Tipik Uygulama Şekli\n",
    "**Hedef metinler** şu şekilde hazırlanır:\n",
    "\n",
    "- `target_input`: `sos` + gerçek cümle  \n",
    "- `target_output`: gerçek cümle + `eos`\n",
    "\n",
    "Bu sayede model:\n",
    "- Ne zaman üretmeye başlayacağını (sos),\n",
    "- Ne zaman durması gerektiğini (eos) öğrenmiş olur.\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Örnek\n",
    "Gerçek cümle: `Merhaba dünya`\n",
    "\n",
    "- `target_input`: `sos Merhaba dünya`  \n",
    "- `target_output`: `Merhaba dünya eos`\n",
    "\n",
    "\n",
    "### 💡 Not\n",
    "- Bu tokenlar, tokenizer'a *manüel* olarak metnin bir parçası gibi eklenmelidir.\n",
    "- Tokenizer, onları kelime listesine dahil ederek ID atar (örneğin: `sos → 2`).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a2e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasıl yardımcı olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nasılsın?</td>\n",
       "      <td>İyiyim, teşekkür ederim. Siz nasılsınız?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adın ne?</td>\n",
       "      <td>Ben bir yapay zekâ asistanıyım. Adım yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaç yaşındasın?</td>\n",
       "      <td>Benim yaşım yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bugün günlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasıl yardımcı olabilirim?\n",
       "1            Nasılsın?           İyiyim, teşekkür ederim. Siz nasılsınız?\n",
       "2             Adın ne?  Ben bir yapay zekâ asistanıyım. Adım yok ama y...\n",
       "3      Kaç yaşındasın?                        Benim yaşım yok, dijitalim!\n",
       "4  Bugün günlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"örnek_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da620d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['input'].astype(str).to_list()\n",
    "target_texts = df['output'].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31202ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "token_in = ['<start> ' + text for text in target_texts]\n",
    "token_out = [text + ' <end>' for text in target_texts]\n",
    "\n",
    "input_token = Tokenizer(filters='' , oov_token='<OOV>')\n",
    "input_token.fit_on_texts(input_texts)\n",
    "\n",
    "target_token = Tokenizer(filters='' , oov_token='<OOV>')\n",
    "target_token.fit_on_texts(token_in + token_out)\n",
    "\n",
    "input_seq = input_token.texts_to_sequences(input_texts)\n",
    "decoder_in = target_token.texts_to_sequences(token_in)\n",
    "decoder_out = target_token.texts_to_sequences(token_out)\n",
    "\n",
    "max_len_inp = max(len(seq) for seq in input_seq)\n",
    "max_len_out = max(len(seq) for seq in decoder_in + decoder_out)\n",
    "\n",
    "max_len = max(max_len_inp , max_len_out)\n",
    "\n",
    "encoder_input = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "decoder_input = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "decoder_target = pad_sequences(input_seq , maxlen = max_len , padding=\"post\")\n",
    "\n",
    "inp_vocab_size = len(input_token.word_index) + 1 \n",
    "tar_vocab_size = len(target_token.word_index) + 1\n",
    "\n",
    "print(inp_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738418d",
   "metadata": {},
   "source": [
    "* Yukarıda bulunan bu kod klasik bir yapı.Bizim amacımız bu yapıya daha fazla derinlik katmak.Normal tokenizer işlevlerine zaten hakimiz.Yani bu yapıyı küçük-orta ölçekli yerlerde kullanabilirsiniz.Gayet de iş görür.Ama bu notebookdaki temel amaç ön işleme adımlarını daha ciddi yerlere getirmek.Gelin bu yapılar neymiş onlara bakalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3f3e2",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695dc1a",
   "metadata": {},
   "source": [
    "## 📌 Neler Eklenebilir ?\n",
    "\n",
    "### 1. `char_level`  \n",
    "**Ne yapar?**  \n",
    "- Tokenizer’ı kelime (`word`) yerine karakter (`char`) bazlı çalışacak şekilde ayarlar.\n",
    "\n",
    "**Neden önemli?**  \n",
    "- Çok küçük veri setlerinde kelime bazlı model **aşırı “sparse”** (seyreklik) yaşayabilir.  \n",
    "- Yeni (out-of-vocab) kelimelerin tamamını `<OOV>` yerine, karakter dizisi sayesinde anlamlı parçalara ayırabilirsiniz.  \n",
    "- Morfolojik açıdan zengin (örneğin Türkçe) dillerde **kök-ek ayrımı** yapmak kolaylaşır.\n",
    "\n",
    "\n",
    "### 2. `max_vocab_size`  \n",
    "**Ne yapar?**  \n",
    "- Tokenizer’a kaç adet en sık görülen token’ı “aktif” tutacağını söylersiniz; geriye kalanlar hepsi `<OOV>` olur.\n",
    "\n",
    "**Neden önemli?**  \n",
    "- **Bellek & hız**: Gigabaytlarca sözlüğü modelde tutmak yerine, en sık kullanılan 5k–10k token’ı alıp gerisini atmak.  \n",
    "- **Genelleme**: Aşırı nadir kelimeler eğitim sırasında gürültü oluşturabilir; onları `<OOV>` yapmak modelin daha sağlam öğrenmesini sağlar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4a9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ''\n",
    "oov_token = '<OOV>'\n",
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "char_level = False\n",
    "max_vocab_size = 10000\n",
    "padding=\"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6ccbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_in = [f\"{start_token} {t}\" for t in target_texts]\n",
    "token_out = [f\"{t} {end_token}\" for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5799eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = Tokenizer(\n",
    "    num_words=max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    "                        )\n",
    "\n",
    "target_token = Tokenizer(\n",
    "    num_words=max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472afef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token.fit_on_texts(input_texts)\n",
    "target_token.fit_on_texts(token_in+token_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3621febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = input_token.texts_to_sequences(input_texts)\n",
    "decoder_in = target_token.texts_to_sequences(token_in)\n",
    "decoder_out = target_token.texts_to_sequences(token_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0428553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "max_len_inp = max(len(seq) for seq in input_seq)\n",
    "max_len_out = max(len(seq) for seq in decoder_in + decoder_out)\n",
    "max_len = max(max_len_inp , max_len_out)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7799c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(input_seq,maxlen = max_len, padding=padding)\n",
    "decoder_input = pad_sequences(decoder_in,maxlen = max_len, padding=padding)\n",
    "decoder_target = pad_sequences(decoder_out,maxlen = max_len, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b273f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_size = max(len(input_token.word_index) + 1 , max_vocab_size or float(\"inf\"))\n",
    "tar_vocab_size = max(len(target_token.word_index) + 1 , max_vocab_size or float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93236c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size:  10000\n",
      "Target vocab size: 10000\n",
      "<start> token id: 2\n",
      "<end> token id: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Input vocab size: \", inp_vocab_size)\n",
    "print(\"Target vocab size:\", tar_vocab_size)\n",
    "print(f\"{start_token} token id:\", target_token.word_index[start_token])\n",
    "print(f\"{end_token} token id:\", target_token.word_index[end_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1372fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girdi örneği (IDs→text): merhaba\n",
      "Decoder-in örneği: <start> merhaba, size nasıl yardımcı olabilirim?\n",
      "Decoder-out örneği: merhaba, size nasıl yardımcı olabilirim? <end>\n",
      "Temizlenmiş target: merhaba, size nasıl yardımcı olabilirim?\n"
     ]
    }
   ],
   "source": [
    "# ==== TOKENIZER SONRASI HIZLI KONTROL ====\n",
    "\n",
    "# 1) Ham ID dizilerini direkt geri metne çevir:\n",
    "sample_idx = 0  # ilk örneği incele\n",
    "print(\"Girdi örneği (IDs→text):\",\n",
    "      input_token.sequences_to_texts([ input_seq[sample_idx] ])[0])\n",
    "print(\"Decoder-in örneği:\",\n",
    "      target_token.sequences_to_texts([ decoder_in[sample_idx] ])[0])\n",
    "print(\"Decoder-out örneği:\",\n",
    "      target_token.sequences_to_texts([ decoder_out[sample_idx] ])[0])\n",
    "\n",
    "# 2) <start> ve <end>'i temizleyip nihai cümleyi al:\n",
    "raw = target_token.sequences_to_texts([ decoder_out[sample_idx] ])[0]\n",
    "clean = raw.replace(f\"{start_token} \", \"\").replace(f\" {end_token}\", \"\").strip()\n",
    "print(\"Temizlenmiş target:\", clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f85d9e",
   "metadata": {},
   "source": [
    "----\n",
    "## 🚀 Profesyonel Tokenizer Pipeline İyileştirmeleri\n",
    "\n",
    "Aşağıda “sadece çalışıp geçmek” yerine **güvenilir**, **ölçeklenebilir** ve **esnek** bir preprocessing pipeline’ı oluşturmak için başlıca adımlar özetlenmiştir.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Temizlik & Normalizasyon\n",
    "- **Unicode Normalization** (`NFC`/`NFKC`):  \n",
    "  Farklı biçimlerde kodlanmış karakterleri tekilleştirir.\n",
    "- **Lowercasing**:  \n",
    "  Tüm metni küçük harfe indirerek sözlükteki duplikasyonları önler.\n",
    "- **Diakritik/Karak­ter Temizleme**:  \n",
    "  Türkçe “ç, ş, ı” gibi işaretleri gerektiğinde standartlaştırır.\n",
    "\n",
    "\n",
    "### 2. Özel Token’lar & Sabitleme\n",
    "- **\\<pad\\>** token’ını açıkça tanımlayıp index’ini sıfır yapın.  \n",
    "- **\\<unk\\>** veya **\\<OOV\\>** farkını belirleyin ve sabitleyin.  \n",
    "- `fit_on_texts` öncesi özel token’ları manuel ekleyerek aynı ID’leri garantileyin.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Vocab Analizi & Sınırlandırma\n",
    "- **max_vocab_size**: En sık kullanılan N token’ı aktif tutar, gerisi `<OOV>`.  \n",
    "- **min_freq**: Belirli bir frekans altındaki token’ları da otomatik atar.  \n",
    "- **Coverage Raporu**: Toplam tokenların % kaçı top-N vocab’a giriyor?  \n",
    "\n",
    "\n",
    "\n",
    "### 4. Subword / BPE / WordPiece Desteği\n",
    "- **SentencePiece** veya **HuggingFace Tokenizers** kullanarak:  \n",
    "  - Byte-Pair Encoding (BPE)  \n",
    "  - Unigram LM  \n",
    "  - WordPiece  \n",
    "- Böylece OOV sorunu en aza iner, vocab boyutu küçülür.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Parametrik & Modüler Yapı\n",
    "- Expose edilecek parametreler:  \n",
    "  - `lowercase: bool`  \n",
    "  - `normalize_unicode: bool`  \n",
    "  - `filters: str`  \n",
    "  - `min_freq: int`  \n",
    "  - `max_vocab_size: int`  \n",
    "  - `char_level: bool`  \n",
    "  - `tokenizer_backend: \"keras\" | \"sentencepiece\" | \"hf\"`  \n",
    "\n",
    "\n",
    "\n",
    "### 6. Kaydetme / Versiyonlama / Geri Yükleme\n",
    "- **JSON + Metadata** (parametreler, min_freq vs.) bir arada saklayın.  \n",
    "- Versiyonlama: `tokenizer_v1.0.json`, `tokenizer_v1.1.json`…  \n",
    "- Aynı pipeline’ı projeler arası tekrar kullanmak için.\n",
    "\n",
    "\n",
    "\n",
    "### 7. Performans & Ölçeklenebilirlik\n",
    "- **Rust-tabanlı** tokenizers (HuggingFace) ile paralel hız.  \n",
    "- **Disk cache**: TFRecord/LMDB gibi formatlarda tokenize edilmiş veriyi saklama.  \n",
    "- `tf.data.Dataset` içinde “cache → map → batch → prefetch” akışı.\n",
    "\n",
    "\n",
    "\n",
    "### 8. Debug & İzlenebilirlik\n",
    "- **Sample Çıktılar**: Her adımda 3–5 örnek  \n",
    "- **Logging**:  \n",
    "  - Toplam kelime sayısı vs. vocab boyutu  \n",
    "  - Padding/truncation dağılımı  \n",
    "- **Unit Test**:  \n",
    "  - Edge-case cümleler  \n",
    "  - Özel token ID kontrolü  \n",
    "  - Truncation & padding davranışı\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7fab7",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098de60",
   "metadata": {},
   "source": [
    "### 1. Lowercasing ve Unicode normalize etmek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790de84",
   "metadata": {},
   "source": [
    "* Basit bir clean_text() fonksiyonu yazıp, fit_on_texts çağrısından önce tüm metinleri ona geçiriyoruz.\n",
    "\n",
    "* Bunu tokenizerların en başına ekliyoruz.\n",
    "\n",
    "* Böylece hem bütün metinler aynı formata gelir hem tokenizer’a “gereksiz” karakterler girmez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63dee88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_texts(text,lowercase=True,normalize_unicod = True):\n",
    "\n",
    "    if normalize_unicod:\n",
    "        text = unicodedata.normalize(\"NFC\",text)\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zçğıöşü0-9\\s]\", \" \", text)\n",
    "    # birden fazla arka arkaya boşluğu tek boşluk yap\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "input_texts = [clean_texts(t) for t in df[\"input\"].astype(str)]\n",
    "target_texts = [clean_texts(t) for t in df[\"output\"].astype(str)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8577566",
   "metadata": {},
   "source": [
    "---\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24648604",
   "metadata": {},
   "source": [
    "## Özel Token’lar & Sabitleme (Teori)\n",
    "\n",
    "### Neden Özel Token’lar?\n",
    "- **\\<pad\\>**  \n",
    "  - Dizilerin sabit uzunluğa getirilmesi için kullanılan dolgu (padding) token’ı.  \n",
    "  - Modelin gerçek veriden ayırt edebilmesi için özel bir ID’ye sahip olmalıdır.\n",
    "\n",
    "- **\\<unk\\> / \\<OOV\\>**  \n",
    "  - Eğitim sırasında görülmeyen veya nadir kelimeleri temsil eder.  \n",
    "  - Bilinmeyen kelimelerin modelde tutarlı şekilde işlenmesini sağlar.\n",
    "\n",
    "- **\\<sos\\> (Start of Sequence)**  \n",
    "  - Decoder’ın diziyi nereden üretmeye başlayacağını belirtir.  \n",
    "  - Sıralı üretim algoritmalarında mutlaka ilk token olarak kullanılır.\n",
    "\n",
    "- **\\<eos\\> (End of Sequence)**  \n",
    "  - Decoder’ın üretimi nerede durduracağını işaretler.  \n",
    "  - Özellikle beam search veya greedy decoding sırasında önem taşır.\n",
    "\n",
    "\n",
    "\n",
    "### Sabitlemenin Amacı\n",
    "1. **Tutarlılık**  \n",
    "   - Aynı token her çalışmada aynı ID’ye karşılık gelmeli.  \n",
    "   - Özellikle eğitim ve çıkarım (inference) ortamları arasında uyumluluk sağlar.\n",
    "\n",
    "2. **Padding Güvenliği**  \n",
    "   - \\<pad\\> token’ının **ID = 0** olması, `pad_sequences` gibi fonksiyonların varsayılan dolgu değeriyle örtüşmesini garantiler.\n",
    "\n",
    "3. **OOV Yönetimi**  \n",
    "   - \\<unk\\> token’ının sabit bir ID’ye sahip olması, bilinmeyen kelimelerin daima aynı şekilde kodlanmasını sağlar ve modelin genel performansını korur.\n",
    "\n",
    "4. **Decoder Kontrolü**  \n",
    "   - \\<sos\\> ve \\<eos\\> token’larının ID’leri değişmediği sürece, dizinin başlangıç ve bitiş sınırları model tarafından kesinlikle doğru algılanır.\n",
    "\n",
    "\n",
    "### Sabitleme Adımları (Özet)\n",
    "- Özel token’ları içeren bir sabit eşleme (`word → ID`) tanımlanır.  \n",
    "- Tokenizer başlatıldıktan hemen sonra bu sabit eşleme yüklenir, böylece özel token’lar kesin ID’lere sahip olur.  \n",
    "- Ardından gerçek metin verisi ile tokenizasyon işlemi yapılır; özel token’ların ID’leri hiçbir zaman değişmez.\n",
    "\n",
    "\n",
    "### Faydaları\n",
    "- Modelin hem eğitim hem de çıkarım aşamasında **öngörülebilir ve tutarlı** davranmasını sağlar.  \n",
    "- **Kodun bakımı** ve **yeniden üretilebilirlik** (reproducibility) açısından kritik bir uygulamadır.  \n",
    "- Sıralı metin üretimi (seq2seq, Transformer) gibi ileri düzey görevlerde hata yapmayı engeller.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189db7d",
   "metadata": {},
   "source": [
    "## `min_freq` (Minimum Frequency) Teorisi\n",
    "\n",
    "### Neden Gerekli?\n",
    "- **Gürültü Azaltma**  \n",
    "  - Veri setindeki bazı kelimeler yalnızca birkaç kez geçer; bu nadir kelimeler modelin öğrenme sürecini olumsuz etkileyebilir.  \n",
    "- **Vocab Boyutunu Kontrol Etme**  \n",
    "  - Büyük metin koleksiyonlarında onbinlerce farklı token olabilir.  \n",
    "  - Sözlüğü küçültmek, hem bellek kullanımını hem de eğitim süresini iyileştirir.\n",
    "\n",
    "\n",
    "### Nasıl Çalışır?\n",
    "1. **Frekans Eşiği**  \n",
    "   - Bir token’ın sözlüğe dahil edilmesi için **en az** `min_freq` kez görünmesi gerekir.\n",
    "2. **Filtreleme**  \n",
    "   - `min_freq = 3` ise, veri setinde 1 veya 2 kez görülen tüm token’lar  \n",
    "     - Model girişinde `<unk>` (OOV) olarak işaretlenir  \n",
    "     - Sözlükte yer almaz  \n",
    "3. **Sonuç**  \n",
    "   - Sözlükte sadece **yeterli sıklıkta** geçen token’lar kalır  \n",
    "   - Nadir token’lar, modelin “garbage” öğrenmesini önler\n",
    "\n",
    "\n",
    "### Faydaları\n",
    "- **Daha Temiz Vocab**  \n",
    "  - Gerçekten anlamlı ve istatistiksel olarak güçlü token’lar tutulur.  \n",
    "- **Daha İyi Genelleme**  \n",
    "  - Nadir ve gürültülü token’lar OOV’a düştüğü için model, ana kalıpları daha iyi kavrar.  \n",
    "- **Kaynak Verimliliği**  \n",
    "  - Küçük bir sözlük, daha az bellek ve daha hızlı matris işlemleri demektir.\n",
    "\n",
    "\n",
    "### Uygulama Adımları (Teori)\n",
    "1. **Ön Analiz**  \n",
    "   - Tüm metinlerdeki token frekanslarını topla  \n",
    "2. **Eşik Belirleme**  \n",
    "   - `min_freq` değerini seç (örn. 2–5)  \n",
    "3. **Token Seçimi**  \n",
    "   - Sözlüğe sadece `freq ≥ min_freq` olan token’ları ekle  \n",
    "4. **OOV Atama**  \n",
    "   - Geri kalan tüm token’lar `<unk>` olarak işaretlenir  \n",
    "\n",
    "\n",
    "\n",
    "**Not:** `min_freq` değeri çok yüksek ayarlanırsa önemli ama az geçen kelimeler de OOV olur; çok düşük ayarlanırsa nadir gürültüye izin verir. İdeal `min_freq`, veri setinin büyüklüğüne ve çeşitliliğine bağlı olarak deneysel belirlenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16698e",
   "metadata": {},
   "source": [
    "* Şimdi bütün bu teorik konularını diğer öğrendiğimiz yapılarla birleştirip yola devam edelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a125ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Temizleyici Fonksiyon ===\n",
    "def clean_text(text, lowercase=True, normalize_unicode=True):\n",
    "    if normalize_unicode:\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"[^a-zçı0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "50b776a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Parametreler ===\n",
    "filters        = \"\"           # noktalama silme filtresi\n",
    "oov_token      = \"<unk>\"\n",
    "pad_token      = \"<pad>\"\n",
    "sos_token      = \"<sos>\"\n",
    "eos_token      = \"<eos>\"\n",
    "char_level     = False        # True=char, False=word\n",
    "max_vocab_size = None         # None = sınırsız\n",
    "min_freq       = 2            # en az 2 kez geçen tokenʼlar kalacak\n",
    "padding        = \"post\"\n",
    "\n",
    "fixed_index = {\n",
    "    pad_token : 0,\n",
    "    oov_token : 1,\n",
    "    sos_token : 2,\n",
    "    eos_token : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c87a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Veri Yükleme & Temizleme ===\n",
    "df = pd.read_csv(\"örnek_set.csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a183acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts  = [clean_text(t) for t in raw_inputs]\n",
    "target_texts = [clean_text(t) for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cd270696",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in_text  = [f\"{sos_token} {t}\" for t in target_texts]\n",
    "dec_out_text = [f\"{t} {eos_token}\" for t in target_texts]\n",
    "all_texts    = input_texts + dec_in_text + dec_out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09c1c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Geçici Tokenizer ile Frekansları Topla ===\n",
    "tmp_tok = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    ")\n",
    "tmp_tok.fit_on_texts(all_texts)\n",
    "word_counts = tmp_tok.word_counts  # OrderedDict {word: count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac726467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. min_freq Uygula ===\n",
    "if min_freq is not None:\n",
    "    keep_tokens = [w for w, cnt in word_counts.items() if cnt >= min_freq]\n",
    "else:\n",
    "    keep_tokens = list(word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7573ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6. max_vocab_size Uygula ===\n",
    "if max_vocab_size is not None:\n",
    "    keep_tokens = sorted(\n",
    "        keep_tokens,\n",
    "        key=lambda w: word_counts[w],\n",
    "        reverse=True\n",
    "    )[: max_vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2463f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Final Tokenizer Oluştur ve Fit Et ===\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=(len(keep_tokens) + 1) if max_vocab_size is None else max_vocab_size,\n",
    "    filters=filters,\n",
    "    oov_token=oov_token,\n",
    "    char_level=char_level\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46f39a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Önce sadece keep listesiyle fit, böylece index sabitlenir\n",
    "tokenizer.fit_on_texts(keep_tokens)\n",
    "# Ardından tüm metinlerle fit\n",
    "tokenizer.fit_on_texts(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "646933e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Sequence Dönüşümü & Padding ===\n",
    "inp_seq    = tokenizer.texts_to_sequences(input_texts)\n",
    "dec_in_seq = tokenizer.texts_to_sequences(dec_in_text)\n",
    "dec_out_seq= tokenizer.texts_to_sequences(dec_out_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "09df0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max(\n",
    "    max(len(s) for s in inp_seq),\n",
    "    max(len(s) for s in dec_in_seq),\n",
    "    max(len(s) for s in dec_out_seq)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "72e0958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input  = pad_sequences(inp_seq,    maxlen=maxlen, padding=padding)\n",
    "decoder_input  = pad_sequences(dec_in_seq, maxlen=maxlen, padding=padding)\n",
    "decoder_target = pad_sequences(dec_out_seq,maxlen=maxlen, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06fd9e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab size:    329\n",
      "Max sequence length: 14\n",
      "'<sos>' id: 2\n",
      "'<eos>' id: 3\n"
     ]
    }
   ],
   "source": [
    "vocab_size = min(len(tokenizer.word_index) + 1, max_vocab_size or float(\"inf\"))\n",
    "print(\"Final vocab size:   \", vocab_size)\n",
    "print(\"Max sequence length:\", maxlen)\n",
    "print(f\"{sos_token!r} id:\", tokenizer.word_index.get(sos_token))\n",
    "print(f\"{eos_token!r} id:\", tokenizer.word_index.get(eos_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary coverage (min_freq=2): 94.61%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nVerideki her 100 token’ın yaklaşık 94.6’sı, sözlüğünüzde tuttuğunuz token’lar tarafından karşılanıyor. \\nGeriye kalan ~5.4% ise nadir token’lar (1 kez geçenler) olduğu için <unk> olarak işleniyor.\\n\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage = sum(word_counts[w] for w in keep_tokens) / sum(word_counts.values())\n",
    "print(f\"Vocabulary coverage (min_freq={min_freq}): {coverage:.2%}\")\n",
    "\n",
    "'''\n",
    "Verideki her 100 token’ın yaklaşık 94.6’sı, sözlüğünüzde tuttuğunuz token’lar tarafından karşılanıyor. \n",
    "Geriye kalan ~5.4% ise nadir token’lar (1 kez geçenler) olduğu için <unk> olarak işleniyor.\n",
    "\n",
    "Ne kadar yüksek ise modelinizin gördüğü gerçek kelimeleri ne kadar iyi kapsadığını gösterir.\n",
    "\n",
    "Çok düşükse, belki min_freq veya max_vocab_size’ı yeniden düşünmek gerekir.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "810f8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens:\n",
      "<unk>      → 1\n",
      "<sos>      → 2\n",
      "<eos>      → 3\n",
      "bir        → 4\n",
      "g          → 5\n",
      "ama        → 6\n",
      "n          → 7\n",
      "i          → 8\n",
      "ben        → 9\n",
      "ne         → 10\n",
      "m          → 11\n",
      "yardımcı   → 12\n",
      "yok        → 13\n",
      "de         → 14\n",
      "ba         → 15\n",
      "ve         → 16\n",
      "nedir      → 17\n",
      "r          → 18\n",
      "python     → 19\n",
      "ya         → 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 tokens:\")\n",
    "for word, idx in list(tokenizer.word_index.items())[:20]:\n",
    "    print(f\"{word:<10} → {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41f9a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input text:   merhaba <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Sample decoder-in:   <sos> merhaba size nasıl yardımcı olabilirim <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Sample decoder-out:  merhaba size nasıl yardımcı olabilirim <eos> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample input text:  \", tokenizer.sequences_to_texts([encoder_input[0]])[0])\n",
    "print(\"Sample decoder-in:  \", tokenizer.sequences_to_texts([decoder_input[0]])[0])\n",
    "print(\"Sample decoder-out: \", tokenizer.sequences_to_texts([decoder_target[0]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3980ec1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## tf.data.Dataset Entegrasyonu (Teori)\n",
    "\n",
    "**Nedir?**  \n",
    "TensorFlow’un `tf.data.Dataset` API’si, önceden tokenize edip pad ettiğiniz NumPy dizileri veya diğer veri kaynaklarını “pipeline” olarak adlandırılan akışa dönüştürür. Bu akış üzerinde **shuffle**, **batch**, **cache**, **map** ve **prefetch** gibi işlemleri zincirleyerek, modelin eğitim sırasında veriyle kesintisiz ve verimli bir şekilde beslenmesini sağlar.\n",
    "\n",
    "\n",
    "## Ne Zaman Kullanılır?\n",
    "- **Büyük veri setleri** belleğe sığmadığında veya diskten “streaming” okumak gerektiğinde  \n",
    "- Eğitim sırasında **shuffle** ve **batch** işlemlerini CPU/GPU ile paralel hale getirerek performansı artırmak istediğinizde  \n",
    "- **Veri augmentasyonu** veya ek ön işleme adımlarını (örneğin, `map` ile token filtreleme) model akışına dahil etmek istediğinizde  \n",
    "- TFRecord, CSV, görüntü dosyaları gibi formatlardan doğrudan okuma ve önbellekleme (`cache`) gerektiğinde  \n",
    "\n",
    "\n",
    "\n",
    "## Entegrasyon Aşamaları\n",
    "1. **Tokenizer & Padding**  \n",
    "   - Ham metni `texts_to_sequences` ve `pad_sequences` ile NumPy dizilerine dönüştürün.  \n",
    "2. **Dataset Oluşturma**  \n",
    "   - `tf.data.Dataset.from_tensor_slices((inputs_dict, targets))` ile veri akışını başlatın.  \n",
    "3. **Pipeline Adımları**  \n",
    "   - **Shuffle:** Rastgelelik için `shuffle(buffer_size)`  \n",
    "   - **Batch:** GPU/TPU’ya uygun sabit boyutlarda gruplama için `batch(batch_size)`  \n",
    "   - **Prefetch:** Bir sonraki batch’i önceden yüklemek için `prefetch(tf.data.AUTOTUNE)`  \n",
    "   - İsteğe bağlı: `cache()`, `repeat()` ve paralel `map()` gibi fonksiyonlar.  \n",
    "4. **Model.fit**  \n",
    "   - Oluşturduğunuz `dataset` objesini doğrudan `model.fit(dataset, epochs=…)` ile kullanın.  \n",
    "\n",
    "\n",
    "## Nerede Yazılmalı?\n",
    "- **Tokenize & pad** adımından hemen sonra,  \n",
    "- **Model tanımı** ve `.compile()` çağrısından **önce**,  \n",
    "- Böylece `dataset` hazır olduktan sonra doğrudan `model.fit(dataset…)` satırını yazabilirsiniz.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a3d0c",
   "metadata": {},
   "source": [
    "* Modelin oluşumundan hemen önce ya da fit fonksiyonundan hemen önce yapmanız gerekecek.Notebook sonunda yapılacak olan projede gösterilecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b8f43711",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4f537ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    \n",
    "    { \n",
    "      \"encoder_input\" : encoder_input,\n",
    "      \"decoder_input\" : decoder_input\n",
    "    },\n",
    "    decoder_target\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    # Shuffle: veri kümesindeki örnekleri karıştırarak her epoch’ta farklı sıralama sağlar.\n",
    "    # buffer_size: kaç örneğin bellekte karıştırılacağını belirler.\n",
    "    #   - Büyük değer = daha random sıra, daha fazla bellek kullanımı.\n",
    "    .shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # Batch: örnekleri batch_size kadar gruplar.\n",
    "    # batch_size: her adımda modele kaç örnek yollanacağını belirler.\n",
    "    # drop_remainder=True: son batch’te batch_size’a tamamlanamayan\n",
    "    #   eksik grup varsa, o küçük batch’i atlar.\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch: bir sonraki batch’i eğitim çalışırken önceden hazırlar.\n",
    "    # tf.data.AUTOTUNE: TensorFlow’un uygun sayıda thread seçmesini sağlar.\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7600d",
   "metadata": {},
   "source": [
    "* Daha da ileriye taşımak istersek ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# === tf.data.Dataset Entegrasyonu ===\n",
    "\n",
    "# 1) NumPy dizilerinden Dataset oluştur:\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "            \"encoder_input\": encoder_input,  # Encoder’a gidecek padded giriş\n",
    "            \"decoder_input\": decoder_input   # Decoder’a gidecek padded giriş\n",
    "        },\n",
    "        decoder_target  # Modelin öğreneceği gerçek çıktılar\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Pipeline adımları:\n",
    "dataset = (\n",
    "    dataset\n",
    "    .cache()                            # 1. İlk kullanımda cache’le (bellek/disk), sonraki epoch’lar için hızlı erişim\n",
    "    .shuffle(buffer_size=buffer_size)   # 2. Veriyi buffer_size kadar örnekle karıştırarak rastgelelik ekle\n",
    "    .repeat()                           # 3. Dataset’i sonsuz (veya istenen sayıda) tekrar et\n",
    "    .batch(batch_size, drop_remainder=True)  # 4. batch_size kadar gruplar; eksik batch’leri atar\n",
    "    .prefetch(tf.data.AUTOTUNE)        # 5. Bir sonraki batch’i arka planda hazırla\n",
    ")\n",
    "\n",
    "# Artık dataset, doğrudan model.fit ile kullanılmaya hazır:\n",
    "# model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13617e1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##  Morfolojik Analiz  \n",
    "\n",
    "**Kavram:**  \n",
    "Türkçe gibi eklemeli (agglutinative) dillerde, bir kelime tek bir string’de kök + çok sayıda ek barındırır.  \n",
    "Morfolojik analiz, bu kelimeyi “kök” ve “ek” bileşenlerine ayırarak anlamsal yapıyı ortaya çıkarır.\n",
    "\n",
    "\n",
    "### Nasıl Çalışır?  \n",
    "1. **Kök Çıkarımı (Stemming/Lemmatization)**  \n",
    "   - “yapabilir­-dim” → kök: “yap”, ekler: [“-abilir”, “-dim”]  \n",
    "2. **Ek Sınıflandırması**  \n",
    "   - Çekim ekleri (“-dim”, “-siniz”) vs. yapım ekleri (“-lük”, “-ci”)  \n",
    "3. **Sözlük & Kurallar**  \n",
    "   - Zemberek gibi kütüphaneler, kapsamlı morfolojik sözlük + dilbilgisi kural seti kullanır.  \n",
    "4. **Çıktı**  \n",
    "   - Hem kök hem de ek türleri elde edilir; örnek:  \n",
    "     ```json\n",
    "     {\n",
    "       \"surface\": \"yapabilir­dim\",\n",
    "       \"lemma\": [\"yap\"],\n",
    "       \"tags\": [\"Verb\",\"Ability\",\"Past\",\"FirstPerson\"]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### Kullanılabilecek Araçlar  \n",
    "- **Zemberek (Java)**  \n",
    "  - `turkish-morphology` modülü  \n",
    "  - Python için `py4j` üzerinden erişim veya `zemberek-python` sarımları  \n",
    "- **TRnlp / TRmorph**  \n",
    "  - Saf Python implementasyonlar  \n",
    "- **SpaCy + tr_spacy**  \n",
    "  - SpaCy çerçevesi içinde Türkçe morfoloji ekleri  \n",
    "- **Stanza (Stanford NLP)**  \n",
    "  - Çok dilli morfolojik analiz, Türkçe desteği de var\n",
    "\n",
    "\n",
    "### Uygulama Örneği (Zemberek-Python)  \n",
    "```python\n",
    "from zemberek import TurkishMorphology\n",
    "\n",
    "morph = TurkishMorphology.create_with_defaults()\n",
    "analysis = morph.analyze(\"yardımcılarınızdan\")\n",
    "for result in analysis:\n",
    "    print(result.get_stem(), result.get_morphemes())\n",
    "# Çıktı örneği:\n",
    "# ya1rdımcı ['Noun', 'Pos', 'Plur', 'A3sg', 'P2pl']\n",
    "\n",
    "-- **  Ne Zaman Kullanılır? ** -- \n",
    "\n",
    "* Türkçe veya Fin-Ural dilleri gibi eklemeli dillerde\n",
    "\n",
    "* Küçük-orta boy veri setlerinde, kelime bazlı token sayısını düşürmek istediğinizde\n",
    "\n",
    "* Özellikle bilgi çıkarımı, soru-cevap veya anlamsal analiz gibi görevlerde kök-ek yapısının önemli olduğu durumlarda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87721f6",
   "metadata": {},
   "source": [
    "## Özetle:\n",
    "\n",
    "* Morfolojik analiz, clean_text()’ten sonra, Tokenizer’dan önce gelen “ön-işleme” adımıdır.\n",
    "\n",
    "* Keras’ın Tokenizer sınıfı içinde otomatik bulunmaz; senin kendi pipeline’ında açıkça bu işlemi çağırman gerekir.\n",
    "\n",
    "* Böylece, tokenizer’a verdiğin her “token” zaten tüm eki atılmış kök veya ek bilgisiyle birlikte birim olarak girer ve vocab’in çok daha temiz, genellemesi güçlü olur.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# === 1) spaCy Türkçe Modelini Kur ve Yükle ===\n",
    "# Terminal’de bir kez çalıştır: \n",
    "#   python -m spacy download tr_core_news_sm\n",
    "nlp = spacy.load(\"tr_core_news_sm\")\n",
    "\n",
    "# === 2) Temizleyici Fonksiyon ===\n",
    "def clean_text(text, lowercase=True, normalize_unicode=True):\n",
    "    if normalize_unicode:\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    # Türkçe karakterleri koruyup diğerlerini boşlukla değiştir\n",
    "    text = re.sub(r\"[^a-zçğıöşü0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# === 3) Lemmatizasyon Fonksiyonu ===\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    spaCy modeliyle verilen metindeki her token'ın lemma (kök) hali.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_ for token in doc)\n",
    "\n",
    "# === 4) Örnek Çalıştırma ===\n",
    "df = pd.read_csv(\"örnek_set.csv\")\n",
    "raw_texts = df[\"input\"].astype(str).tolist()\n",
    "\n",
    "for orig in raw_texts[:5]:\n",
    "    cleaned   = clean_text(orig)\n",
    "    lemmatized = lemmatize_text(cleaned)\n",
    "    print(f\"Orijinal:   {orig}\")\n",
    "    print(f\"Cleaned:    {cleaned}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf0633",
   "metadata": {},
   "source": [
    "* Gerekli kod aktarımları yukarıda verilmiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed2886",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a25b97",
   "metadata": {},
   "source": [
    "##  Subword / BPE / WordPiece Desteği\n",
    "\n",
    "### Kavram  \n",
    "- **Subword tokenization**: Metni tüm kelimeler yerine, sık tekrar eden **alt kelime parçalara** (subword) bölerek işler.  \n",
    "- **Byte-Pair Encoding (BPE)**: En sık geçen bitişik karakter çiftlerini (byte-pair) iteratif olarak birleştirip sözlüğü oluşturur.  \n",
    "- **WordPiece**: Google’ın BERT modeli için geliştirdiği, sonraki parça olasılıklarını dikkate alarak alt kelime birimleri seçen yöntem.\n",
    "\n",
    "\n",
    "### Nasıl Çalışır?  \n",
    "1. **Başlangıç**: Tüm karakterler veya tek karakter token’larıyla başlar.  \n",
    "2. **Ölçüm**: Karakter çiftleri veya subword parça frekanslarını hesaplar.  \n",
    "3. **Birleştirme**: En sık çiftleri veya parçaları birleştirir.  \n",
    "4. **Tekrar**: İstenilen vocab büyüklüğüne ulaşana dek adım 2–3’ü sürdürür.  \n",
    "5. **Sonuç**: Hem tam kelimeler, hem de alt parçalar (ör. “yardım” + “cılar”) içeren subword sözlüğü elde edilir.\n",
    "\n",
    "\n",
    "\n",
    "### Faydaları  \n",
    "- **OOV Problemini Minimize Eder**  \n",
    "  - Komple yeni kelimeler bile mevcut alt parçaların birleşimiyle ifade edilir.  \n",
    "- **Küçük, Etkili Sözlük**  \n",
    "  - Binlerce nadir kelime yerine, yüzlerce subword birimi yeterli olur.  \n",
    "- **Çok Dilli Destek**  \n",
    "  - Birden fazla dilde ortak subword birimleri paylaşarak transfer öğrenmeyi kolaylaştırır.  \n",
    "- **Hafıza & Hız Kazancı**  \n",
    "  - Daha küçük embedding matrisleri, daha hızlı eğitim ve çıkarım.\n",
    "\n",
    "\n",
    "\n",
    "### Ne Zaman Kullanılır?  \n",
    "- **Büyük veri setleri** ve **çok dilli** uygulamalarda  \n",
    "- **OOV oranının yüksek** olduğu durumlarda  \n",
    "- Modelin **daha ince anlamsal** genellemeye ihtiyacı varsa  \n",
    "- **Transformer** veya **BERT** benzeri subword-temelli modellerde zorunlu\n",
    "\n",
    "\n",
    "\n",
    "### Uygulama Özet  \n",
    "- **SentencePiece** (Google): BPE & Unigram modelleri, bağımsız olarak dil ve platformdan  \n",
    "- **HuggingFace Tokenizers** (Rust tabanlı): Hem BPE hem WordPiece hem Unigram  \n",
    "- **Tokenizers.fit** → **encode** / **decode** metotlarıyla subword kelimeleri otomatik işle\n",
    "\n",
    "> Subword tokenization, modern NLP modellerinin temel taşlarından biridir ve OOV sorununu büyük ölçüde ortadan kaldırır.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131dc6c",
   "metadata": {},
   "source": [
    "### Subword / BPE Tokenization’da “Tokenizer” Nasıl Çalışır?\n",
    "\n",
    "1. **Klasik `Tokenizer` yerine Subword Model**  \n",
    "   - Keras’ın `Tokenizer` sınıfını kullanmak yerine, `SentencePiece` (veya HuggingFace Tokenizers) tarafından eğitilmiş bir model kullanırsınız.  \n",
    "   - Bu model `.Train()` adımıyla bir “vocab.model” dosyası ve “vocab.vocab” dosyası oluşturur.\n",
    "\n",
    "2. **`fit_on_texts` Gibi Adım Yok**  \n",
    "   - Metinleri “öğreten” kod, `SentencePieceTrainer.Train(...)` fonksiyonudur; bu fonksiyon ham metin dosyasını (“all_texts.txt”) okur ve BPE sözlüğünü çıkarır.  \n",
    "   - Dolayısıyla `tokenizer.fit_on_texts(input_texts)` yerine:\n",
    "     ```python\n",
    "     spm.SentencePieceTrainer.Train(\n",
    "         input=\"all_texts.txt\",\n",
    "         model_prefix=\"bpe_spm\",\n",
    "         vocab_size=8000,\n",
    "         model_type=\"bpe\"\n",
    "     )\n",
    "     ```\n",
    "     satırları kullanılır.\n",
    "\n",
    "3. **Model Yükleme & Kodlama**  \n",
    "   - Elde edilen `bpe_spm.model` dosyasını:\n",
    "     ```python\n",
    "     sp = spm.SentencePieceProcessor()\n",
    "     sp.Load(\"bpe_spm.model\")\n",
    "     ```\n",
    "     ile yüklüyorsunuz.  \n",
    "   - Bu `sp` nesnesi **tokenizer**’ınız olur.  \n",
    "   - Ardından metni ID’lere çevirmek için:\n",
    "     ```python\n",
    "     ids = sp.EncodeAsIds(\"merhaba dünya\")\n",
    "     ```\n",
    "     veya parça bazlı metne döndürmek için\n",
    "     ```python\n",
    "     pieces = sp.EncodeAsPieces(\"merhaba dünya\")\n",
    "     ```\n",
    "\n",
    "4. **Keras’ın `pad_sequences` ile Entegrasyon**  \n",
    "   - Kendi `sp` nesnenizle dönüşüm yaptıktan sonra, ID listelerini `pad_sequences(..., value=pad_id)` ile pad edersiniz.  \n",
    "   - Bu aşamada Keras’a özgü `Tokenizer` kullanımı tamamen devreden çıkar.\n",
    "\n",
    "\n",
    "**Özet:**  \n",
    "- Subword tokenization’da **`SentencePiece` veya `Tokenizers` model dosyası** kendi “tokenizer”ınızdır.  \n",
    "- `fit_on_texts` yerine **`Trainer.Train(...)`** adımını kullanırsınız.  \n",
    "- Kodlama/çözme işlemleri `sp.EncodeAsIds` ve `sp.EncodeAsPieces` ile yapılır.  \n",
    "- Sonrasında `pad_sequences` gibi araçlarla Keras pipeline’ınıza bağlarsınız.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c47cc0",
   "metadata": {},
   "source": [
    "## Kelime-bazlı `Tokenizer` vs. Subword (SentencePiece/BPE)\n",
    "\n",
    "| Kriter | Keras `Tokenizer.fit_on_texts` <br>(Kelime-bazlı) | SentencePiece / BPE / WordPiece <br>(Subword) |\n",
    "|--------|---------------------------------------------------|-----------------------------------------------|\n",
    "| **Kurulum** | Keras içinde hazır, ek paket gerekmez | Ek paket ( `sentencepiece` veya *HF Tokenizers*), model eğitimi gerekir |\n",
    "| **OOV (Out-of-Vocab) Oranı** | Yüksek — nadir veya yeni kelimeler `<unk>` olur | Çok düşük — yeni kelimeler mevcut alt parçaların birleşimiyle yazılır |\n",
    "| **Vocab Boyutu** | Kelime başına 1 giriş ⇒ büyük sözlük | Aynı kapsama için çok daha küçük (∼8 k) |\n",
    "| **Bellek / Embed Matris** | Büyük matris, yavaşlar | Küçük matris, daha hızlı eğitim/çıkarım |\n",
    "| **Dil Desteği** | İngilizce-benzeri dillerde sorun az | Eklemeli dillerde (TR, FI) özellikle güçlü |\n",
    "| **İnsan Okunabilirlik** | Çıktı tam kelime; log okumak kolay | Parçalı çıktı (“yardımcı▁lar”), yorumlamak zor |\n",
    "| **Prototip Hızı** | Çok hızlı başlar, kod sade | Model + dosya üretmek ek adım |\n",
    "| **Güncelleme Kolaylığı** | `fit_on_texts` tekrar çalıştırmak yeter | Yeni veri için BPE modelini yeniden eğitmek gerekir |\n",
    "| **Büyük Veri** | Nadir kelimeler ve OOV patlar | Kapsama hâlâ yüksek, ölçeklenebilir |\n",
    "| **Standart Modern Modeller** | LSTM/GRU tabanlı seq2seq | BERT, T5, GPT, Transformer standardı |\n",
    "\n",
    "\n",
    "\n",
    "### Ne Zaman Hangi Yöntem?\n",
    "\n",
    "- **Hızlı prototip / küçük veri**  \n",
    "  - *Kelime-bazlı Tokenizer*  \n",
    "  - Daha okunaklı log, daha az dış bağımlılık.\n",
    "\n",
    "- **Veri > ~1 M cümle, OOV % ↑, çok dilli veya eklemeli dil**  \n",
    "  - *SentencePiece (BPE / WordPiece)*  \n",
    "  - Daha küçük vocab, neredeyse sıfır OOV, modern Transformer’larla tam uyum.\n",
    "\n",
    "- **Ara Çözümler**  \n",
    "  - Kelime-bazlı Tokenizer + **lemmatizasyon & `min_freq`**  \n",
    "  - OOV hâlâ yüksekse → **Subword**’a geç.\n",
    "\n",
    "> **Pratik Yol Haritası**  \n",
    "> 1. Kelime-bazlı ile başla.  \n",
    "> 2. OOV % > 5–10 ise lemmatizasyon/min_freq uygula.  \n",
    "> 3. Hâlâ sorunluysa veya veri büyüyorsa SentencePiece’e geç.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6529609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import unicodedata, re, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a084e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"örnek_set.csv\")\n",
    "\n",
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()\n",
    "\n",
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşü0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "input_texts = [clean_texts(t)  for t in raw_inputs]\n",
    "target_texts = [clean_texts(t) for t in raw_targets]\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) BPE modelini SentencePiece ile eğit\n",
    "# ------------------------------------\n",
    "\n",
    "model_prefix = \"bpe_tr\"\n",
    "vocab_size = 8000\n",
    "data_txt = \"corpus.txt\"\n",
    "\n",
    "with io.open(data_txt,\"w\",encoding = \"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "# 2) BPE Modelini eğitirken:\n",
    "UNIQUE_LIMIT = 1800          # Trainer’ın verdiği üst sınırın biraz altı\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=data_txt,\n",
    "    model_prefix=model_prefix,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(vocab_size, UNIQUE_LIMIT),   # 8k yerine 1 800\n",
    "    user_defined_symbols=[\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Modeli yükle & özel ID’ler\n",
    "# -----------------------------\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# 4) Metinleri ID listesine dönüştür\n",
    "# ---------------------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int)         for t in target_texts]\n",
    "dec_out  = [            sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "\n",
    "# --------------  \n",
    "# 5) Padding\n",
    "# --------------\n",
    "max_len = max(max(map(len, enc_seqs)),\n",
    "              max(map(len, dec_in)),\n",
    "              max(map(len, dec_out)))\n",
    "\n",
    "print(max_len)\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9401282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline hazır ➜ dataset örneği: (32, 12)\n",
      "pad_id: 3 sos_id: 4 eos_id: 5\n",
      "max_len: 12 vocab_size: 8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 6) tf.data.Dataset pipeline\n",
    "# -----------------------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"encoder_input\": encoder_input,\n",
    "         \"decoder_input\": decoder_input},\n",
    "        decoder_target\n",
    "    )\n",
    ").shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pipeline hazır ➜ dataset örneği:\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(\"pad_id:\", pad_id, \"sos_id:\", sos_id, \"eos_id:\", eos_id)\n",
    "print(\"max_len:\", max_len, \"vocab_size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cff07",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "\n",
    "## Chatbot-Odaklı Transformer İçin Ek Tokenizer / Veri İyileştirmeleri\n",
    "\n",
    "| Başlık | Neden Önemli? | Nasıl Eklenir? |\n",
    "|--------|---------------|----------------|\n",
    "| **Konuşmacı Etiketleri** (`<user>`, `<bot>`) | Rol ayrımı, diyaloğun yönünü netleştirir; model hangi repliğin kime ait olduğunu öğrenir. | Her cümleye başlık token’ı ekleyin: `\" <user> merhaba\"` / `\" <bot> merhaba, nasıl yardımcı olabilirim?\"` |\n",
    "| **Diyalog Tur Ayırıcı** (`<turn>`) | Uzun geçmişte cümle sınırlarını netleştirir; kontekst “kaymasını” azaltır. | Çok-cümleli geçmişi tek dizide birleştirip her cümle sonuna `<turn>` ekleyin. |\n",
    "| **Persona / Stil Token’ları** (`<formal>`, `<casual>`) | Kişiselleştirme ve ton kontrolü; aynı modelle birden fazla stil üretimi. | Eğitim verisinde örnek yanıtın başına uygun token ekleyin. |\n",
    "| **Subword Regularization** (BPE sampling) | Çeşitli alt-parça varyasyonları → overfitting düşer, robustluk artar. | `SentencePieceProcessor.Encode(..., enable_sampling=True, nbest_size=-1, alpha=0.1)` |\n",
    "| **Dynamic Padding Buckets** | Sabit `max_len` yerine yakın uzunlukta cümleleri gruplayıp %30-40 GPU kazanımı. | `tf.data.experimental.bucket_by_sequence_length()` kullanın. |\n",
    "| **Numerik / Tarih Placeholder’ları** (`<NUM>`, `<DATE>`) | Rakam kümeleri anlamsal gürültü yaratır; generatif hataları azaltır. | Regex ile sayıları `<NUM>123</NUM>` veya sadece `<NUM>` şeklinde maskeleyin. |\n",
    "| **NER-tabanlı De-lexicalization** (`<PERSON>`, `<ORG>`) | Gizlilik + daha iyi genelleme; özel isimler OOV yaratmaz. | SpaCy / Stanza NER → özel isimleri placeholder ile değiştir. |\n",
    "| **Token Dropout Augmentation** | Beklenmedik eksik kelimelere karşı dayanıklılık. | `dataset.map(token_dropout, num_parallel_calls=AUTOTUNE)` |\n",
    "| **Segment ID veya Speaker ID Embedding’i** | Self-Attention’ın “kim konuşuyor” bilgisini doğrudan görmesi. | Transformer girdi dict’ine `speaker_ids` tensörü ekleyin (0=user,1=bot). |\n",
    "| **Label Smoothing** | Hedef dizideki tek-sıcak (one-hot) köşeliğini yumuşatır, over-confidence önler. | `tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)` |\n",
    "| **Top-K / Nucleus Decoding Ayar Token’ları** (`<topk_50>`, `<topp_0.9>`) | Çıkarımda esnek kontrol: tutarlılık vs. yaratıcılık. | Prompt’un başına ayar token’ı koy, decode sırasında parse edip karşılık gelen sampling config’i kullan. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1c6e1",
   "metadata": {},
   "source": [
    "* Şimdi aşağıya artık son demlerine gelmiş bir tokenizer kodu bırakıyorum ( Subword Tokenization – SentencePiece ile Byte-Pair Encoding (BPE ).Bu diğer tokenizer işlemi.Bundan sonra ise ilk başlarda uyguladığımız tokenizer işlemlerine eklemeler yapacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dff9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset örneği encoder shape: (32, 12)\n",
      "pad_id:3  sos_id:4  eos_id:5  unk_id:0\n",
      "max_len: 12    vocab_size: 1800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SUBWORD-TABANLI TOKENIZER PIPELINE  (masking yok)\n",
    "\n",
    "● SentencePiece-BPE modeli (1 800 parçalık sözlük)\n",
    "● <sos>/<eos>/<pad>/<unk> özel token’ları\n",
    "● Rastgele %10 token’ı <unk> yaparak UNK-dropout (data augmentation)\n",
    "● tf.data.Dataset → shuffle-batch-prefetch zinciri\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------ 0. Gereksinimler ------------------------\n",
    "# pip install sentencepiece tensorflow==2.18 pandas\n",
    "\n",
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ------------------------ 1. Veriyi Yükle & Temizle ---------------\n",
    "df = pd.read_csv(\"örnek_set.csv\")           # input ve output sütunları\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "input_texts  = [clean(t) for t in raw_inputs]\n",
    "target_texts = [clean(t) for t in raw_targets]\n",
    "\n",
    "# ------------------------ 2. SentencePiece BPE Eğit ----------------\n",
    "MODEL_PREFIX = \"bpe_tr\"\n",
    "VOCAB_SIZE   = 8000                 # istenen\n",
    "UNIQUE_LIMIT = 1800                 # küçük korpus üst limiti\n",
    "CORPUS_TXT   = \"corpus.txt\"\n",
    "\n",
    "with io.open(CORPUS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=CORPUS_TXT,\n",
    "    model_prefix=MODEL_PREFIX,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(VOCAB_SIZE, UNIQUE_LIMIT),\n",
    "    user_defined_symbols=[\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "\n",
    "# ------------------------ 3. Modeli Yükle & ID’ler ----------------\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.piece_to_id(\"<unk>\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# ------------------------ 4. Encode & Padding ---------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int) for t in target_texts]\n",
    "dec_out  = [sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "max_len = max(\n",
    "    max(map(len, enc_seqs)),\n",
    "    max(map(len, dec_in)),\n",
    "    max(map(len, dec_out))\n",
    ")\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "\n",
    "# ------------------------ 5. UNK-Dropout Fonksiyonu ---------------\n",
    "DROPOUT_RATE = 0.10   # %10 token rastgele <unk>\n",
    "\n",
    "def unk_dropout(inputs, targets, rate=DROPOUT_RATE, unk=unk_id):\n",
    "    enc = inputs[\"encoder_input\"]\n",
    "    dec = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_enc = tf.cast(tf.random.uniform(tf.shape(enc)) < rate, enc.dtype)\n",
    "    mask_dec = tf.cast(tf.random.uniform(tf.shape(dec)) < rate, dec.dtype)\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_enc == 1, unk, enc)\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_dec == 1, unk, dec)\n",
    "    return inputs, targets\n",
    "\n",
    "# ------------------------ 6. Dataset Pipeline ---------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\"encoder_input\": encoder_input,\n",
    "             \"decoder_input\": decoder_input},\n",
    "            decoder_target\n",
    "        )\n",
    "    )\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)   # sadece UNK-dropout\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ------------------------ 7. Kontrol Çıktıları --------------------\n",
    "enc_ex = next(iter(dataset.take(1)))[0][\"encoder_input\"]\n",
    "print(\"dataset örneği encoder shape:\", enc_ex.shape)\n",
    "print(f\"pad_id:{pad_id}  sos_id:{sos_id}  eos_id:{eos_id}  unk_id:{unk_id}\")\n",
    "print(\"max_len:\", max_len, \"   vocab_size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34a81b",
   "metadata": {},
   "source": [
    "* Son olarak da \"<user\" ve \"<bot\" taglarını ekleyip işi bırakalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f7d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder batch shape: (32, 16)\n",
      "pad_id:3  sos_id:4  eos_id:5  unk_id:0\n",
      "max_len: 16    vocab_size: 1800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TOKENIZER PIPELINE + KONUŞMACI & STİL TOKEN’LARI\n",
    "-------------------------------------------------\n",
    "• SentencePiece-BPE (≈1 800 parçalık sözlük)\n",
    "• Özel semboller: <pad> <sos> <eos> <user> <bot> <formal> <casual>\n",
    "• UNK-Dropout (%10)  →  dayanıklı girdi\n",
    "• tf.data.Dataset  →  shuffle • batch • prefetch\n",
    "(Attention mask EKLENMEDİ — yalnızca tokenizer & pipeline)\n",
    "\"\"\"\n",
    "\n",
    "# --------------------- 0. Gereksinimler ---------------------\n",
    "# pip install sentencepiece pandas tensorflow==2.18\n",
    "\n",
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --------------------- 1. Veriyi Yükle & Temizle ------------\n",
    "df = pd.read_csv(\"örnek_set.csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# Konuşmacı + stil etiketleri ekle\n",
    "SPEAKER_USER = \"<user>\"\n",
    "SPEAKER_BOT  = \"<bot>\"\n",
    "STYLE_CASUAL = \"<casual>\"\n",
    "\n",
    "input_texts  = [f\"{SPEAKER_USER} {STYLE_CASUAL} {clean(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{SPEAKER_BOT} {STYLE_CASUAL} {clean(t)}\" for t in raw_targets]\n",
    "\n",
    "# --------------------- 2. SentencePiece BPE Eğit ------------\n",
    "MODEL_PREFIX = \"bpe_tr\"\n",
    "VOCAB_SIZE   = 8000\n",
    "LIMIT        = 1800                     # küçük korpus sınırı\n",
    "CORPUS_TXT   = \"corpus.txt\"\n",
    "\n",
    "with io.open(CORPUS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in input_texts + target_texts:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=CORPUS_TXT,\n",
    "    model_prefix=MODEL_PREFIX,\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=min(VOCAB_SIZE, LIMIT),\n",
    "    user_defined_symbols=[\n",
    "        \"<pad>\", \"<sos>\", \"<eos>\",\n",
    "        SPEAKER_USER, SPEAKER_BOT, STYLE_CASUAL\n",
    "    ]\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
    "\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# --------------------- 3. Encode & Pad ----------------------\n",
    "enc_seqs = [sp.encode(t, out_type=int) for t in input_texts]\n",
    "dec_in   = [[sos_id] + sp.encode(t, out_type=int) for t in target_texts]\n",
    "dec_out  = [sp.encode(t, out_type=int) + [eos_id] for t in target_texts]\n",
    "\n",
    "max_len = max(\n",
    "    max(map(len, enc_seqs)),\n",
    "    max(map(len, dec_in)),\n",
    "    max(map(len, dec_out))\n",
    ")\n",
    "\n",
    "encoder_input  = pad_sequences(enc_seqs, maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_input  = pad_sequences(dec_in,   maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "decoder_target = pad_sequences(dec_out,  maxlen=max_len, padding=\"post\", value=pad_id)\n",
    "\n",
    "# --------------------- 4. UNK-Dropout -----------------------\n",
    "DROPOUT_RATE = 0.10   # %10 token rastgele <unk>\n",
    "\n",
    "def unk_dropout(inputs, targets, rate=DROPOUT_RATE, unk=unk_id):\n",
    "    enc = inputs[\"encoder_input\"]\n",
    "    dec = inputs[\"decoder_input\"]\n",
    "    mask_enc = tf.cast(tf.random.uniform(tf.shape(enc)) < rate, enc.dtype)\n",
    "    mask_dec = tf.cast(tf.random.uniform(tf.shape(dec)) < rate, dec.dtype)\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_enc == 1, unk, enc)\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_dec == 1, unk, dec)\n",
    "    return inputs, targets\n",
    "\n",
    "# --------------------- 5. Dataset Pipeline ------------------\n",
    "BATCH_SIZE  = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\"encoder_input\": encoder_input,\n",
    "             \"decoder_input\": decoder_input},\n",
    "            decoder_target\n",
    "        )\n",
    "    )\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)  # sadece tokenizer augmentation\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# --------------------- 6. Kontrol Çıktıları -----------------\n",
    "enc_example = next(iter(dataset.take(1)))[0][\"encoder_input\"]\n",
    "print(\"Encoder batch shape:\", enc_example.shape)\n",
    "print(f\"pad_id:{pad_id}  sos_id:{sos_id}  eos_id:{eos_id}  unk_id:{unk_id}\")\n",
    "print(\"max_len:\", max_len, \"   vocab_size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4eed02",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261213dd",
   "metadata": {},
   "source": [
    "## Mevcut Kelime-Bazlı Tokenizer Kodunu “Derinleştirme” Yol Haritası\n",
    "\n",
    ">Amaç: **BPE’yi zaten çözdük**; şimdi klasik `Tokenizer` senaryosunu daha esnek, yeniden-kullanılabilir ve “üretim hazır” hâle getirmek.\n",
    "\n",
    "\n",
    "### 1. Kodun Fonksiyonlaştırılması\n",
    "Tek seferlik betik yerine **parametrik fonksiyon** yazmak:\n",
    "```python\n",
    "def build_word_tokenizer(\n",
    "        input_texts,\n",
    "        target_texts,\n",
    "        min_freq    = 2,\n",
    "        max_vocab   = None,\n",
    "        filters     = \"\",\n",
    "        oov_token   = \"<unk>\",\n",
    "        pad_token   = \"<pad>\",\n",
    "        sos_token   = \"<sos>\",\n",
    "        eos_token   = \"<eos>\",\n",
    "        char_level  = False,\n",
    "        padding     = \"post\"):\n",
    "\n",
    "    # (kodun 1-8. adımları burada özetlenmiş hâlde)\n",
    "    return tokenizer, encoder_input, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4dbc0",
   "metadata": {},
   "source": [
    "### Konuşmacı & Stil Etiketlerini Kelime-Bazlı Tokenizer’a Ekleme\n",
    "\n",
    "#### 1. Neden Gerekli?\n",
    "| Sorun | Etiketlerin Çözümü |\n",
    "|-------|-------------------|\n",
    "| **Rol Belirsizliği** – Model “kimin” konuştuğunu ayırt edemez. | `<user>` ve `<bot>` token’ları konuşmacı rolünü açıkça belirtir. |\n",
    "| **Ton Kontrolü** – Aynı modelle resmî / samimî yanıt üretmek. | `<casual>` / `<formal>` gibi stil token’ları istenen tonu sinyaller. |\n",
    "| **Kontekst Kayması** – Uzun diyaloğun nerede “bölündüğü” anlaşılmaz. | Etiketler satır başında sabit kalıp kontekst hizalamasını korur. |\n",
    "\n",
    "#### 2. Nasıl Çalışır?\n",
    "1. **Ön-işleme**:  \n",
    "   Her ham cümlenin başına **rol** ve **stil** token’ı eklenir:  \n",
    "   ```text\n",
    "   <user> <casual> merhaba\n",
    "   <bot>  <casual> merhaba, nasıl yardımcı olabilirim?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ebe9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\Transformerlar\\Konu Anlatımları\\Encoder - Decoder - PE - ATTN ( İLERİ DÜZEY ) = TF\\Tokenizer\\örnek_set .csv\")\n",
    "raw_inputs  = df[\"input\"].astype(str).tolist()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb4e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\", text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c305d",
   "metadata": {},
   "source": [
    "* Aşağıda bulunan 2 satır bu işlemi anlatıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11079438",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER , BOT ,CASUAL = \"<user>\" , \"<bot>\" , \"<casual>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f395895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"{USER} {CASUAL} {clean(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean(t)}\"  for t in raw_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda7be4",
   "metadata": {},
   "source": [
    "* Diğer adımlar aynı şekilde işleme devam eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600460e",
   "metadata": {},
   "source": [
    "## Dynamic UNK-Dropout (Kelime-bazlı Tokenizer’a Zarar Vermeden)\n",
    "\n",
    "### Neden Dinamik?\n",
    "* **Sabit oran** (ör. 0.10) her epoch boyunca aynı gürültüyü verir.  \n",
    "* **Dinamik takvim**:  \n",
    "  - **Erken epoch’larda** düşük dropout ⇒ model temel yapıyı öğrenir.  \n",
    "  - **Orta epoch’larda** yüksek dropout ⇒ genelleme güçlenir.  \n",
    "  - **Sonlara doğru** oranı tekrar düşür ⇒ duyarlılık kazanılır.\n",
    "\n",
    "\n",
    "### Basit Takvim Örneği  \n",
    "| Epoch Aralığı | Oran (`rate`) | Yorum |\n",
    "|---------------|--------------|-------|\n",
    "| 0 – 4         | 0.05         | “Isınma” – saf veriye yakın |\n",
    "| 5 – 14        | 0.12         | “Sağlamlaştırma” |\n",
    "| 15 – 19       | 0.08         | “İnce ayar” |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153688ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDİNAMİK UNK-DROPOUT + KONUŞMACI/STİL ETİKETLİ KELİME-BAZLI TOKENIZER\\n────────────────────────────────────────────────────────────────────\\n\\n• <user> / <bot> + <casual> etiketleri\\n• min_freq filtresi (nadir kelimeleri OOV’a at)\\n• Dynamic UNK-Dropout takvimi:\\n      epoch 0-4  →  0.05\\n      epoch 5-14 →  0.12\\n      epoch 15-19→  0.08\\n• tf.data.Dataset → shuffle • batch • prefetch\\n(Masking/model kısmı bu betiğe dâhil değil)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DİNAMİK UNK-DROPOUT + KONUŞMACI/STİL ETİKETLİ KELİME-BAZLI TOKENIZER\n",
    "────────────────────────────────────────────────────────────────────\n",
    "\n",
    "• <user> / <bot> + <casual> etiketleri\n",
    "• min_freq filtresi (nadir kelimeleri OOV’a at)\n",
    "• Dynamic UNK-Dropout takvimi:\n",
    "      epoch 0-4  →  0.05\n",
    "      epoch 5-14 →  0.12\n",
    "      epoch 15-19→  0.08\n",
    "• tf.data.Dataset → shuffle • batch • prefetch\n",
    "(Masking/model kısmı bu betiğe dâhil değil)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81cecc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 0) Gereksinimler\n",
    "#    pip install pandas tensorflow==2.18\n",
    "# -----------------------------------------------------------------\n",
    "import re, unicodedata, json, pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52d11ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters        = \"\"            # Noktalama işleme filtresi\n",
    "oov_token      = \"<unk>\"       # Out-of-vocab token\n",
    "pad_token      = '<pad>'       # Padding token\n",
    "sos_token      = \"<sos>\"       # Decoder başlangıç token\n",
    "eos_token      = \"<eos>\"       # Decoder bitiş token\n",
    "char_level     = False         # True = karakter, False = kelime\n",
    "max_vocab_size = None          # None = sınırsız\n",
    "min_freq       = 2             # En az 2 kez geçen token'lar tutulacak\n",
    "padding       = \"post\"         # \"pre\" da seçilebilir\n",
    "\n",
    "# === 2. Sabit Token Tanımları ===\n",
    "USER, BOT, CASUAL = \"<user>\", \"<bot>\", \"<casual>\"\n",
    "\n",
    "fixed_index = {\n",
    "    pad_token : 0,\n",
    "    oov_token : 1,\n",
    "    sos_token : 2,\n",
    "    eos_token : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str)->str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c33040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"{USER} {CASUAL} {clean_text(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean_text(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0ebfef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. SOS/EOS Ekleme ===\n",
    "dec_in_text  = [f\"{sos_token} {t}\" for t in target_texts]\n",
    "dec_out_text = [f\"{t} {eos_token}\" for t in target_texts]\n",
    "all_texts    = input_texts + dec_in_text + dec_out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d88b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- 4. Geçici Tokenizer -----------------\n",
    "tmp_tok = Tokenizer(filters=filters, oov_token=oov_token, char_level=char_level)\n",
    "tmp_tok.fit_on_texts(all_texts)\n",
    "word_counts = tmp_tok.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56416209",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = [w for w, c in word_counts.items() if c >= min_freq] # min_freq uugulaması.\n",
    "for special in (USER,BOT,CASUAL,pad_token,sos_token,eos_token):\n",
    "    if special not in keep:\n",
    "        keep_tokens.append(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7f1677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(keep) + 1, filters=filters,oov_token=oov_token ,char_level=char_level)\n",
    "tokenizer.fit_on_texts(keep)\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b727b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_seq = tokenizer.texts_to_sequences(input_texts)\n",
    "dec_in_seq = tokenizer.texts_to_sequences(dec_in_text)\n",
    "dec_in_out = tokenizer.texts_to_sequences(dec_out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a856bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "maxlen = max(map(len, inp_seq+dec_in_seq+dec_in_out))\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae046b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in  = pad_sequences(inp_seq,    maxlen=maxlen, padding=padding, value=0)\n",
    "dec_in  = pad_sequences(dec_in_seq, maxlen=maxlen, padding=padding, value=0)\n",
    "dec_out = pad_sequences(dec_in_out,maxlen=maxlen, padding=padding, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e4def",
   "metadata": {},
   "source": [
    "# --- Dinamik UNK-dropout için değişken tanımı ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ad88bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = tokenizer.word_index[oov_token]\n",
    "drop_var = tf.Variable(0.05 , trainable=False,dtype=tf.float32)\n",
    "\n",
    "def unk_dropout_dyn(inputs,targets):\n",
    "\n",
    "    e = inputs[\"encoder_input\"] ; d = inputs[\"decoder_input\"]\n",
    "    m_e = tf.cast(tf.random.uniform(tf.shape(e)) < drop_var , e.dtype)\n",
    "    m_d = tf.cast(tf.random.uniform(tf.shape(d)) < drop_var , d.dtype)\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(m_e==1 , unk_id, e)\n",
    "    inputs[\"decoder_input\"] = tf.where(m_d==1 , unk_id , d)\n",
    "\n",
    "    return inputs , targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "106dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch , BUFFER = 32, 1000\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((\n",
    "        {\"encoder_input\" : enc_in , \"decoder_input\" : dec_in} , dec_out\n",
    "        ))\n",
    "        .map(unk_dropout_dyn , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(BUFFER)\n",
    "        .batch(batch,drop_remainder=True)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a44789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callback ile oran takvimi ---\n",
    "\n",
    "def schedule(ep):\n",
    "\n",
    "    if ep < 5: return 0.05\n",
    "    if ep < 15 : return 0.12\n",
    "\n",
    "    return 0.08\n",
    "\n",
    "class UnkScheduler(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        new = schedule(epoch)\n",
    "        drop_var.assign(new)\n",
    "        print(f\"\\n[UNK-Dropout] epoch {epoch} → rate={new:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc31a4",
   "metadata": {},
   "source": [
    "#####  Artık model.fit(dataset, epochs=20, callbacks=[UnkScheduler()]) diyebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14773d91",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ff34e",
   "metadata": {},
   "source": [
    "# Düz Tokenizer vs. Subword-BPE Pipeline: Hangisi Daha Profesyonel ve Kullanışlı?\n",
    "\n",
    "Aşağıda “1) Düz kelime-bazlı tokenizer + Dynamic UNK-Dropout + speaker/stil token’ları” ile “2) SentencePiece-BPE tabanlı tokenizer + UNK-Dropout” yaklaşımlarını karşılaştırdım. Projenizin ölçeği, hedefleri ve ekosistemi doğrultusunda hangisinin size daha uygun olduğuna karar verebilirsiniz.\n",
    "\n",
    "| Özellik                          | 1) Düz Kelime-bazlı Tokenizer                       | 2) Subword-BPE (SentencePiece)                         |\n",
    "|----------------------------------|------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Kurulum / Bağımlılıklar**      | Sadece `tensorflow`/`keras`                           | + `sentencepiece` paketi                                |\n",
    "| **Sözlük Oluşumu**               | `Tokenizer.fit_on_texts` + `min_freq` + `max_vocab`  | `SentencePieceTrainer.Train` (dış model dosyası)        |\n",
    "| **OOV (Bilinmeyen Token)**       | Yüksek (%OOV ≈ 100−coverage)                         | Pratikte sıfıra yakın (her yeni kelime parçalara bölünür) |\n",
    "| **Vocab Boyutu**                 | Kelime sayısına göre çok büyük, `min_freq` şart      | Küçük ve kontrollü (örn. 1 000–8 000 parça)             |\n",
    "| **Bellek & Performans**          | Büyük embed matrisi → daha fazla bellek              | Küçük embed matrisi → hem eğitim hem çıkarım hızlanır  |\n",
    "| **Dil Yapısı Uyum**              | Eklemeli dillerde (Türkçe) çok nadir token’lı        | Türkçe gibi eklemeli dillerde doğal subword ayrışımı     |\n",
    "| **Konuşmacı/Stil Token’ları**    | Her iki yaklaşmada kolayca eklenir                   | Aynı etiketler `user_defined_symbols` ile eklenir       |\n",
    "| **Dynamic UNK-Dropout**          | Callback + `tf.data.map` içinde uygulamak kolay      | Yine `tf.data.map` ile direkt entegre                  |\n",
    "| **Pipeline Karmaşıklığı**        | Daha sade, tek kod bloğu                             | Biraz daha uzun; model eğitimi + encode + pad adımları |\n",
    "| **Model Uyumluluğu**             | LSTM/GRU veya basit seq2seq                          | Modern Transformer / BERT/GPT tabanlı mimarilerle birebir uyum |\n",
    "| **Prototip & Ölçek**             | Küçük projeler, hızlı deneme                         | Büyük veri, çok dilli veya üretim odaklı               |\n",
    "\n",
    "\n",
    "\n",
    "## Ne Zaman Hangisi?\n",
    "\n",
    "- **Hızlı Prototip / Küçük Veri**  \n",
    "  Düz kelime-bazlı tokenizer:\n",
    "  - Dış paket kurmadan, birkaç satırlık kodla hazır.\n",
    "  - İnsan okunurluğu yüksek, hata ayıklamak kolay.\n",
    "\n",
    "- **Büyük Veri / Üretim / Transformer-Uyumluluk**  \n",
    "  Subword-BPE:\n",
    "  - OOV sorununu pratikte ortadan kaldırır.\n",
    "  - Sözlük boyutunu kontrol altında tutar.\n",
    "  - Modern NLP modelleriyle tam uyumlu.\n",
    "\n",
    "\n",
    "## Sonuç ve Öneri\n",
    "\n",
    "- Eğer **Türkçe chatbot** projeniz 10 k–100 k cümle civarında, dış bağımlılıklardan kaçınmak istiyorsanız, **düz tokenizer + dynamic UNK-dropout** gayet yeterli olacaktır.\n",
    "- Eğer **veri setiniz milyonlarca** cümleye ulaşıyor, OOV’ları sıfıra yakın görmek ve **Transformer**/**GPT-like** mimarilerle üretim kalitesi hedefliyorsanız, **SentencePiece-BPE** çözümünü tercih edin.\n",
    "\n",
    "Her iki pipeline da konuşmacı/stil token’ları ve dinamik augmentasyon adımlarını destekler. Projeye ilk düz tokenizer’la başlayıp, ölçek ve ihtiyaç arttıkça BPE’ye geçiş yapmak, en az israfla en hızlı sonuç almanıza yardımcı olur.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d370d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
