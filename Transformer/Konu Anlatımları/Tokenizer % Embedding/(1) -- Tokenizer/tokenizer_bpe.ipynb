{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e05b9c",
   "metadata": {},
   "source": [
    "* Merhaba arkadaşlar.Bu notebook içerisinde BPE formatında tokenizer yazmayı öğreneceğiz.(Tokenizer.ipynb) notebook içeriğinden genel anlatımlara erişebilirsiniz.Kısa bir teorikle işe başlayalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789d873",
   "metadata": {},
   "source": [
    "# SentencePiece-BPE Tabanlı Tokenizer Pipeline  \n",
    "*Konuşmacı & Stil Etiketleri, UNK-Dropout Augmentasyonu*  \n",
    "\n",
    "\n",
    "\n",
    "## 1. Teorik Arka Plan — BPE (Byte-Pair Encoding)\n",
    "\n",
    "**Byte-Pair Encoding**, bir karakter çifti sık sık art arda görüldüğünde bu çifti tek bir “parça” olarak sözlüğe ekleyen, yinelemeli (iterative) bir sıkıştırma-esinli algoritmadır.  \n",
    "Neden tercih edilir?\n",
    "\n",
    "| Özellik | Açıklama |\n",
    "|---------|----------|\n",
    "| **OOV Sorununu Azaltır** | Yeni kelimeler, mevcut alt-parçalara bölünerek temsil edilir. |\n",
    "| **Küçük Sözlük** | Binlerce nadir kelime yerine, birkaç bin alt-parça yeterli olur. |\n",
    "| **Dil Bağımsız** | Eklemeli diller (Türkçe gibi) dahil olmak üzere her dilde çalışır. |\n",
    "| **Modern Modellerle Uyum** | Transformer, BERT, GPT vb. modeller BPE/WordPiece tabanlıdır. |\n",
    "\n",
    "Bu projede **SentencePiece** kütüphanesi kullanılarak ~1 800 parçalık bir Türkçe BPE sözlüğü eğitilmiştir.  \n",
    "Ek olarak:\n",
    "\n",
    "- `<pad>`, `<sos>`, `<eos>` → sıra kontrol token’ları  \n",
    "- `<user>`, `<bot>` → konuşmacı rolleri  \n",
    "- `<casual>` → stil/ton etiketi  \n",
    "- UNK-Dropout (%10) → rastgele token’ları `<unk>` ile değiştirerek dayanıklılık (robustness) artırır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93811e92",
   "metadata": {},
   "source": [
    "## Neden “Klasik Tokenizer”dan Farklı?  \n",
    "### (Konuşmacı & Stil Etiketleri + SentencePiece-BPE + UNK-Dropout)\n",
    "\n",
    "\n",
    "\n",
    "### 1. Klasik Tokenizer’ın Sınırı  \n",
    "`Tokenizer.fit_on_texts()` **kelime-frekans tabanlı** bir yöntemdir:\n",
    "\n",
    "1. Metni boşluklara göre böler.  \n",
    "2. En sık kelimeleri sözlüğe alır, geri kalanını `<unk>` yapar.  \n",
    "\n",
    "Bu yapı;  \n",
    "- OOV (Out-of-Vocab) sorununu tam çözmez,  \n",
    "- Konuşmacı rolü / ton kontrolü gibi diyaloğa özgü sinyalleri içermez,  \n",
    "- Her kelimeyi tek parça tuttuğu için eklemeli dillerde (Türkçe) çok büyük vocab oluşturur.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Subword-BPE Neden Farklıdır?  \n",
    "\n",
    "| Özellik | Klasik Kelime Tabanlı | BPE / Subword |\n",
    "|---------|-----------------------|---------------|\n",
    "| **Eğitim** | Sadece `fit_on_texts` (sıklık) | Ayrı **model eğitimi** (SentencePiece) |\n",
    "| **Token Birimi** | Tam kelime | Sık parçalar + kalan karakter dizileri |\n",
    "| **OOV** | Yüksek | Pratikte sıfıra yakın |\n",
    "| **Vocab Boyutu** | Kelime sayısına bağlı (çok büyük) | Aynı kapsama için küçük (∼k) |\n",
    "\n",
    "> **Farklılık:** Subword modeli, klasik tokenizer’dan önce **bir kere** korpus üzerine “öğrenilir” ve `.model` dosyası üretir. Bu ekstra adım klasik yolu ayıran temel noktadır.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Konuşmacı & Stil Etiketleri  \n",
    "\n",
    "- **Amaç:** Modelin diyaloğu “kim söylüyor” (user/bot) ve “hangi ton” (casual/formal) ile ürettiğini kavraması.  \n",
    "- **Yöntem:** Ham cümle başına etiketi metnin **bir parçası** olarak eklemek.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a5e9b",
   "metadata": {},
   "source": [
    "user> casual> merhaba\n",
    "\n",
    "bot> <casual  merhaba, nasıl yardımcı olabilirim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431edfb",
   "metadata": {},
   "source": [
    "- **Farklılık:** Bu etiketler **dilsel anlam** taşımaz; tokenizer’ın kendiliğinden çıkaramayacağı, tasarımcı tarafından eklenmiş “kontrol tokenları”dır.\n",
    "\n",
    "\n",
    "\n",
    "### 4. UNK-Dropout (Token-Level Data Augmentation)  \n",
    "\n",
    "| Aşama | Klasik Tokenizer | Bu Çalışma |\n",
    "|-------|------------------|------------|\n",
    "| **Veri Hazırlığı** | Diziye çevir → pad | Diziye çevir → **rastgele %α token’ı `<unk>` yap** → pad |\n",
    "\n",
    "- **Amaç:** Modeli rastgele OOV’lerle tanıştırarak üretim sırasında beklenmedik kelimelere karşı sağlamlaştırmak.  \n",
    "- **Farklılık:** Bu adım *tokenizer*’ın hemen arkasında, `tf.data.Dataset.map()` içinde yapılır; klasik yolda hiç yoktur.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Özet  \n",
    "\n",
    "| Katman | Klasik Yaklaşım | Gelişmiş (Bu Çalışma) |\n",
    "|--------|-----------------|-----------------------|\n",
    "| **Temizleme** | Basit regex | Aynı |\n",
    "| **Vocab Eğitimi** | `fit_on_texts` | *SentencePiece-BPE* (dış model) |\n",
    "| **Kontrol Tokenları** | Yok | `<user>`, `<bot>`, `<casual>` |\n",
    "| **Augmentation** | Yok | UNK-Dropout |\n",
    "| **Çıktı** | Kelime ID dizisi | Subword ID dizisi + rol/ton sinyali |\n",
    "\n",
    "Bu üç yenilik — **BPE**, **konuşmacı/ton token’ları**, **UNK-dropout** — klasik tokenizer’ın ötesine geçerek diyaloğa özgü kontrol, düşük OOV ve artırılmış dayanıklılık sağlar.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fc461",
   "metadata": {},
   "source": [
    "### KODA UYGULAMA ADIMINA GEÇELİM.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306e2b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSENTENCEPIECE-BPE TOKENIZER PIPELINE  (Chatbot sürümü)\\n───────────────────────────────────────────────────────\\n• Özel semboller: <pad> <sos> <eos> <user> <bot> <casual>\\n• Subword sözlük ≈1 800 parça  (küçük korpus)\\n• UNK-Dropout (%10)  →  veri tabanlı regularization\\n• tf.data.Dataset  →  shuffle • batch • prefetch\\n(Masking/model tarafı sonraki notebook’a bırakıldı)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SENTENCEPIECE-BPE TOKENIZER PIPELINE  (Chatbot sürümü)\n",
    "───────────────────────────────────────────────────────\n",
    "• Özel semboller: <pad> <sos> <eos> <user> <bot> <casual>\n",
    "• Subword sözlük ≈1 800 parça  (küçük korpus)\n",
    "• UNK-Dropout (%10)  →  veri tabanlı regularization\n",
    "• tf.data.Dataset  →  shuffle • batch • prefetch\n",
    "(Masking/model tarafı sonraki notebook’a bırakıldı)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9ba2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 0) Gereksinimler\n",
    "#    pip install sentencepiece pandas tensorflow==2.18\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f84dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\Transformerlar\\Konu Anlatımları\\Encoder - Decoder - PE - ATTN ( İLERİ DÜZEY ) = TF\\Tokenizer\\örnek_set .csv\")\n",
    "\n",
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8b6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6797f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER , BOT , CASUAL = \"<user>\" , \"<bot>\" , \"<casual>\"\n",
    "\n",
    "input_texts = [f\"{USER} {CASUAL} {clean_texts(t)}\" for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean_texts(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "093e083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 2) SentencePiece-BPE Eğitimi\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "MODEL_PREFİX , VOCAB_DESIRED , VOCAB_LIMIT = \"bpe_tr\" , 8000 , 1000\n",
    "\n",
    "with io.open(\"corpus.txt\" , \"w\" ,  encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(  input_texts+target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1d04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input = \"corpus.txt\",\n",
    "    model_prefix = MODEL_PREFİX,\n",
    "    model_type = \"bpe\",\n",
    "    vocab_size = min(VOCAB_DESIRED,VOCAB_LIMIT),\n",
    "    user_defined_symbols =\n",
    "    [\n",
    "     \"<pad>\" , \"<sos>\" , \"<eos>\" ,\n",
    "     USER,BOT,CASUAL\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27cf8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFİX}.model\")\n",
    "pad_id  = sp.piece_to_id(\"<pad>\")\n",
    "sos_id  = sp.piece_to_id(\"<sos>\")\n",
    "eos_id  = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f065649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 3) Encode & Pad\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "enc   = [sp.encode(t, out_type=int)                for t in input_texts]\n",
    "din   = [[sos_id] + sp.encode(t, out_type=int)     for t in target_texts]\n",
    "dout  = [sp.encode(t, out_type=int) + [eos_id]     for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "786f6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_len = max(max(map(len,enc)) , max(map(len,din)) , max(map(len,dout)))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a33ee992",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = pad_sequences(enc,maxlen=max_len ,padding=\"post\" , value=pad_id)\n",
    "dec_in = pad_sequences(din,maxlen=max_len , padding=\"post\" , value=pad_id)\n",
    "dec_out = pad_sequences(dout,maxlen=max_len,padding=\"post\" , value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e25080",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_RATE = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36a7a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unk_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munk_dropout\u001b[39m(inputs,targets,rate\u001b[38;5;241m=\u001b[39mDROP_RATE,unk\u001b[38;5;241m=\u001b[39m\u001b[43munk_id\u001b[49m):\n\u001b[0;32m      2\u001b[0m     e \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     d \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unk_id' is not defined"
     ]
    }
   ],
   "source": [
    "def unk_dropout(inputs,targets,rate=DROP_RATE,unk=unk_id):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"],\n",
    "\n",
    "    mask_e = tf.cast(tf.random.uniform(tf.shape(e)) < rate , e.dtype)\n",
    "    mask_d = tf.cast(tf.random.uniform(tf.shape(d)) < rate , d.dtype)\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_e == 1 , unk , e )\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_d == 1 , unk , d)\n",
    "\n",
    "    return inputs , targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af2ebc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ep \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m15\u001b[39m:  \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.12\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.08\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUnkScheduler\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCallback):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_epoch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m         rate \u001b[38;5;241m=\u001b[39m schedule(epoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def schedule(ep):\n",
    "    if ep < 5:   return 0.05\n",
    "    if ep < 15:  return 0.12\n",
    "    return 0.08\n",
    "\n",
    "class UnkScheduler(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        rate = schedule(epoch)\n",
    "        DROP_RATE.assign(rate)\n",
    "        print(f\"[UNK-Dropout] epoch {epoch} → rate={rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1731da2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m BATCH_SIZE  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m    \u001b[38;5;66;03m# her adımda GPU’ya gidecek örnek sayısı\u001b[39;00m\n\u001b[0;32m      5\u001b[0m BUFFER_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# shuffle havuzu (rastgelelik için)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m      9\u001b[0m         (\n\u001b[0;32m     10\u001b[0m             {\n\u001b[0;32m     11\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: enc_in,   \u001b[38;5;66;03m# ← modelin encoder girdisi\u001b[39;00m\n\u001b[0;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: dec_in    \u001b[38;5;66;03m# ← decoder girdisi (<sos> + hedef)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m             },\n\u001b[0;32m     14\u001b[0m             dec_out                       \u001b[38;5;66;03m# ← decoder hedef dizisi (<eos> ekli)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         )\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# 1️⃣  Rastgele %10 token’ı <unk> yap → dayanıklılık artır\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(unk_dropout, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# 2️⃣  BUFFER_SIZE kadar örnek bellekte karıştır → her epoch farklı sıra\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE)\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# 3️⃣  Sabit boyutlu kümeler oluştur; eksik son batch’i at\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# 4️⃣  Eğitim adımı GPU’da çalışırken bir sonraki batch’i hazırla\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# 5) tf.data.Dataset Pipeline\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "BATCH_SIZE  = 32    # her adımda GPU’ya gidecek örnek sayısı\n",
    "BUFFER_SIZE = 1000  # shuffle havuzu (rastgelelik için)\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"encoder_input\": enc_in,   # ← modelin encoder girdisi\n",
    "                \"decoder_input\": dec_in    # ← decoder girdisi (<sos> + hedef)\n",
    "            },\n",
    "            dec_out                       # ← decoder hedef dizisi (<eos> ekli)\n",
    "        )\n",
    "    )\n",
    "    # 1️⃣  Rastgele %10 token’ı <unk> yap → dayanıklılık artır\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # 2️⃣  BUFFER_SIZE kadar örnek bellekte karıştır → her epoch farklı sıra\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "\n",
    "    # 3️⃣  Sabit boyutlu kümeler oluştur; eksik son batch’i at\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    # 4️⃣  Eğitim adımı GPU’da çalışırken bir sonraki batch’i hazırla\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66a2db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK-Dropout Rate: 10% → ~92 token her epoch’ta <unk> olacak\n",
      "Dataset hazır → (32, 16)\n",
      "pad:3  sos:4  eos:5  unk:0  vocab:1000\n",
      "max_len: 16\n"
     ]
    }
   ],
   "source": [
    "# --------------------- Yüzdesel Bilgi Log’u ----------------------------------\n",
    "total_tokens   = (enc_in != pad_id).sum() + (dec_in != pad_id).sum()\n",
    "unk_tokens_est = int(total_tokens * DROP_RATE)    # teorik beklenti\n",
    "print(f\"UNK-Dropout Rate: {DROP_RATE*100:.0f}% → ~{unk_tokens_est:,} token her epoch’ta <unk> olacak\")\n",
    "\n",
    "print(\"Dataset hazır →\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(f\"pad:{pad_id}  sos:{sos_id}  eos:{eos_id}  unk:{unk_id}  vocab:{vocab_size}\")\n",
    "print(\"max_len:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597126c2",
   "metadata": {},
   "source": [
    "## UNK-Dropout Oranı (0.10) — Yüksek mi, Düşük mü?\n",
    "\n",
    "### 1. Ne Yapıyoruz?\n",
    "`DROP_RATE = 0.10` ⇒ **her epoch** sırasında, pad-dışı token’ların yaklaşık %10’u rastgele `<unk>` (OOV) olarak maskeleniyor.  \n",
    "Bu, modelin “eksik veya bilinmeyen” kelimelerle karşılaştığında paniğe kapılmamasını sağlar (regularization).\n",
    "\n",
    "\n",
    "### 2. Oran Çok **Düşük** Olursa (≤ 0.02)\n",
    "| Sonuç | Açıklama |\n",
    "|-------|----------|\n",
    "| Zayıf Etki | Model neredeyse her zaman gerçek kelimeleri görür → OOV senaryolarına hâlâ kırılgan kalabilir. |\n",
    "| Overfitting Riski | Düzenleyici (noise) çok az → aşırı ezberleme sürer. |\n",
    "\n",
    "\n",
    "\n",
    "### 3. Oran Çok **Yüksek** Olursa (≥ 0.20)\n",
    "| Sonuç | Açıklama |\n",
    "|-------|----------|\n",
    "| Bilgi Kaybı | Cümlenin anlamı bozulur; model yeterli bağlam göremez. |\n",
    "| Eğitim Yavaşlar | Daha fazla adım gerekir, valid/perf metriği düşebilir. |\n",
    "| “Dil” Yıpranması | Özellikle kısa cümlelerde her kelimenin `<unk>` olma riski artar. |\n",
    "\n",
    "\n",
    "\n",
    "### 4. Pratik Öneri  \n",
    "| Veri Boyutu | Önerilen Aralık |\n",
    "|-------------|-----------------|\n",
    "| 10 k – 100 k cümle | **0.05 – 0.10** |\n",
    "| 100 k – 1 M cümle  | **0.10 – 0.15** |\n",
    "| +1 M cümle         | 0.15’e kadar çıkılabilir; 0.20 genelde üst sınır |\n",
    "\n",
    "> **Başlangıç için** `0.10` (yani %10) yaygın ve güvenli bir değerdir.  \n",
    "> Eğitim/valid loss eğrilerini izleyin:  \n",
    "> • Çok oynaklık varsa oranı **azaltın**.  \n",
    "> • Valid-loss ↗️ ama train-loss ↘️ ise overfitting var demektir → oranı **artırın** (0.12–0.15).  \n",
    "\n",
    "\n",
    "### 5. Kademeli (Schedule) Stratejisi  \n",
    "- **İlk epoch’lar**: düşük (0.05) — model temeli öğrensin.  \n",
    "- **Sonraki epoch’lar**: orta (0.10) — dayanıklılığı artır.  \n",
    "- **Fine-tuning** aşamasında: tekrar düşür (0.02–0.05) — son hassasiyet.\n",
    "\n",
    "\n",
    "**Özet:** `0.10` tipik ve dengeli bir seçenektir. Çok düşük oran düzenleyici etkisini zayıflatır; çok yüksek oran ise anlamlı bağlamı aşındırır. Oranı veri boyutu, cümle uzunluğu ve valid-loss davranışına göre ayarlayın.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ccd93",
   "metadata": {},
   "source": [
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dff23b",
   "metadata": {},
   "source": [
    "##### Aslında yapılan bu işlemler kelime bazlı tokenizer işleminde fazlasıyla benzer.Orda yapılan elle token ekleme işlemlerini burda da yapacağız.Unk dropout gibi tabirleri burda da yeteri kadar önemle kullanacağız.Kelime bazlı tokenizer ı eğer anlayabildiyseniz diğer işlemleri de aynı şekilde anlayacaksınız.Şimdi yavaştan bu işlemlere geçelim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363c844",
   "metadata": {},
   "source": [
    "##  1.CSV Okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a5610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasıl yardımcı olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nasılsın?</td>\n",
       "      <td>İyiyim, teşekkür ederim. Siz nasılsınız?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adın ne?</td>\n",
       "      <td>Ben bir yapay zekâ asistanıyım. Adım yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaç yaşındasın?</td>\n",
       "      <td>Benim yaşım yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bugün günlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasıl yardımcı olabilirim?\n",
       "1            Nasılsın?           İyiyim, teşekkür ederim. Siz nasılsınız?\n",
       "2             Adın ne?  Ben bir yapay zekâ asistanıyım. Adım yok ama y...\n",
       "3      Kaç yaşındasın?                        Benim yaşım yok, dijitalim!\n",
       "4  Bugün günlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\Transformerlar\\Konu Anlatımları\\Encoder - Decoder - PE - ATTN ( İLERİ DÜZEY ) = TF\\Tokenizer\\örnek_set .csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebee5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f3080",
   "metadata": {},
   "source": [
    "## 2. Metin Temizleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "588299a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4b878",
   "metadata": {},
   "source": [
    "## User Bot ve Casual Etiketleri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c203a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user,bot,style = \"<user>\" , \"<bot>\" , \"<casual>\" \n",
    "\n",
    "input_texts = [f\"{user} {style} {clean_texts(t)}\" for t in raw_inputs]\n",
    "targets_text = [f\"{bot} {style} {clean_texts(t)}\" for t in raw_targets] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc582f5",
   "metadata": {},
   "source": [
    "## Corpus.txt dosyasını hazırlamak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3bc40",
   "metadata": {},
   "source": [
    "### Neden “corpus.txt” Dosyası Oluşturuyoruz?\n",
    "\n",
    "* SentencePiece’in BPE modelini eğitmek için **saf metin dosyası** (`.txt`) formatında bir girdi ister. Python’daki listeler veya DataFrame doğrudan kullanılamaz:\n",
    "\n",
    "* input_texts ve target_texts listelerindeki her cümleyi tek tek satır olarak dosyaya yazar.\n",
    "\n",
    "\n",
    "\n",
    "* Özet: Bu adım, BPE sözlük üretimi için zorunlu bir ön hazırlıktır—SentencePiece eğitim rutini metni dosyadan okur, belleğe değil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "933ee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PREFİX , VOCAB_DESIRED , VOCAB_LIMIT = \"bpe_tr\" , 8000 , 1000\n",
    "\n",
    "with io.open(\"corpus.txt\" , \"w\" ,  encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(  input_texts+targets_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad9d23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28158402",
   "metadata": {},
   "source": [
    "## BPE Modelini Eğitmek İçin `SentencePieceTrainer.Train` Çağrısı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373228d6",
   "metadata": {},
   "source": [
    "### Alt-Parça Sözlüğü (Subword Vocabulary) Oluşturma\n",
    "\n",
    "* BPE, sık görülen karakter dizilerini “parça” (piece) olarak modelleyerek OOV sorununu büyük ölçüde ortadan kaldırır.\n",
    "\n",
    "* Bu fonksiyon, kelime düzeyinde değil, subword düzeyinde bir sözlük üretir.\n",
    "\n",
    "### Parametrelerin Anlamı\n",
    "\n",
    "* input=\"corpus.txt\"\n",
    "\n",
    "Eğitim verisi olarak kullanılacak ham metin dosyasının yolu.\n",
    "\n",
    "* model_prefix=MODEL_PREFIX\n",
    "\n",
    "Oluşacak model dosyasının (“bpe_tr.model” / “bpe_tr.vocab”) önek adı.\n",
    "\n",
    "* model_type=\"bpe\"\n",
    "\n",
    "Byte-Pair Encoding algoritmasını seçer.\n",
    "\n",
    "* vocab_size=…\n",
    "\n",
    "Kaç subword parçası (ör. 1 800) eğitileceğini sınırlar.\n",
    "\n",
    "* user_defined_symbols=[…]\n",
    "\n",
    "pad>, <sos, eos> gibi özel token’ları ve konuşmacı/stil etiketlerini (USER, BOT, STYLE) sözlükte sabit tutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd1961cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp.SentencePieceTrainer.Train(\n",
    "    input=\"corpus.txt\",\n",
    "    model_prefix = MODEL_PREFİX,\n",
    "    model_type = \"bpe\",\n",
    "    vocab_size = min(VOCAB_DESIRED,VOCAB_LIMIT),\n",
    "    user_defined_symbols=[\n",
    "        \"<pad>\" , \"<sos>\" , \"<eos>\", user , bot , style\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6f14f",
   "metadata": {},
   "source": [
    "## `SentencePieceProcessor` ile Özel Token ID’lerini Alma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c49750",
   "metadata": {},
   "source": [
    "Bu kod bloğunda etiketleme değil, yüklenen BPE modelindeki özel token’ların ID’lerini alıyoruz:\n",
    "\n",
    "* spm.SentencePieceProcessor(...)\n",
    "\n",
    "Önceki adımda eğittiğimiz .model dosyasını yükler.\n",
    "\n",
    "* piece_to_id(\"pad>\"), …\n",
    "\n",
    "pad>, <sos, eos> gibi kontrol token’larının sayısal ID değerlerini döner.\n",
    "\n",
    "* unk_id = sp.unk_id()\n",
    "\n",
    "Modelin varsayılan OOV (unk) ID’sini alır.\n",
    "\n",
    "* vocab_size = sp.get_piece_size()\n",
    "\n",
    "Toplam parça sayısını (vocab büyüklüğü) öğrenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee7c6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file = f\"{MODEL_PREFİX}.model\")\n",
    "\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049aae1",
   "metadata": {},
   "source": [
    "## Encoder ve Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "622d889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [sp.encode(t,out_type=int) for t in input_texts]\n",
    "din = [[sos_id] + sp.encode(t,out_type=int) for t in target_texts]\n",
    "dout = [sp.encode(t,out_type=int) + [eos_id] for t in target_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a25f9b",
   "metadata": {},
   "source": [
    "## Max_len Belirleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6c2ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_len = max(max(map(len,enc)) , max(map(len,din)) , max(map(len,dout)))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac988d",
   "metadata": {},
   "source": [
    "## Pad_Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "410f0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = pad_sequences(enc,maxlen=max_len,padding=\"post\",value=pad_id)\n",
    "dec_in = pad_sequences(din,maxlen=max_len,padding=\"post\" , value=pad_id)\n",
    "dec_out = pad_sequences(dout,maxlen=max_len,padding=\"post\",value=pad_id)\n",
    "\n",
    "# value=pad_id derken kullandığımız sayı pad_id, BPE sözlüğünde <pad> token’ına karşılık gelen sayısal ID’dir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7d558",
   "metadata": {},
   "source": [
    "## UNK DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59b5cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_RATE =0.10\n",
    "\n",
    "def unk_dropout(inputs,targets,rate=DROP_RATE,unk=unk_id):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_e = tf.cast(tf.random.uniform(tf.shape(e)) < rate , e.dtype)\n",
    "    mask_d = tf.cast(tf.random.uniform(tf.shape(d)) < rate ,d.dtype)\n",
    "\n",
    "    e = tf.where(mask_e == 1 , unk ,e)\n",
    "    d = tf.where(mask_d == 1 ,unk , d)\n",
    "\n",
    "    return inputs , targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e7662c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epochs):\n",
    "\n",
    "    if epochs < 5 : return 0.05\n",
    "    if epochs < 15 : return 0.12 \n",
    "\n",
    "    return 0.08 \n",
    "\n",
    "class UnkScheduler(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        rate = schedule(epoch)\n",
    "        DROP_RATE.assign(rate)\n",
    "        print(f\"[UNK_DROPOUT] epoch {epoch} -> rate={rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9d546",
   "metadata": {},
   "source": [
    "## Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba6e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 1000\n",
    "batch = 32\n",
    "\n",
    "dataset =(\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "        {\n",
    "            \"encoder_input\": enc_in,\n",
    "            \"decoder_input\" : dec_in\n",
    "            },dec_out\n",
    "        )\n",
    "    )\n",
    "    .map(unk_dropout,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch,drop_remainder=True)\n",
    "    .shuffle(buffer)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88760abe",
   "metadata": {},
   "source": [
    "## Yüzdesel Bilgi Logu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e81cfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = (enc_in !=pad_id).sum()+(dec_in != pad_id).sum()\n",
    "unk_tokens_est = int(total_tokens * DROP_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a350cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK-Dropout Rate: 10% → ~92 token her epoch’ta <unk> olacak\n",
      "Dataset hazır → (32, 16)\n",
      "pad:3  sos:4  eos:5  unk:0  vocab:1000\n",
      "max_len: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"UNK-Dropout Rate: {DROP_RATE*100:.0f}% → ~{unk_tokens_est:,} token her epoch’ta <unk> olacak\")\n",
    "\n",
    "print(\"Dataset hazır →\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(f\"pad:{pad_id}  sos:{sos_id}  eos:{eos_id}  unk:{unk_id}  vocab:{vocab_size}\")\n",
    "print(\"max_len:\", max_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5e204",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8780f1e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
