{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae95b385",
   "metadata": {},
   "source": [
    "## Kelime-Bazlı Tokenizer Pipeline — Teorik Özeti\n",
    "\n",
    "### 1. Temizleme & Normalizasyon  \n",
    "- **Unicode Normalizasyon** (`NFC`) ve **küçültme**: Metindeki tüm harfleri standart forma getirir, büyük/küçük farkını ortadan kaldırır.  \n",
    "- **Regex Filtreleme**: Sadece Türkçe karakterler, sayılar ve boşluklar korunur; geri kalan tüm işaretler boşluğa dönüştürülür.  \n",
    "\n",
    "### 2. Konuşmacı & Stil Etiketleri  \n",
    "- **`<user>` / `<bot>`**: Diyalog metnindeki rol bilgisini netleştirir.  \n",
    "- **`<casual>`** (veya `<formal>`): Modelin hangi tonda cevap vereceğini belirtir.  \n",
    "\n",
    "Metin başına eklenen bu token’lar, modelin “kimin” ve “hangi tarzda” konuştuğunu öğrenmesine imkân tanır.\n",
    "\n",
    "### 3. Sıra Kontrol Token’ları  \n",
    "- **`<sos>` / `<eos>`**: Decoder’a “başla” ve “bitir” sinyalleri gönderir.  \n",
    "- **Padding (`0` ID’si)**: Tüm diziler aynı uzunluğa getirilir; model sabit boyutlu tensörlerle çalışır.\n",
    "\n",
    "### 4. Frekans-Bazlı Sözlük İnşası  \n",
    "- **Geçici Tokenizer** ile tüm korpustaki token frekansları sayılır.  \n",
    "- **`min_freq` Filtresi**: Çok nadir (örn. yalnızca 1 kez) görülen kelimeler sözlük dışına alınır → gürültü azalır.  \n",
    "- **Özel Token’ların Eklenmesi**: `<user>`, `<bot>`, `<casual>`, `<unk>`, `<sos>`, `<eos>` kesinlikle sözlükte kalır.\n",
    "\n",
    "### 5. Final Tokenizer & Dönüşüm  \n",
    "- Sabitlenen `keep_tokens` listesiyle **`Tokenizer.fit_on_texts`** çağrıları yapılır.  \n",
    "- Metinler, **`texts_to_sequences`** ile ID dizilerine; ardından **`pad_sequences`** ile eşit uzunluğa dönüştürülür.  \n",
    "\n",
    "### 6. Dynamic UNK-Dropout  \n",
    "- Eğitim sırasında **rastgele** belli oranla (örn. `%5–12`) gerçek token’lar **`<unk>`** (OOV) ile değiştirilir.  \n",
    "- **Epoch takvimi** ile dropout oranı  \n",
    "  - Erken dönemde düşük (%5) → temel yapıyı öğrenme  \n",
    "  - Orta dönemde yüksek (%12) → genelleme  \n",
    "  - Son dönemde orta seviyeye (%8) → ince ayar  \n",
    "- Model, eksik veya bilinmeyen kelimelere karşı daha dayanıklı hâle gelir.\n",
    "\n",
    "### 7. `tf.data.Dataset` Pipeline  \n",
    "- **`.map(unk_dropout)`** → her batch’te augmentasyon  \n",
    "- **`.shuffle()`** → her epoch veriyi karıştırma  \n",
    "- **`.batch(drop_remainder=True)`** → sabit batch boyutu  \n",
    "- **`.prefetch()`** → GPU/CPU paralelliği  \n",
    "\n",
    "\n",
    "Bu adımlar bir araya geldiğinde, **küçük ve orta ölçekli Türkçe chatbot** projeleri için hafif, esnek ve üretime hazır bir tokenizer hattı elde edilmiş olur—dış bağımlılıksız, okunabilir ve kolayca ayarlanabilir. ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91b5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c8f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"\"\n",
    "OOV_TOKEN = \"<unk>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "CHAR_LEVEL = False\n",
    "MİN_FREQ = 1\n",
    "MAX_VOCAB = None\n",
    "PADDING = \"post\"\n",
    "\n",
    "USER_TOKEN , BOT_TOKEN , STYLE_TOKEN = \"<user>\" , \"<bot>\" , \"<casual>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93536d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasıl yardımcı olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nasılsın?</td>\n",
       "      <td>İyiyim, teşekkür ederim. Siz nasılsınız?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adın ne?</td>\n",
       "      <td>Ben bir yapay zekâ asistanıyım. Adım yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaç yaşındasın?</td>\n",
       "      <td>Benim yaşım yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bugün günlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasıl yardımcı olabilirim?\n",
       "1            Nasılsın?           İyiyim, teşekkür ederim. Siz nasılsınız?\n",
       "2             Adın ne?  Ben bir yapay zekâ asistanıyım. Adım yok ama y...\n",
       "3      Kaç yaşındasın?                        Benim yaşım yok, dijitalim!\n",
       "4  Bugün günlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"örnek_set .csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c723aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d45bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text:str)->str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf295834",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [f\"{USER_TOKEN} {STYLE_TOKEN} {clean_texts(t)}\" for t in raw_input]\n",
    "targets = [f\"{BOT_TOKEN} {STYLE_TOKEN} {clean_texts(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_in = [f\"{SOS_TOKEN} {t}\" for t in targets]\n",
    "dec_out = [f\"{t} {EOS_TOKEN}\" for t in targets]\n",
    "all_texts = inputs + dec_in + dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091a02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "İlk geçişte tüm metinleri tarayıp her token’ın kaç kez göründüğünü sayıyoruz\n",
    "Böylece hangi kelimelerin “nadiren” (örneğin yalnızca 1 kez) geçtiğini görebiliriz\n",
    "'''\n",
    "\n",
    "# ── 3) Geçici tokenizer → frekans tablosu ─────────────────────\n",
    "tmp_tok = Tokenizer(filters=FILTERS, oov_token=OOV_TOKEN, char_level=CHAR_LEVEL)\n",
    "tmp_tok.fit_on_texts(all_texts)\n",
    "word_counts = tmp_tok.word_counts   # {token: count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49d78477",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeps_token = [w for w,c in word_counts.items() if c >= MİN_FREQ]\n",
    "\n",
    "for sp in (USER_TOKEN,BOT_TOKEN,STYLE_TOKEN,OOV_TOKEN,SOS_TOKEN,EOS_TOKEN):\n",
    "    if sp not in keeps_token:\n",
    "        keeps_token.append(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a08a837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " '<casual>',\n",
       " 'merhaba',\n",
       " 'nasılsın',\n",
       " 'adın',\n",
       " 'ne',\n",
       " 'kaç',\n",
       " 'yaşındasın',\n",
       " 'bugün',\n",
       " 'günlerden',\n",
       " 'hangi',\n",
       " 'dilleri',\n",
       " 'konuşuyorsun',\n",
       " 'bana',\n",
       " 'bir',\n",
       " 'şaka',\n",
       " 'yap',\n",
       " 'seni',\n",
       " 'kim',\n",
       " 'yaptı',\n",
       " 'işe',\n",
       " 'yararsın',\n",
       " 'telefon',\n",
       " 'numaran',\n",
       " 'nedir',\n",
       " 'hava',\n",
       " 'nasıl',\n",
       " 'seviyorum',\n",
       " 'sıkıldım',\n",
       " 'film',\n",
       " 'öner',\n",
       " 'yemek',\n",
       " 'yapayım',\n",
       " 'benimle',\n",
       " 'konuş',\n",
       " 'yalnızım',\n",
       " 'uyuyamıyorum',\n",
       " 'biraz',\n",
       " 'müzik',\n",
       " 'aç',\n",
       " 'şiir',\n",
       " 'oku',\n",
       " 'sence',\n",
       " 'hayat',\n",
       " 'arkadaşın',\n",
       " 'var',\n",
       " 'mı',\n",
       " 'oyun',\n",
       " 'oyna',\n",
       " 'sen',\n",
       " 'zeki',\n",
       " 'misin',\n",
       " 'gece',\n",
       " 'mi',\n",
       " 'gündüz',\n",
       " 'mü',\n",
       " 'yarın',\n",
       " 'olacak',\n",
       " 'yazılım',\n",
       " 'bilgisayar',\n",
       " 'i',\n",
       " 'nterneti',\n",
       " 'buldu',\n",
       " 'gerçek',\n",
       " 'robot',\n",
       " 'musun',\n",
       " 'en',\n",
       " 'iyi',\n",
       " 'programlama',\n",
       " 'dili',\n",
       " 'hangisi',\n",
       " 'kodlama',\n",
       " 'öğrenmek',\n",
       " 'zor',\n",
       " 'mu',\n",
       " 'duygusal',\n",
       " 'mısın',\n",
       " 'yapay',\n",
       " 'zek',\n",
       " 'makine',\n",
       " 'öğrenmesi',\n",
       " 'veri',\n",
       " 'bilimi',\n",
       " 'python',\n",
       " 'java',\n",
       " 'tarih',\n",
       " 'rastgele',\n",
       " 'bilgi',\n",
       " 'ver',\n",
       " 'sevilen',\n",
       " 'uyumalı',\n",
       " 'mıyım',\n",
       " 'motive',\n",
       " 'olabilirim',\n",
       " 'hikaye',\n",
       " 'anlat',\n",
       " 'ilham',\n",
       " 'beni',\n",
       " 'güldür',\n",
       " 'sıkıcı',\n",
       " 'hoşça',\n",
       " 'kal',\n",
       " '<sos>',\n",
       " '[t]',\n",
       " '<bot>',\n",
       " 'size',\n",
       " 'yardımcı',\n",
       " '<eos>',\n",
       " 'yiyim',\n",
       " 'teşekkür',\n",
       " 'ederim',\n",
       " 'siz',\n",
       " 'nasılsınız',\n",
       " 'ben',\n",
       " 'asistanıyım',\n",
       " 'adım',\n",
       " 'yok',\n",
       " 'ama',\n",
       " 'benim',\n",
       " 'yaşım',\n",
       " 'dijitalim',\n",
       " 'maalesef',\n",
       " 'bilgim',\n",
       " 'sistem',\n",
       " 'saatinden',\n",
       " 'kontrol',\n",
       " 'edebilirsiniz',\n",
       " 'türkçe',\n",
       " 'başta',\n",
       " 'olmak',\n",
       " 'üzere',\n",
       " 'birçok',\n",
       " 'anlayabiliyorum',\n",
       " 'elbette',\n",
       " 'neden',\n",
       " 'denize',\n",
       " 'giremez',\n",
       " 'çünkü',\n",
       " 'su',\n",
       " 'geçirmez',\n",
       " 'değil',\n",
       " 'geliştiricisi',\n",
       " 'tarafından',\n",
       " 'eğitildim',\n",
       " 'sorularınızı',\n",
       " 'yanıtlayabilir',\n",
       " 'verebilirim',\n",
       " 'üzgünüm',\n",
       " 'numaram',\n",
       " 'zamanlı',\n",
       " 'alamıyorum',\n",
       " 'durumu',\n",
       " 'sitesinden',\n",
       " 'bakabilirsiniz',\n",
       " 'de',\n",
       " 'için',\n",
       " 'buradayım',\n",
       " 'sterseniz',\n",
       " 'ya',\n",
       " 'da',\n",
       " 'önerebilirim',\n",
       " 'esaretin',\n",
       " 'bedeli',\n",
       " 'harika',\n",
       " 'olabilir',\n",
       " 'makarna',\n",
       " 'menemen',\n",
       " 'hızlı',\n",
       " 've',\n",
       " 'lezzetli',\n",
       " 'seçeneklerdir',\n",
       " 'tabii',\n",
       " 'ki',\n",
       " 'hakkında',\n",
       " 'konuşmak',\n",
       " 'istersiniz',\n",
       " 'sohbet',\n",
       " 'edebiliriz',\n",
       " 'bugünün',\n",
       " 'tarihini',\n",
       " 'rahatlatıcı',\n",
       " 'dinlemeyi',\n",
       " 'deneyin',\n",
       " 'açamıyorum',\n",
       " 'öneride',\n",
       " 'bulunabilirim',\n",
       " 'lo',\n",
       " 'fi',\n",
       " 'chill',\n",
       " 'gider',\n",
       " 'gökyüzü',\n",
       " 'masmavi',\n",
       " 'umutlar',\n",
       " 'gibi',\n",
       " 'paylaşmaktır',\n",
       " 'arkadaşlarım',\n",
       " 'verilerim',\n",
       " 'kelime',\n",
       " 'oyunu',\n",
       " 'oynayabiliriz',\n",
       " 'istersen',\n",
       " 'başlayalım',\n",
       " 'verilerle',\n",
       " 'bu',\n",
       " 'yüzden',\n",
       " 'bazı',\n",
       " 'konularda',\n",
       " 'bilgiye',\n",
       " 'erişemem',\n",
       " 'saatinize',\n",
       " 'geleceği',\n",
       " 'bilemem',\n",
       " 'güzel',\n",
       " 'gün',\n",
       " 'olmasını',\n",
       " 'dilerim',\n",
       " 'bilgisayarlara',\n",
       " 'yapacağını',\n",
       " 'söyleyen',\n",
       " 'komutlar',\n",
       " 'bütünüdür',\n",
       " 'işleyebilen',\n",
       " 'elektronik',\n",
       " 'cihazdır',\n",
       " 'nternetin',\n",
       " 'temelini',\n",
       " 'arpanet',\n",
       " 'tim',\n",
       " 'berners',\n",
       " 'lee',\n",
       " 'attı',\n",
       " 'dijital',\n",
       " 'varlığım',\n",
       " 'fiziksel',\n",
       " 'değilim',\n",
       " 'tabanlı',\n",
       " 'asistanım',\n",
       " 'amaca',\n",
       " 'göre',\n",
       " 'değişir',\n",
       " 'oldukça',\n",
       " 'popülerdir',\n",
       " 'başlangıçta',\n",
       " 'pratikle',\n",
       " 'çok',\n",
       " 'keyifli',\n",
       " 'hale',\n",
       " 'gelir',\n",
       " 'duygularım',\n",
       " 'sizi',\n",
       " 'anlayabilirim',\n",
       " 'insan',\n",
       " 'benzeri',\n",
       " 'görevleri',\n",
       " 'yapabilen',\n",
       " 'yazılımlardır',\n",
       " 'verilerden',\n",
       " 'öğrenerek',\n",
       " 'tahmin',\n",
       " 'algoritmalardır',\n",
       " 'veriden',\n",
       " 'anlamlı',\n",
       " 'çıkarmaya',\n",
       " 'yarayan',\n",
       " 'bilim',\n",
       " 'dalıdır',\n",
       " 'sade',\n",
       " 'güçlü',\n",
       " 'dilidir',\n",
       " 'yeni',\n",
       " 'başlayanlar',\n",
       " 'genelde',\n",
       " 'önerilir',\n",
       " 'bilgisine',\n",
       " 'erişimim',\n",
       " 'balinalar',\n",
       " 'nefes',\n",
       " 'almak',\n",
       " 'yüzeyine',\n",
       " 'çıkar',\n",
       " 'kişiye',\n",
       " 'pizza',\n",
       " 'sevilir',\n",
       " 'eğer',\n",
       " 'yorgunsanız',\n",
       " 'uyumanız',\n",
       " 'olur',\n",
       " 'küçük',\n",
       " 'hedefler',\n",
       " 'koyarak',\n",
       " 'başlayabilirsiniz',\n",
       " 'zamanlar',\n",
       " 'asistan',\n",
       " 'vardı',\n",
       " 'hep',\n",
       " 'olmaya',\n",
       " 'çalışırdı',\n",
       " 'her',\n",
       " 'daha',\n",
       " 'ileri',\n",
       " 'gidebilirsin',\n",
       " 'peki',\n",
       " 'bilgisayarlar',\n",
       " 'kışın',\n",
       " 'üşümez',\n",
       " 'cache',\n",
       " 'leri',\n",
       " 'vardır',\n",
       " 'şeyler',\n",
       " 'hayatı',\n",
       " 'renklendirebilir',\n",
       " 'görüşmek',\n",
       " 'olabildiysem',\n",
       " 'mutlu',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keeps_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0581f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_VOCAB:\n",
    "    keeps_token = sorted(\n",
    "        keeps_token,\n",
    "        key = lambda w :word_counts.get(w,0),\n",
    "        reverse=True\n",
    "        )[:MAX_VOCAB]\n",
    "    \n",
    "    '''\n",
    "Sıralama: En yüksek frekanslı kelimeler öncelikli.\n",
    "\n",
    "Dilime Alma [:MAX_VOCAB]: Sadece ilk N kelime kalır.\n",
    "\n",
    "Böylece embed tablonuzun boyutu doğrudan MAX_VOCAB ile sınırlanır, geri kalan her şey eğitimin tamamında <unk> olarak temsil edilir.\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaf66e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer = Tokenizer(filters=FILTERS , oov_token=OOV_TOKEN, char_level= CHAR_LEVEL,num_words=len(keeps_token)+1)\n",
    "\n",
    "Tokenizer.fit_on_texts(keeps_token)\n",
    "Tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c646baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "inp_seq = Tokenizer.texts_to_sequences(inputs)\n",
    "dec_in_seq = Tokenizer.texts_to_sequences(dec_in)\n",
    "dec_out_seq = Tokenizer.texts_to_sequences(dec_out)\n",
    "\n",
    "max_len = max(map(len , inp_seq + dec_in_seq + dec_out_seq))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "902259fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(inp_seq,padding=PADDING,value=0,maxlen=max_len)\n",
    "decoder_input = pad_sequences(dec_in_seq,padding=PADDING,value=0,maxlen=max_len)\n",
    "decoder_output = pad_sequences(dec_out_seq,padding=PADDING,value=0,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f96a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary coverage (min_freq=1): 100.00%\n"
     ]
    }
   ],
   "source": [
    "coverage = sum(word_counts.get(w, 0) for w in keeps_token) / sum(word_counts.values())\n",
    "print(f\"Vocabulary coverage (min_freq={MİN_FREQ}): {coverage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e7c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_id == 316\n"
     ]
    }
   ],
   "source": [
    "unk_id = Tokenizer.word_index.get(OOV_TOKEN)\n",
    "drop_var = tf.Variable(0.05,trainable=False,dtype= tf.float32)\n",
    "\n",
    "print(\"unk_id ==\" ,unk_id)\n",
    "\n",
    "def unk_dropout(inputs,targets):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"]\n",
    "\n",
    " # maskeler bool olmalı\n",
    "    mask_e = tf.random.uniform(tf.shape(e), dtype=tf.float32) < drop_var\n",
    "    mask_d = tf.random.uniform(tf.shape(d), dtype=tf.float32) < drop_var\n",
    "\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_e, unk_id, e)\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_d, unk_id, d)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ac5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    if epoch < 5: return 0.05\n",
    "    if epoch < 15 : return 0.12\n",
    "\n",
    "    return 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3532a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnkSchedule(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epoch,logs=None):\n",
    "        rate = schedule(epoch)\n",
    "        drop_var.assign(rate)\n",
    "        print(f\"[UNK-Dropout] epoch {epoch} → rate={rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53360d5",
   "metadata": {},
   "source": [
    "### Modeli bu pipeline ile eğitmek için:\n",
    "#### model.fit(dataset, epochs=20, callbacks=[UnkScheduler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f29508",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d034880",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((\n",
    "        {\"encoder_input\": encoder_input, \n",
    "         \"decoder_input\": decoder_input\n",
    "        },\n",
    "         decoder_output)\n",
    "    )\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71570a3b",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7eeb6",
   "metadata": {},
   "source": [
    "## Bazı kodların ne anlama geldiğine gelin biraz daha bakalım.Kısa fonksiyonlar ya da bakıldığında soru işareti içerebilecek sorunları ortadan kaldırmaya çalışalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b928d1",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd99328f",
   "metadata": {},
   "source": [
    "* Önce frame i ekleyelim.\n",
    "* Sonra clean_texts ile frame i temizleyelim.\n",
    "* Sonra frame i input ve targets olarak 2 parçaya ayıralım.\n",
    "* Sonra bot user ve casual etiketlerini geçirelim.\n",
    "* Sonra sos ve eos tokenlarını projeye entegre edelim.\n",
    "* Geçici tokenizer i eğittikden sonra min_freq uygulamasına geçelim.\n",
    "* Sonrasında max_vocab ile sınırlandırmaları belirleyelim.\n",
    "* Sonrasında fit_on_texts ve texts_to_sequences fonklarıyla tokenizer i eğitelim\n",
    "* Sonrasında maxlen ve padding işlemlerini yapalım.\n",
    "* Sonrasında unk_dropout işlemlerini yapalım ve dataset yolunu oluşturalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7586c5",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed5148",
   "metadata": {},
   "source": [
    "### Keeps tokens i inceleyelim.Bakalım burada yapılan işlemlerin amacı neymiş.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d795e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "keeps_token = [w for w,c  in word_counts.items() if c>= MİN_FREQ]\n",
    "\n",
    "for sp in (USER_TOKEN,BOT_TOKEN,SOS_TOKEN,EOS_TOKEN,OOV_TOKEN,STYLE_TOKEN):\n",
    "    if sp not in keeps_token:\n",
    "        keeps_token.append(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d785cc1",
   "metadata": {},
   "source": [
    "* Burada yapılan işlem keeps token değişkenindeki c sabiti eğer min_freq ( min frekans ) değerinden büyük ve eşit ise çalışıyor.Yani keeps token içerisinde bulunan word_counts değişkenindeki her kelime önce min freq ile kıyaslanıyor ve eğer min_freq değerinden büyükse işleme alınıyor.Sp denilen değişken ise eğer keeps_token içerisindeki değerlere sahip değilse elle ekleniyor.Aslında temel mantık min_freq işleminine göre kelime bazlı kontrol yapılıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d113f",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ced08",
   "metadata": {},
   "source": [
    "### Max_Vocab değerini ölçelim ve bunu neden işleme sokuyoruz tartışalım..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d184488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAX_VOCAB:\n",
    "    keeps_token = sorted(\n",
    "           keeps_token,\n",
    "           key = lambda w: word_counts.get(w,0),\n",
    "           reverse = True        \n",
    "                   )[:MAX_VOCAB]\n",
    "    \n",
    "    '''\n",
    "    reverse=True\n",
    "\n",
    "Bu sayede en yüksek frekansa sahip kelimeler listenin başına gelir (azalan düzende sıralama).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    key=lambda w: word_counts.get(w,0)\n",
    "\n",
    "sorted() fonksiyonuna verdiğiniz bu key parametresi, her öğe (w) için hangi değere göre sıralama yapacağınızı tanımlar.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee039dd",
   "metadata": {},
   "source": [
    "* Keeps tokenin içerisindeki değerler tek tek en büyük ve en küçük sıralamasına sokuluyor.Keepss token üzerinden işlem yapılarak bir key oluşturuluyor.Bu key her w için hangi değere göre sıralama yapacağını belitir.Reverse parametresi en büyük değeri listenin başına yerleştirir.Ve bu işlemler sıralamadan sonra listenin ilk MAX_VOCAB elemanını alır.Yani keeps_token değişkeni artık en sık geçen ilk MAX_VOCAB kelimeyi içerir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace7cfe",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a59dbd",
   "metadata": {},
   "source": [
    "### Son olarak unk_dropout kısmına göz gezdirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0cdf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = Tokenizer.word_index.get(OOV_TOKEN)\n",
    "drop_var = tf.Variable(0.05,trainable=False,dtype = tf.float32)\n",
    "\n",
    "def unk_dropout(inputs,targets):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_e = tf.random.uniform(tf.shape(e) , dtype = tf.float32) < drop_var  \n",
    "    mask_d = tf.random.uniform(tf.shape(d) , dtype = tf.float32) < drop_var\n",
    "\n",
    "    inputs[\"decoder_input\"] = tf.where(mask_d,unk_id ,d)\n",
    "    inputs[\"encoder_input\"] = tf.where(mask_e,unk_id,e)\n",
    "\n",
    "    return targets,inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6dac82",
   "metadata": {},
   "source": [
    "* Önce OOV_TOKEN değerinin id numarasını unk_id değişkenine atadık.Drop var değişkeni yüzde 5 den başlayarak eğitilmeden devam edecek.UNK_DROPOUT fonksiyonunda ise birazdan tanımlayacağımız daha doğrusu parametre olarak alacağımız değerleri kullanacağız.Bu çıktının temel amacı belirli yüzdeliklerde bulunan değerlerle epoch sayısına oran bağlamak.Diğer fonksiyona geçelim.Çok daha rahat anlayacaksınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4cc1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    if epoch < 5 :  return 0.05\n",
    "    if epoch < 15 : return 0.12\n",
    "\n",
    "    return 0.08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6b03a",
   "metadata": {},
   "source": [
    "* Bu fonksiyon hangi epoch değerlerinde hangi drop_var oranına sahip olacağını belirler.Yani 3. epochta yüzde 5 inceleme , 12. epochda 0.12 inceleme devreye girecek.Eğer epoch sayısı if e giremezse yüzde 8 lik bir işlem uygulanacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnkSchedule(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self ,epoch ,  logs=None):\n",
    "        rate = schedule(epoch)\n",
    "        drop_var.assign(rate)\n",
    "        print(f\"[UNK-Dropout] epoch {epoch} → rate={rate:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15364cdc",
   "metadata": {},
   "source": [
    "* Burada bir callbacks haline getirdik.Aslında dataset pipeline ı gibi bir işlem bu.Nasılsa EarlyStopping bir callbacks ise , bu da bir callbacks haline geldi.Yukarıda anlatıldığı gibi işleme soktuk.\n",
    "\n",
    "* Orada bulunan logs parametresi şunu ifade eder:\n",
    "\n",
    "---- Biz loss fonksiyonu çağırırken val_loss gibi değişkenler ortaya çıkıyor.Aslında bu logs değişkeni yanındaki logları ifade ediyor. :d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db3dfc",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89f829",
   "metadata": {},
   "source": [
    "### -- Bir kaç soru cevap ekleyelim.İlk öğrendiğimde bana zor gelen bir kaç soru üzerinden sizinle sohbet edelim. -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62082885",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e940884",
   "metadata": {},
   "source": [
    "### 1) Geçici Tokenizer Neden Oluşturuldu?  \n",
    "**(Hangi Kontrolleri Yapıyor, Bize Nasıl Fayda Sağlıyor?)**\n",
    "\n",
    "1. **Frekans Sayımı İçin Tek Geçiş**  \n",
    "   - Korpustaki **tüm kelimeleri** tarar,  \n",
    "   - `word_counts = {token: görüldüğü_sayısı}` sözlüğü üretir.\n",
    "\n",
    "2. **`min_freq` Filtresi Uygulamak**  \n",
    "   - `min_freq` altındaki “çok nadir” kelimeleri sözlük dışı bırakır.  \n",
    "   - Gürültüyü azaltır, bellek kullanımını düşürür.\n",
    "\n",
    "3. **Özel Token’ları Garantiye Almak**  \n",
    "   - `<user>`, `<bot>`, `<casual>`, `<unk>`, `<sos>`, `<eos>`  \n",
    "     frekansa bakılmaksızın `keep_tokens` listesine eklenir.  \n",
    "\n",
    "4. **`max_vocab` Kesiti (İsteğe Bağlı)**  \n",
    "   - En yüksek frekanslı ilk `N` kelimeyi tutarak sözlüğü sınırlar.  \n",
    "   - Embedding tablosu küçük kalır, model hızı artar.\n",
    "\n",
    "> Eğer geçici tokenizer yerine doğrudan `Tokenizer.fit_on_texts` yapsaydık:  \n",
    "> - **Tüm nadir kelimeler** sözlüğe girerdi → Vocab şişerdi.  \n",
    "> - “UNK fırlaması” sonrası sorunlar için geriye dönüp temizleme gerekir, verimsiz olur.\n",
    "\n",
    "Kısacası: **Geçici tokenizer**, kalıcı sözlüğü “temiz & kontrollü” inşa etmek için ara toplama ve filtreleme katmanıdır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e15e15",
   "metadata": {},
   "source": [
    "### 2) `keep_tokens` Değişkeni Ne İşe Yarar?  \n",
    "**(Tokenizer İçindeki Rolü)**\n",
    "\n",
    "`keep_tokens`, korpusta tutmak istediğimiz **“geçerli”** kelimelerin listesidir.  \n",
    "Bu liste iki aşamalı filtrelemenin sonucunda oluşur:\n",
    "\n",
    "1. **`min_freq` Filtresi**  \n",
    "   - `word_counts[token] ≥ min_freq` olan kelimeler seçilir.  \n",
    "   - Böylece tek-tük (çok nadir) kelimeler elenir → sözlük şişmez.\n",
    "\n",
    "2. **Özel Token Zorlaması**  \n",
    "   - `<user>`, `<bot>`, `<casual>`, `<unk>`, `<sos>`, `<eos>`  \n",
    "     frekansa bakılmaksızın listeye eklenir.  \n",
    "   - Model, rol/stil ve kontrol sembollerini kesinlikle tanır.\n",
    "\n",
    "> **Sonuç:**  \n",
    "> - `keep_tokens` **yalnızca** sözlükte yer alacak kelimeleri içerir.  \n",
    "> - Nihai `Tokenizer.fit_on_texts(keep_tokens)` çağrısıyla **kelime-ID haritası** sabitlenir.  \n",
    "> - Geri kalan tüm nadir kelimeler eğitim sırasında **`<unk>`** olarak temsil edilir, model gereksiz gürültüyle uğraşmaz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8882c2",
   "metadata": {},
   "source": [
    "### 3) `MAX_VOCAB` Parametresi ve Sıralama Fonksiyonunun Amacı  \n",
    "\n",
    "`MAX_VOCAB` ≈ “sözlük için **üst sınır**”.  \n",
    "Kodda yapılan işlem:\n",
    "\n",
    "```python\n",
    "if MAX_VOCAB:\n",
    "    keep_tokens = sorted(\n",
    "        keep_tokens,\n",
    "        key=lambda w: word_counts.get(w, 0),  # en sık kelimeler öne\n",
    "        reverse=True\n",
    "    )[:MAX_VOCAB]                             # ilk N kelimeyi tut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddd830",
   "metadata": {},
   "source": [
    "### 4-) Kelime kapsamı ( coverage) neden kontrol ediliyor? %100 değişkeni ya da min_freq'in oradaki görevi nedir ?\n",
    "\n",
    "* Kelime kapsamı tokenizerın kelime-kelime verinin yüzde kaçını kapsadığına bakıyor.Daha açıklayıcı bir tanım yapmak istersek ;\n",
    "\n",
    "-- Min_freq = 1 iken % 100\n",
    "\n",
    "-- Min_freq = 2 iken %96\n",
    "\n",
    "-- Min_freq = 6 iken %67 lik bir kapsam ortaya çıktığını varsayalım.\n",
    "\n",
    "* Frekans değerini 1 olarak ayarladığımız zaman ( her kelime 1 kere geçiyorsa ) modelin tamamına eşleştirme yapabiliyoruz.Aynı şekilde frekans değerini 6 yaparsak , tokenizer ( 6 kere geçen kelimeler üzerinden ) %67 lik bir kapsam oluşturabiliyor.Aslında burada temel prensibe gelecek olursak: \n",
    "\n",
    "##### UNK ya da OOV tokenını modele mantıklı bir şekilde öğretmek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6e224",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
