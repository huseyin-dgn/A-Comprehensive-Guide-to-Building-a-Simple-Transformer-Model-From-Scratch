{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbc101a",
   "metadata": {},
   "source": [
    "## Bu ipynb dosyasÄ±nda Positionel Encoder ve Embedding katmanlarÄ±nÄ± inceleyeceÄŸiz.Bu katmanlarÄ± daha iyi hale getirmeye Ã§alÄ±ÅŸacaÄŸÄ±z.Ama tÃ¼m bu iÅŸlemlerden Ã¶nce tokenizer kodlarÄ±nÄ± yazalÄ±m.YazdÄ±ktan sonra PE ve Embedding kavramlarÄ±nÄ±n detayÄ±na inmeye devam edelim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f7f0",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c7c0a",
   "metadata": {},
   "source": [
    "### Ã–NCE TOKENÄ°ZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0ee3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047b1bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NasÄ±lsÄ±n?</td>\n",
       "      <td>Ä°yiyim, teÅŸekkÃ¼r ederim. Siz nasÄ±lsÄ±nÄ±z?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdÄ±n ne?</td>\n",
       "      <td>Ben bir yapay zekÃ¢ asistanÄ±yÄ±m. AdÄ±m yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KaÃ§ yaÅŸÄ±ndasÄ±n?</td>\n",
       "      <td>Benim yaÅŸÄ±m yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BugÃ¼n gÃ¼nlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasÄ±l yardÄ±mcÄ± olabilirim?\n",
       "1            NasÄ±lsÄ±n?           Ä°yiyim, teÅŸekkÃ¼r ederim. Siz nasÄ±lsÄ±nÄ±z?\n",
       "2             AdÄ±n ne?  Ben bir yapay zekÃ¢ asistanÄ±yÄ±m. AdÄ±m yok ama y...\n",
       "3      KaÃ§ yaÅŸÄ±ndasÄ±n?                        Benim yaÅŸÄ±m yok, dijitalim!\n",
       "4  BugÃ¼n gÃ¼nlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\MasaÃ¼stÃ¼\\Transformerlar\\Konu AnlatÄ±mlarÄ±\\Encoder - Decoder - PE - ATTN ( Ä°LERÄ° DÃœZEY ) = TF\\(2) -- PE & Embedding\\Ã¶rnek_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce3bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0761be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼ÄŸ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ae409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER, BOT , CASUAL = \"<user>\" , \"<bot>\" , \"<casual>\"\n",
    "\n",
    "input_texts = [f\"{USER} {CASUAL} {clean_texts(t)}\"  for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean_texts(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ab262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix , vocab_desired , vocab_limit = \"bpe_tr\" , 8000 , 1000 \n",
    "\n",
    "with io.open(\"corpus.txt\" , \"w\" , encoding=\"utf-8\") as f :\n",
    "    f.write(\"\\n\".join(input_texts + target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d30175",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input = \"corpus.txt\",\n",
    "    model_prefix = model_prefix , \n",
    "    model_type = \"bpe\" , \n",
    "    vocab_size = min(vocab_desired,vocab_limit) , \n",
    "    user_defined_symbols= [\n",
    "        \"<pad>\", \"<sos>\" , \"<eos>\" , \n",
    "        BOT  , CASUAL , USER\n",
    "        ]\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae13667",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112df9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [sp.encode(t , out_type = int)  for t in input_texts]\n",
    "din = [[sos_id] + sp.encode(t,out_type = int) for t in target_texts]\n",
    "dec = [sp.encode(t,out_type=int) + [eos_id] for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbf123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_len = max(max(map(len,dec)) , max(map(len,din)) , max(map(len,dec)))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570e6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = pad_sequences(enc,maxlen=max_len ,padding=\"post\" , value=pad_id)\n",
    "dec_in = pad_sequences(din,maxlen=max_len , padding=\"post\" , value=pad_id)\n",
    "dec_out = pad_sequences(dec ,maxlen=max_len,padding=\"post\" , value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e770a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.1\n",
    "\n",
    "def unk_dropout(inputs,targets,rate=drop_rate,unk=unk_id):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_e = tf.cast(tf.random.uniform(tf.shape(e)) < rate , e.dtype)\n",
    "    mask_d = tf.cast(tf.random.uniform(tf.shape(d)) <rate , d.dtype)\n",
    "\n",
    "    inputs[\"encoder_inputs\"] = tf.where(mask_e ==1 , unk  , e)\n",
    "    inputs[\"decoder_inputs\"] = tf.where(mask_d == 1 ,unk , d)\n",
    "\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544050e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(ep):\n",
    "    if ep < 5 : return 0.05\n",
    "    if ep < 15 :  return 0.12\n",
    "\n",
    "    return 0.08\n",
    "\n",
    "class UnkSchedule(tf.keras.callbacks.Callback):\n",
    "    def en_epoch_begin(self,epoch,logs = None):\n",
    "        rate = schedule(epoch)\n",
    "        drop_rate.assign(rate)\n",
    "        print(f\"[UNK-Dropout] epoch {epoch} â†’ rate={rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd798bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 32    # her adÄ±mda GPUâ€™ya gidecek Ã¶rnek sayÄ±sÄ±\n",
    "BUFFER_SIZE = 1000  # shuffle havuzu (rastgelelik iÃ§in)\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"encoder_input\": enc_in,   # â† modelin encoder girdisi\n",
    "                \"decoder_input\": dec_in    # â† decoder girdisi (<sos> + hedef)\n",
    "            },\n",
    "            dec_out                       # â† decoder hedef dizisi (<eos> ekli)\n",
    "        )\n",
    "    )\n",
    "    # 1ï¸âƒ£  Rastgele %10 tokenâ€™Ä± <unk> yap â†’ dayanÄ±klÄ±lÄ±k artÄ±r\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # 2ï¸âƒ£  BUFFER_SIZE kadar Ã¶rnek bellekte karÄ±ÅŸtÄ±r â†’ her epoch farklÄ± sÄ±ra\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "\n",
    "    # 3ï¸âƒ£  Sabit boyutlu kÃ¼meler oluÅŸtur; eksik son batchâ€™i at\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    # 4ï¸âƒ£  EÄŸitim adÄ±mÄ± GPUâ€™da Ã§alÄ±ÅŸÄ±rken bir sonraki batchâ€™i hazÄ±rla\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f12ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK-Dropout Rate: 10% â†’ ~92 token her epochâ€™ta <unk> olacak\n",
      "Dataset hazÄ±r â†’ (32, 16)\n",
      "pad:3  sos:4  eos:5  unk:0  vocab:1000\n",
      "max_len: 16\n"
     ]
    }
   ],
   "source": [
    "# --------------------- YÃ¼zdesel Bilgi Logâ€™u ----------------------------------\n",
    "total_tokens   = (enc_in != pad_id).sum() + (dec_in != pad_id).sum()\n",
    "unk_tokens_est = int(total_tokens * drop_rate)    # teorik beklenti\n",
    "print(f\"UNK-Dropout Rate: {drop_rate*100:.0f}% â†’ ~{unk_tokens_est:,} token her epochâ€™ta <unk> olacak\")\n",
    "\n",
    "print(\"Dataset hazÄ±r â†’\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(f\"pad:{pad_id}  sos:{sos_id}  eos:{eos_id}  unk:{unk_id}  vocab:{vocab_size}\")\n",
    "print(\"max_len:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff89b50",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09553413",
   "metadata": {},
   "source": [
    "### Åimdi PE ve Embedding e geÃ§ebiliriz. Ã–nce minik bir teorik anlatÄ±mÄ± yapalÄ±m sonrasÄ±nda da kod olarak iÃ§erisine dalalÄ±m.YukarÄ±da bulunan tokenizer Ä±n Ã¼stÃ¼ne koya koya iÅŸleri halletmeye Ã§alÄ±ÅŸalÄ±m.DosyalarÄ± kontrol etmeyi unutmayÄ±n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe7169",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24852b98",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Embedding KatmanÄ± â€” Ne Yapar?\n",
    "\n",
    "- **GiriÅŸ**  \n",
    "  Tokenizerâ€™dan gelen **tamsayÄ± kimlikleri** (token IDâ€™leri).\n",
    "\n",
    "- **Ä°ÅŸlem**  \n",
    "  Her kimliÄŸi, Ã¶ÄŸrenilebilir bir tabloya (embedding matrisi) bakarak\n",
    "  **yoÄŸun (float) vektÃ¶re** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.  \n",
    "  Matris boyutu **\\(V \\times d_{\\text{model}}\\)**  \n",
    "  \\(\\;\\,V\\): sÃ¶zlÃ¼k bÃ¼yÃ¼klÃ¼ÄŸÃ¼â€ƒ|â€ƒ\\(d_{\\text{model}}\\): vektÃ¶r boyutu\n",
    "\n",
    "- **Ã‡Ä±kÄ±ÅŸ**  \n",
    "  Dizi uzunluÄŸu \\(L\\) ise tensÃ¶r ÅŸekli  \n",
    "  **\\((\\text{batch\\_size},\\, L,\\, d_{\\text{model}})\\)**\n",
    "\n",
    "- **Neden Ã–nemli?**  \n",
    "  - Benzer anlamlÄ± tokenâ€™larÄ±n vektÃ¶rleri **yakÄ±nlaÅŸÄ±r**, farklÄ±lar uzaklaÅŸÄ±r.  \n",
    "  - SayÄ±sal (ID) dizisini, sinir aÄŸÄ±nÄ±n iÅŸleyebileceÄŸi **sÃ¼rekli vektÃ¶r** dizisine Ã§evirir.\n",
    "\n",
    "- **Positional Encoding**  \n",
    "  Embedding konum bilgisi taÅŸÄ±maz; **sÄ±ra bilgisini** enjekte etmek iÃ§in\n",
    "  bu vektÃ¶rlere positional encoding **toplanÄ±r**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31371fec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043748b",
   "metadata": {},
   "source": [
    "### Ã–ncelikle token embedding fonksiyonunu tanÄ±mlÄ±caz.SonrasÄ±nda ise positionel encoding i tanÄ±mlÄ±caz ve ikisini beraber geliÅŸtireceÄŸiz.GidiÅŸatÄ± iyi anlamak iÃ§in ayrÄ± ayrÄ± modÃ¼llerde birleÅŸtireceÄŸiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc148b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98be23",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ Temel Token-Embedding (Keras Tokenizerâ€™a gÃ¶re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e8fe4",
   "metadata": {},
   "source": [
    "* Transformerâ€™da ilk adÄ±m: dizideki her token_id â†’ sabit boyutlu vektÃ¶r\n",
    "(gÃ¶mme/embedding). AÅŸaÄŸÄ±daki sÄ±nÄ±f bu iÅŸi yaparken\n",
    "\n",
    "* âˆšd_model Ã¶lÃ§eklemesi\n",
    "\n",
    "* pad> vektÃ¶rlerini sÄ±fÄ±rlama\n",
    "\n",
    "* otomatik maske Ã¼retme\n",
    "saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e229c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, pad_id=0, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.pad_id = pad_id                      # <pad> id'si\n",
    "        self.scale  = math.sqrt(d_model)          # âˆšd_model\n",
    "        self.table  = tf.keras.layers.Embedding(  # V Ã— D matris\n",
    "            vocab_size, d_model)\n",
    "\n",
    "    def compute_mask(self,ids,mask=None):\n",
    "        return tf.not_equal(ids , self.pad_id)\n",
    "    \n",
    "    def call(self, ids):                   # ids: (batch, seq_len)\n",
    "        x = self.table(ids) * self.scale   # â‘  lookup + âˆšd_model Ã¶lÃ§ekle\n",
    "        x *= tf.cast(tf.not_equal(ids, self.pad_id)[..., None], x.dtype)\n",
    "                                           # â‘¡ <pad> satÄ±rlarÄ±nÄ± sÄ±fÄ±rla\n",
    "        return x                           # â‘¢ Ã§Ä±ktÄ±: (batch, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22119f7e",
   "metadata": {},
   "source": [
    "### `compute_mask()` tam olarak ne yapÄ±yor?\n",
    "\n",
    "| AdÄ±m | AÃ§Ä±klama |\n",
    "|------|----------|\n",
    "| **Fonksiyon Ä°mzasÄ±**<br>`compute_mask(self, ids, mask=None)` | *Keras*, katmana bir tensÃ¶r gÃ¶nderdiÄŸinde otomatik olarak bu metodu Ã§aÄŸÄ±rÄ±r.<br>â€¢ `ids` â†’ `(batch, seq_len)` boyutunda **token-id** dizisi<br>â€¢ `mask=None` â†’ Ãœst katmandan gelen bir maske varsa (`None` yerine tensÃ¶r) kullanabilirdik; ama burada **yok sayÄ±yoruz** ve kendi maskemizi oluÅŸturuyoruz. |\n",
    "| **Maske OluÅŸturma**<br>`tf.not_equal(ids, self.pad_id)` | DÃ¶nÃ¼ÅŸ deÄŸeri boolean tensÃ¶rÃ¼dÃ¼r.<br>`True`  â†’  gerÃ§ek token<br>`False` â†’  `<pad>` token (yani yoksayÄ±lacak) |\n",
    "| **Maskenin Devri** | Bu bool tensÃ¶r, Kerasâ€™Ä±n otomatik â€œmask zinciriâ€ sayesinde Ã¼st katmanlara iletilir.<br>Ã–rneÄŸin `MultiHeadAttention` pad konumlarÄ±nÄ± gÃ¶rmezden gelir. |\n",
    "| **DoÄŸru `pad_id` ÅartÄ±** | `pad_id` gerÃ§ekten PADâ€™i temsil etmiyorsa âš ï¸ maskede hatalÄ± sonuÃ§ Ã§Ä±kar â†’ Model PADâ€™leri â€œgerÃ§ekâ€ sanÄ±r. |\n",
    "\n",
    "> **Ã–zet:** `compute_mask()` dÄ±ÅŸarÄ±dan maske almÄ±yor (`None`), onun yerine **`pad_id`**â€™ye bakarak **yeni bir maske** oluÅŸturup Kerasâ€™a geri veriyor. Bu mekanizma yalnÄ±zca `pad_id` doÄŸru ayarlanmÄ±ÅŸsa iÅŸe yarar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d446f",
   "metadata": {},
   "source": [
    "#### `call()` â€“ Ne yapar?\n",
    "\n",
    "| AÅŸama | Kod | Ne olur? |\n",
    "|-------|-----|----------|\n",
    "| 1. **Lookup** | `x = self.table(ids)` | `(B, L)` token-ID â†’ `(B, L, D)` vektÃ¶r |\n",
    "| 2. **Ã–lÃ§ekle** | `* self.scale` | TÃ¼m vektÃ¶rleri âˆšd_model ile Ã§arp |\n",
    "| 3. **PADâ€™leri sÄ±fÄ±rla** | `x *= tf.cast(ids!=pad_id, x.dtype)[...,None]` | `<pad>` vektÃ¶rleri 0 yapÄ±lÄ±r |\n",
    "| 4. **DÃ¶n** | `return x` | Ã‡Ä±ktÄ±: `(B, L, D)` embed tensÃ¶rÃ¼ |\n",
    "\n",
    "> **KÄ±saca:** lookup â†’ âˆšd_model â†’ pad sÄ±fÄ±rla â†’ embed edilmiÅŸ Ã§Ä±ktÄ±.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b087dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
