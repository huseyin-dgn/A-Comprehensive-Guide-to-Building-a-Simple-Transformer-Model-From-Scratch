{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbc101a",
   "metadata": {},
   "source": [
    "## Bu ipynb dosyasında Positionel Encoder ve Embedding katmanlarını inceleyeceğiz.Bu katmanları daha iyi hale getirmeye çalışacağız.Ama tüm bu işlemlerden önce tokenizer kodlarını yazalım.Yazdıktan sonra PE ve Embedding kavramlarının detayına inmeye devam edelim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f7f0",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c7c0a",
   "metadata": {},
   "source": [
    "### ÖNCE TOKENİZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0ee3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re, unicodedata, sentencepiece as spm\n",
    "import pandas as pd, tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047b1bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merhaba</td>\n",
       "      <td>Merhaba, size nasıl yardımcı olabilirim?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nasılsın?</td>\n",
       "      <td>İyiyim, teşekkür ederim. Siz nasılsınız?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adın ne?</td>\n",
       "      <td>Ben bir yapay zekâ asistanıyım. Adım yok ama y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaç yaşındasın?</td>\n",
       "      <td>Benim yaşım yok, dijitalim!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bugün günlerden ne?</td>\n",
       "      <td>Maalesef tarih bilgim yok, ama sistem saatinde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                                             output\n",
       "0              Merhaba           Merhaba, size nasıl yardımcı olabilirim?\n",
       "1            Nasılsın?           İyiyim, teşekkür ederim. Siz nasılsınız?\n",
       "2             Adın ne?  Ben bir yapay zekâ asistanıyım. Adım yok ama y...\n",
       "3      Kaç yaşındasın?                        Benim yaşım yok, dijitalim!\n",
       "4  Bugün günlerden ne?  Maalesef tarih bilgim yok, ama sistem saatinde..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\Transformerlar\\Konu Anlatımları\\Encoder - Decoder - PE - ATTN ( İLERİ DÜZEY ) = TF\\(2) -- PE & Embedding\\örnek_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce3bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = df[\"input\"].astype(str).to_list()\n",
    "raw_targets = df[\"output\"].astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0761be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text:str) -> str:\n",
    "    text = unicodedata.normalize(\"NFC\",text).lower()\n",
    "    text = re.sub(r\"[^a-zçğıöşüğ0-9\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ae409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER, BOT , CASUAL = \"<user>\" , \"<bot>\" , \"<casual>\"\n",
    "\n",
    "input_texts = [f\"{USER} {CASUAL} {clean_texts(t)}\"  for t in raw_inputs]\n",
    "target_texts = [f\"{BOT} {CASUAL} {clean_texts(t)}\" for t in raw_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ab262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix , vocab_desired , vocab_limit = \"bpe_tr\" , 8000 , 1000 \n",
    "\n",
    "with io.open(\"corpus.txt\" , \"w\" , encoding=\"utf-8\") as f :\n",
    "    f.write(\"\\n\".join(input_texts + target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d30175",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input = \"corpus.txt\",\n",
    "    model_prefix = model_prefix , \n",
    "    model_type = \"bpe\" , \n",
    "    vocab_size = min(vocab_desired,vocab_limit) , \n",
    "    user_defined_symbols= [\n",
    "        \"<pad>\", \"<sos>\" , \"<eos>\" , \n",
    "        BOT  , CASUAL , USER\n",
    "        ]\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae13667",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=f\"{model_prefix}.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "sos_id = sp.piece_to_id(\"<sos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "unk_id = sp.unk_id()\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112df9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [sp.encode(t , out_type = int)  for t in input_texts]\n",
    "din = [[sos_id] + sp.encode(t,out_type = int) for t in target_texts]\n",
    "dec = [sp.encode(t,out_type=int) + [eos_id] for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbf123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "max_len = max(max(map(len,dec)) , max(map(len,din)) , max(map(len,dec)))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570e6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_in = pad_sequences(enc,maxlen=max_len ,padding=\"post\" , value=pad_id)\n",
    "dec_in = pad_sequences(din,maxlen=max_len , padding=\"post\" , value=pad_id)\n",
    "dec_out = pad_sequences(dec ,maxlen=max_len,padding=\"post\" , value=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e770a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.1\n",
    "\n",
    "def unk_dropout(inputs,targets,rate=drop_rate,unk=unk_id):\n",
    "    e = inputs[\"encoder_input\"]\n",
    "    d = inputs[\"decoder_input\"]\n",
    "\n",
    "    mask_e = tf.cast(tf.random.uniform(tf.shape(e)) < rate , e.dtype)\n",
    "    mask_d = tf.cast(tf.random.uniform(tf.shape(d)) <rate , d.dtype)\n",
    "\n",
    "    inputs[\"encoder_inputs\"] = tf.where(mask_e ==1 , unk  , e)\n",
    "    inputs[\"decoder_inputs\"] = tf.where(mask_d == 1 ,unk , d)\n",
    "\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544050e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(ep):\n",
    "    if ep < 5 : return 0.05\n",
    "    if ep < 15 :  return 0.12\n",
    "\n",
    "    return 0.08\n",
    "\n",
    "class UnkSchedule(tf.keras.callbacks.Callback):\n",
    "    def en_epoch_begin(self,epoch,logs = None):\n",
    "        rate = schedule(epoch)\n",
    "        drop_rate.assign(rate)\n",
    "        print(f\"[UNK-Dropout] epoch {epoch} → rate={rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd798bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 32    # her adımda GPU’ya gidecek örnek sayısı\n",
    "BUFFER_SIZE = 1000  # shuffle havuzu (rastgelelik için)\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "                \"encoder_input\": enc_in,   # ← modelin encoder girdisi\n",
    "                \"decoder_input\": dec_in    # ← decoder girdisi (<sos> + hedef)\n",
    "            },\n",
    "            dec_out                       # ← decoder hedef dizisi (<eos> ekli)\n",
    "        )\n",
    "    )\n",
    "    # 1️⃣  Rastgele %10 token’ı <unk> yap → dayanıklılık artır\n",
    "    .map(unk_dropout, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # 2️⃣  BUFFER_SIZE kadar örnek bellekte karıştır → her epoch farklı sıra\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "\n",
    "    # 3️⃣  Sabit boyutlu kümeler oluştur; eksik son batch’i at\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    # 4️⃣  Eğitim adımı GPU’da çalışırken bir sonraki batch’i hazırla\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f12ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK-Dropout Rate: 10% → ~92 token her epoch’ta <unk> olacak\n",
      "Dataset hazır → (32, 16)\n",
      "pad:3  sos:4  eos:5  unk:0  vocab:1000\n",
      "max_len: 16\n"
     ]
    }
   ],
   "source": [
    "# --------------------- Yüzdesel Bilgi Log’u ----------------------------------\n",
    "total_tokens   = (enc_in != pad_id).sum() + (dec_in != pad_id).sum()\n",
    "unk_tokens_est = int(total_tokens * drop_rate)    # teorik beklenti\n",
    "print(f\"UNK-Dropout Rate: {drop_rate*100:.0f}% → ~{unk_tokens_est:,} token her epoch’ta <unk> olacak\")\n",
    "\n",
    "print(\"Dataset hazır →\", next(iter(dataset.take(1)))[0][\"encoder_input\"].shape)\n",
    "print(f\"pad:{pad_id}  sos:{sos_id}  eos:{eos_id}  unk:{unk_id}  vocab:{vocab_size}\")\n",
    "print(\"max_len:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff89b50",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09553413",
   "metadata": {},
   "source": [
    "### Şimdi PE ve Embedding e geçebiliriz. Önce minik bir teorik anlatımı yapalım sonrasında da kod olarak içerisine dalalım.Yukarıda bulunan tokenizer ın üstüne koya koya işleri halletmeye çalışalım.Dosyaları kontrol etmeyi unutmayın"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe7169",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24852b98",
   "metadata": {},
   "source": [
    "## 🔹 Embedding Katmanı — Ne Yapar?\n",
    "\n",
    "- **Giriş**  \n",
    "  Tokenizer’dan gelen **tamsayı kimlikleri** (token ID’leri).\n",
    "\n",
    "- **İşlem**  \n",
    "  Her kimliği, öğrenilebilir bir tabloya (embedding matrisi) bakarak\n",
    "  **yoğun (float) vektöre** dönüştürür.  \n",
    "  Matris boyutu **\\(V \\times d_{\\text{model}}\\)**  \n",
    "  \\(\\;\\,V\\): sözlük büyüklüğü | \\(d_{\\text{model}}\\): vektör boyutu\n",
    "\n",
    "- **Çıkış**  \n",
    "  Dizi uzunluğu \\(L\\) ise tensör şekli  \n",
    "  **\\((\\text{batch\\_size},\\, L,\\, d_{\\text{model}})\\)**\n",
    "\n",
    "- **Neden Önemli?**  \n",
    "  - Benzer anlamlı token’ların vektörleri **yakınlaşır**, farklılar uzaklaşır.  \n",
    "  - Sayısal (ID) dizisini, sinir ağının işleyebileceği **sürekli vektör** dizisine çevirir.\n",
    "\n",
    "- **Positional Encoding**  \n",
    "  Embedding konum bilgisi taşımaz; **sıra bilgisini** enjekte etmek için\n",
    "  bu vektörlere positional encoding **toplanır**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31371fec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043748b",
   "metadata": {},
   "source": [
    "### Öncelikle token embedding fonksiyonunu tanımlıcaz.Sonrasında ise positionel encoding i tanımlıcaz ve ikisini beraber geliştireceğiz.Gidişatı iyi anlamak için ayrı ayrı modüllerde birleştireceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc148b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98be23",
   "metadata": {},
   "source": [
    "# 1️⃣ Temel Token-Embedding (Keras Tokenizer’a göre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e8fe4",
   "metadata": {},
   "source": [
    "* Transformer’da ilk adım: dizideki her token_id → sabit boyutlu vektör\n",
    "(gömme/embedding). Aşağıdaki sınıf bu işi yaparken\n",
    "\n",
    "* √d_model ölçeklemesi\n",
    "\n",
    "* pad> vektörlerini sıfırlama\n",
    "\n",
    "* otomatik maske üretme\n",
    "sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e229c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, pad_id=0, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.pad_id = pad_id                      # <pad> id'si\n",
    "        self.scale  = math.sqrt(d_model)          # √d_model\n",
    "        self.table  = tf.keras.layers.Embedding(  # V × D matris\n",
    "            vocab_size, d_model)\n",
    "\n",
    "    def compute_mask(self,ids,mask=None):\n",
    "        return tf.not_equal(ids , self.pad_id)\n",
    "    \n",
    "    def call(self, ids):                   # ids: (batch, seq_len)\n",
    "        x = self.table(ids) * self.scale   # ① lookup + √d_model ölçekle\n",
    "        x *= tf.cast(tf.not_equal(ids, self.pad_id)[..., None], x.dtype)\n",
    "                                           # ② <pad> satırlarını sıfırla\n",
    "        return x                           # ③ çıktı: (batch, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22119f7e",
   "metadata": {},
   "source": [
    "### `compute_mask()` tam olarak ne yapıyor?\n",
    "\n",
    "| Adım | Açıklama |\n",
    "|------|----------|\n",
    "| **Fonksiyon İmzası**<br>`compute_mask(self, ids, mask=None)` | *Keras*, katmana bir tensör gönderdiğinde otomatik olarak bu metodu çağırır.<br>• `ids` → `(batch, seq_len)` boyutunda **token-id** dizisi<br>• `mask=None` → Üst katmandan gelen bir maske varsa (`None` yerine tensör) kullanabilirdik; ama burada **yok sayıyoruz** ve kendi maskemizi oluşturuyoruz. |\n",
    "| **Maske Oluşturma**<br>`tf.not_equal(ids, self.pad_id)` | Dönüş değeri boolean tensörüdür.<br>`True`  →  gerçek token<br>`False` →  `<pad>` token (yani yoksayılacak) |\n",
    "| **Maskenin Devri** | Bu bool tensör, Keras’ın otomatik “mask zinciri” sayesinde üst katmanlara iletilir.<br>Örneğin `MultiHeadAttention` pad konumlarını görmezden gelir. |\n",
    "| **Doğru `pad_id` Şartı** | `pad_id` gerçekten PAD’i temsil etmiyorsa ⚠️ maskede hatalı sonuç çıkar → Model PAD’leri “gerçek” sanır. |\n",
    "\n",
    "> **Özet:** `compute_mask()` dışarıdan maske almıyor (`None`), onun yerine **`pad_id`**’ye bakarak **yeni bir maske** oluşturup Keras’a geri veriyor. Bu mekanizma yalnızca `pad_id` doğru ayarlanmışsa işe yarar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d446f",
   "metadata": {},
   "source": [
    "#### `call()` – Ne yapar?\n",
    "\n",
    "| Aşama | Kod | Ne olur? |\n",
    "|-------|-----|----------|\n",
    "| 1. **Lookup** | `x = self.table(ids)` | `(B, L)` token-ID → `(B, L, D)` vektör |\n",
    "| 2. **Ölçekle** | `* self.scale` | Tüm vektörleri √d_model ile çarp |\n",
    "| 3. **PAD’leri sıfırla** | `x *= tf.cast(ids!=pad_id, x.dtype)[...,None]` | `<pad>` vektörleri 0 yapılır |\n",
    "| 4. **Dön** | `return x` | Çıktı: `(B, L, D)` embed tensörü |\n",
    "\n",
    "> **Kısaca:** lookup → √d_model → pad sıfırla → embed edilmiş çıktı.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b087dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
